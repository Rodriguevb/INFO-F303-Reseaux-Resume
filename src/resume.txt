
Chapitre 1
Introduction
1.1

Qu’est-ce qu’un r´
eseau ?

Le r´eseau est compos´e de
– syst`emes d’extr´emit´e (ou end systems) qui sont des machines qui communiquent
entre elles `a travers le r´eseau, i.e. elles se servent du r´eseau pour communiquer
entre elles
– liens, routeurs, etc. qui forment la structure mˆeme du r´eseau
Pour communiquer entre eux, les syst`emes p´eriph´eriques utilisent diff´erents protocoles.
Un protocole est un algorithme d´ecrivant la m´ethode utilis´ee pour l’envoi de donn´ees a`
travers le r´eseau. Ils d´efinissent plusieurs choses, comme le type de routage et de gestion
des congestions, c`ad qu’ils d´efinissent par o`
u acheminer les paquets pour limiter la charge,
´eviter les obstacles, etc. Certains protocoles garantissent la fiabilit´e des transferts (tous
les paquets arrivent, et dans le bon ordre), d’autres font moins de v´erifications mais sont
plus rapides, etc.

1.2

P´
eriph´
erie du r´
eseau

La p´eriph´erie du r´eseau (ou network edge) est l’ensemble des applications et syst`emes
qui utilisent le r´eseau (par opposition aux liens et routeurs qui forment le coeur du r´eseau).
Elle est g´en´eralement organis´ee selon un de ces deux mod`eles :
Mod`
ele client/serveur Une machine (le serveur) contient les informations et est toujours pr´esente sur le r´eseau. D’autres machines (les clients) se connectent a` celle-ci
pour communiquer ; ils peuvent se connecter et se d´econnecter a` tout moment ; les
communications ne se font qu’entre clients et serveur.
Mod`
ele peer-to-peer Chaque syst`eme est a` la fois client et serveur. L’information est
d´ecentralis´ee, et les diff´erents syst`emes communiquent entre eux directement.

1.3

Aspects physiques

Pour transmettre des donn´ees, on peut utiliser le r´eseau t´el´ephonique classique. Pour
cela, on utilise un modem qui permet de (d´e)coder l’information. Il existe plusieurs types
5

CHAPITRE 1. Introduction
de modulation :
Modulation d’amplitude (AM) Pour coder un signal binaire, on change l’amplitude
de l’onde selon que le bit soit `a 1 ou a` 0
Modulation de fr´
equence (FM) Pour coder un signal binaire, on change la fr´equence
de l’onde selon que le bit soit `a 1 ou a` 0
De plus, pour augmenter le d´ebit, on essaye de coder plusieurs bits d’un coup en combinant ces techniques (on peut par exemple utiliser la phase et la fr´equence pour coder
plusieurs bits d’un coup). On mesure alors le d´ebit en baud, 1 baud ´etant 1 symbole/seconde (c`ad 1 ensemble de bits par seconde)

1.3.1

Limites de d´
ebit

Avec le r´eseau t´el´ephonique classique, le baud-rate (quantit´e de bauds par seconde)
est limit´ee `a 4KHz, car c’est la fr´equence pr´evue pour le t´el´ephone (fr´equence de la voix
humaine). On ne peut donc coder qu’un nombre limit´e de bits par bauds. De plus, il y a
une limite au maintien du signal : au minimum 1/2 p´eriode (sous ce seuil, on ne sait plus
interpr´eter le signal de mani`ere univoque car on ne connaˆıt p.e. pas l’amplitude). Enfin,
il y a une limitation quand a` la quantit´e de bits cod´es par baud : on ne peut pas prendre
un amplitude trop grande pour coder les diff´erents ensembles de bits, sans quoi on risque
de griller le fil de cuivre.
De plus, le data-rate (nombre de bits par seconde) est limit´e aussi, mais par la loi de
Shannon. En effet, moins le signal est maintenu longtemps, plus le bruit est important,
ce qui limite donc le d´ebit.

1.3.2

R´
eseau DSL

Le r´eseau DSL utilise la mˆeme technique que la connexion dial-up, `a la diff´erence
qu’on utilise d’autres bandes de fr´equences. Cela permet de pouvoir t´el´ephoner et utiliser
le r´eseau simultan´ement, mais cela n´ecessite un ´equipement un peu plus important. On
s´epare aussi le trafic ”upstream” (vers le r´eseau) du trafic ”downstream” (venant du
r´eseau), en donnant au premier une bande de fr´equences moins importante, car on suppose
qu’on recevra plus de donn´ees qu’on en enverra la plupart du temps.

1.3.3

R´
eseau cabl´
e et fibre optique

Cette fois, on n’utilise plus les fils de cuivre du r´eseau t´el´ephonique comme support,
mais le cable TV et de la fibre optique (g´en´eralement un m´elange des deux). A la diff´erence
de l’ADSL, l’acc`es au r´eseau par cˆable est partag´e, c`ad que les diff´erents utilisateurs
partagent la mˆeme connexion, et donc les donn´ees et le d´ebit. C’est fort pratique pour
la t´el´evision (usage pr´evu de la fibre optique/coaxial), mais ¸ca peut poser des probl`emes
pour Internet (en termes de d´ebit ou de protection des donn´ees).

1.3.4

Types de cˆ
ables

Historiquement, on utilise des cˆables compos´es de deux fils de cuivre torsad´es. Ils sont
torsad´es pour limiter le courant induit (le passage de courant dans le fil cr´ee un champ
6

CHAPITRE 1. Introduction
magn´etique, et le passage de ce champ dans la boucle cr´ee un autre courant, plus faible,
en sens inverse), et au plus ils sont torsad´es, au plus cela a de l’incidence sur le d´ebit
offert (on r´eduit le bruit, donc par la loi de Shannon on peut augmenter le signal).
Le cable coaxial fonctionne sur le mˆeme principe, a` la diff´erence que les conducteurs
sont concentriques (il y a un fil de cuivre `a l’int´erieur, puis une couche d’isolant, un autre
fil de cuivre et le plastique qui entoure le cable).
Enfin, il y a la fibre optique, qui transporte l’information sous forme d’impulsions
lumineuses. Elles permettent le transfert de donn´ees a` grande vitesse (vitesse de la lumi`ere
dans le verre) et sont insensibles aux perturbations ´electromagn´etiques (contrairement
aux fils de cuivre), mais il y a un probl`eme li´e `a la r´efraction de la lumi`ere. En effet,
si on envoie une s´erie de photons au mˆeme instant au d´epart, ils auront tous un angle
de d´epart diff´erent, donc un angle d’incidence diff´erent aux bords de la fibre. Du coup,
ils ne seront pas r´efl´echis avec le mˆeme angle, ce qui signifie que certains feront plus de
”rebonds” que d’autres, donc parcoureront plus de distance, donc arriveront plus tard.
Au final, une impulsion lumineuse arrive dispers´ee, ce qui ralentit le d´ebit puisqu’il faut
attendre l’arriv´ee de tous les photons avant d’envoyer l’impulsion suivante. Pour r´egler
ce probl`eme, plusieurs types de fibre ont ´et´e invent´es :
Fibre multimode Il s’agit du type de fibre qui ne r´esoud pas le probl`eme d´ecrit : les
photons sont dispers´es et arrivent avec un certain d´ecalage ; tant pis, on fait avec
Fibre monomode Ce type de fibre r´esoud le probl`eme en laissant une zone de propagation extrˆemement ´etroite (2.4 µm). De cette mani`ere, les photons seront presque
parall`eles a` la fibre ; ils se r´efl´echiront donc tr`es peu sur les parois et seront peu
dispers´es
Fibre multimode `
a coefficient variable Ce type de fibre ressemble a` la fibre multimode, `a la diff´erence que le coefficient de r´efraction est diff´erent en tout point de
la fibre. De cette mani`ere, on s’arrange pour que les photons qui s’approchent du
bord de la fibre soient acc´el´er´es par rapport a` ceux allant ”tout droit”. Au final,
certains photons auront donc fait plus de chemins que d’autres, mais ils arriveront
en mˆeme temps puisque leur vitesse varie.

1.4

Coeur du r´
eseau

Le r´eseau ´etant partag´e entre tous ses utilisateurs de mani`ere ”transparente”, il faut
trouver un moyen pour que ces diff´erents syst`emes d’extr´emit´e puissent communiquer. Il
existe principalement deux techniques :

1.4.1

Circuit switching

Lorsqu’un syst`eme d’extr´emit´e veut se connecter a` un autre, on ´etablit un lien ”fixe”
entre ces deux syst`emes. Cela permet d’ˆetre sˆ
ur que l’on aura toujours un chemin menant
a` l’autre syst`eme (si on a su se connecter au d´epart, on saura toujours communiquer
tant qu’on ne se d´econnecte pas), et cela permet d’avoir une connexion non-partag´ee.
En revanche, les ressources sont immobilis´ees sur ce circuit le temps de la connexion, i.e.
personne d’autre que les 2 syst`emes connect´es ne pourront utiliser ces liens tant que la

7

CHAPITRE 1. Introduction
connexion dure. De plus, il faut d’abord ´etablir un circuit avant de pouvoir communiquer,
ce qui n’est pas toujours possible (r´eseau satur´e, ...)

1.4.2

Packet switching

Lorsqu’un syst`eme envoie des donn´ees a` un autre, celles-ci sont d´ecoup´ees en paquets
qui ont chacun un circuit (partag´e par tout le monde). On n’´etablit donc pas de connexion
avec la destination `a priori, chaque paquet pouvant avoir un itin´eraire diff´erent. De plus,
pour ´eviter d’immobiliser les ressources, on les partage entre les diff´erents utilisateurs.
Trois techniques sont possibles :
Frequency Division Multiplexing (FDM) On attribue `a chaque utilisateur une certaine bande de fr´equence (cela fait donc diminuer le d´ebit par utilisateur !). De cette
mani`ere, les diff´erents utilisateurs peuvent envoyer des donn´ees simultan´ement, elles
ne seront juste pas `a la mˆeme fr´equence. L’inconv´enient de cette technique est qu’elle
n’est pas flexible : il est difficile d’ajouter ou enlever des utilisateurs.
Time Division Multiplexing (TDM) Fonctionne sur le mˆeme principe que le time
sharing dans les OS : on permet a` chaque utilisateur d’utiliser l’ensemble des
fr´equences disponibles, mais il n’y a pas acc`es tout le temps ; l’usage est partag´e
entre les diff´erents utilisateurs. De cette mani`ere, on peut facilement ajouter ou
enlever des utilisateurs, et ceux-ci peuvent utiliser la totalit´e du d´ebit quand c’est
leur tour ; en revanche, ils ne pourront ´emettre qu’une certaine partie du temps.
Wavelength Division Multiplexing (WDM) Technique semblable `a la FDM dans
le cas des fibres optiques : chaque utilisateur se voit attribuer une longueur d’onde
diff´erente (donc une couleur diff´erente si on ´emet dans le spectre visible). La principale diff´erence avec la FDM est que la FDM s’applique a` des courants ´electriques
(dont elle change la fr´equence), tandis que WDM s’applique a` des ondes lumineuses
(dont elle change la longueur d’onde)

1.5

Structure d’Internet

Internet a une structure relativement hi´erarchique. On trouve d’abord des ISP 1 de
niveau 1, c’est-`a-dire les ”gros” ISP qui desservent un pays, un continent, etc. Ils sont
connect´es entre eux a` des points de peering, c’est-`a-dire qu’ils se mettent d’accord pour
s’´echanger les donn´ees. Ensuite viennent les ISP de niveau 2, qui viennent se connecter
aux ISP de niveau 1 et qui desservent une zone plus petite, et ainsi de suite. Attention,
les ISP de niveau 2 payent pour se connecter aux ISP de niveau 1 ; il y a une structure
hi´erarchique. Dans le cas du peering par contre, il s’agit de deux ISP de mˆeme niveau (pas
forc´ement de niveau 1) qui ont d´ecid´e de s’interconnecter ; il n’y a donc pas r´eellement
un ISP ”sup´erieur” `a l’autre.
Lorsqu’un paquet est envoy´e a` travers Internet, il passe alors dans un premier temps
vers des ISP de plus haut niveau, jusqu’`a atteindre un ”plateau” pour apr`es utiliser des
ISP de plus bas niveau jusqu’`a atteindre la destination (on ne ”remonte” pas vers des
ISP de haut niveau une fois qu’on est redescendu a` des ISP locaux).
1. Internet Service Provider, ou Fournisseur d’Acc`es `a Internet. Pensez Belgacom.

8

CHAPITRE 1. Introduction

1.6

Retard et perte de paquets

Lorsqu’un paquet est envoy´e, chaque routeur attend que le paquet lui soit arriv´e en
entier avant de l’envoyer vers le routeur suivant (il stocke donc le paquet en m´emoire le
temps n´ecessaire). On appelle ce d´elai la vitesse de transmission, c’est-`a-dire le temps
n´ecessaire pour qu’un paquet dans son enti`eret´e parcoure 1 noeud (`a ne pas confondre
avec la vitesse de propagation, qui est le temps n´ecessaire a` un seul bit pour traverser le
lien). Cela peut d’ailleurs poser probl`eme en cas de congestion (les paquets peuvent ˆetre
mis dans une ”file d’attente” dans les routeurs, voire mˆeme ˆetre perdus si la m´emoire du
routeur est satur´ee et qu’il ne peut donc plus accepter de nouveaux paquets entrants).
Les retards peuvent donc ˆetre dˆ
us `a la vitesse de propagation, `a la vitesse de transmission (qui dans le pire des cas peut mener `a des pertes), voire a` la vitesse de traitement
du routeur lui-mˆeme (si le protocole impose qu’il fasse des v´erifications d’int´egrit´e du
paquet par exemple, cela prend du temps).

1.7

Protocoles et couches

Un protocole est un algorithme d´ecrivant la mani`ere dont plusieurs syst`emes communiquent. Les protocoles r´eseaux sont organis´es en couches, chaque couche impl´ementant
un service. Le mod`ele OSI compte 5 couches :
Application La couche utilis´ee par les applications et la couche de plus haut niveau.
Offre des services sans se soucier de ce qui se trouve en-dessous
Transport Couche interm´ediaire. Suppose que les transmissions sur le r´eseau se font
correctement ; la couche de transport se charge de r´ecup´erer ce qui arrive du r´eseau
et le transmettre aux bons processus (ou invers´ement, prendre ce que les processus
veulent envoyer sur le r´eseau et l’envoyer correctement)
R´
eseau Se charge d’acheminer les paquets a` bon port ; c’est cette couche-l`a qui s’occupe
des probl`emes de routage entre autres
Liaison Se charge de transf´erer correctement un paquet d’un ´el´ement du r´eseau `a l’´el´ement
suivant
Physique Impl´ementation physique du r´eseau : propagation des bits dans les cˆables.
En plus de ces couches, l’OSI avait pr´evu une couche de pr´esentation et une couche
de session, qui n’ont finalement pas ´et´e impl´ement´ees et on ´et´e fusionn´ees respectivement
aux couches d’application et de transport.

9

Chapitre 2
Couche applicative
2.1

Principe de base

Le but de la couche applicative est de permettre de cr´eer des applications (sur les
syst`emes d’extr´emit´e) sans toucher `a la structure du r´eseau. Cela permet de concevoir et
d´eployer des applications sans toucher au r´eseau lui-mˆeme, et cela permet `a ces applications de communiquer entre elles sans connaˆıtre le fonctionnement du r´eseau. On utilise
pour cela des sockets (voir plus bas)

2.1.1

Architectures client-serveur et peer-to-peer

Il s’agit des deux mani`eres les plus courantes d’organiser une application en r´eseau.
Dans le mod`ele client/serveur, on consid`ere qu’il y a une machine (le serveur) qui est
toujours pr´esente sur le r´eseau et facilement identifiable (IP statique par exemple). A
cette machine viennent se connecter d’autres machines (les clients), qui communiquent
avec le serveur et qui, elles, peuvent se d´econnecter et se reconnecter comme elles veulent.
Il s’agit donc d’une architecture centralis´ee.
Au contraire, dans le mod`ele peer-to-peer, chaque client est aussi un serveur, c’esta`-dire que chaque client peut se connecter `a n’importe quel autre client pour obtenir
l’information voulue. Cela permet de r´esoudre un probl`eme r´ecurrent de l’architecture
client-serveur qui est le probl`eme de la congestion : lorsqu’il y a trop de clients connect´es,
la charge de travail pour le serveur augmente, ce qui n´ecessite donc d’utiliser plus de
ressources (plus de serveurs ou des serveurs plus puissants). Dans le mod`ele P2P, il y a
toujours autant de serveurs que de clients donc le probl`eme ne se pose (en principe) pas.
Par contre, il est plus difficile de trouver les autres clients sur le r´eseau puisqu’il n’y a
pas de serveur central a` contacter.

2.1.2

Programmation socket

Les sockets sont des interfaces pour le r´eseau qui permettent aux applications de
communiquer sans (trop) se soucier des services sous-jacents. Ces sockets (ainsi que les
couches inf´erieures du r´eseau) sont g´er´ees par l’OS ; autrement dit, l’application pr´ecise
juste qu’elle veut cr´eer un socket pour communiquer avec une autre machine, et c’est l’OS

10

CHAPITRE 2. Couche applicative
qui s’occupe du reste. L’application doit quand mˆeme pr´eciser le protocole de transport
qu’elle veut utiliser (voire chapitre suivant), ainsi que le processus a` contacter.
Pour identifier un processus (et donc pouvoir lui envoyer des messages), il faut d’abord
identifier la machine sur laquelle ce processus est lanc´e. Cela est fait grˆace `a son adresse IP,
un entier de 32 bits identifiant la machine de mani`ere univoque. Il faut ensuite identifier
le processus sur cette machine, ce qui se fait grˆace au num´ero de port. Le num´ero de
port est un num´ero associ´e `a un processus lanc´e sur la machine. On n’utilise pas le
PID du processus directement car il y a plusieurs avantages a` utiliser un autre num´ero ;
par exemple, on peut changer le processus associ´e `a un port en cours d’utilisation (voir
chapitre suivant pour plus de d´etails).

2.1.3

Protocoles de transport

Comme indiqu´e plus haut, lors de la cr´eation du socket, l’application doit pr´eciser quel
protocole elle veut utiliser. La raison `a cela est que diff´erents protocoles offrent diff´erents
types de services. Certains protocoles sont plus rapides que d’autres ; certains sont fiables
tandis que d’autres peuvent perdre ou corrompre des donn´ees ; certains sont s´ecuris´es ;
etc.
Il y a principalement deux protocoles de la couche de transport qui sont utilis´es
couramment : TCP et UDP. TCP garantit que toutes les donn´ees envoy´ees arriveront,
et dans le bon ordre, mais il est plus lent qu’UDP qui, lui, ne fait aucune v´erification.

2.2

Le protocole HTTP

HTTP est le protocole utilis´e sur le Web. Il permet l’envoi de pages Web compos´ee
de diff´erents objets (page HTML, images, ...). Il s’agit d’un protocole client/serveur qui
fonctionne sous forme de requˆetes et r´eponses. Pour cela, HTTP utilise TCP puisque le
transfert de donn´ees doit ˆetre fiable (on ne veut pas qu’il manque des ”bouts de page
Web” qui se soient perdus sur le r´eseau). Un transfert HTTP se passe g´en´eralement de
cette mani`ere :
– Le client ´etablit une connexion TCP avec le serveur
– Le serveur accepte la connexion
– Le client envoie une requˆete au serveur
– Le serveur envoie une r´eponse au client
– Le client demande de fermer la connexion TCP
– Le serveur clˆot la connexion
Notons qu’il existe des connections HTTP persistantes, o`
u le client et le serveur
peuvent s’´echanger plusieurs objets avant de refermer la connexion, et des connections
HTTP non-persistantes, o`
u la connexion est ferm´ee `a chaque fois qu’un objet est transf´er´e.
Le mode persistant est ´evidemment plus avantageux puisqu’il permet de gagner du temps
et du trafic sur le r´eseau (en mode non-persistant, il faut renvoyer une demande de
connexion a` chaque fois, ce qui prend du temps). De plus, le mode persistant permet le
pipelining, c’est-`a-dire que le navigateur Web peut envoyer toutes les requˆetes HTTP a`
la suite l’une de l’autre, sans forc´ement attendre la r´eponse de la premi`ere requˆete pour
envoyer la suivante.

11

CHAPITRE 2. Couche applicative
Enfin, signalons que le protocole HTTP est stateless, c’est-`a-dire qu’une requˆete HTTP
ne tient pas compte des actions effectu´ees pr´ec´edemment. Une mani`ere de prendre en
compte l’´etat du syst`eme est d’utiliser des cookies. Un cookie est un petit fichier, stock´e
chez le client mais cr´e´e par une requˆete HTTP 1 , qui enregistre une information. Ils sont
utilis´es par exemple pour les sites Web qui n´ecessitent une identification (achat en ligne,
etc.)

2.3

Les proxys et la mise en cache

Il s’agit de deux m´ecanismes compl´ementaires utilis´es de mani`ere intensive sur Internet
(cf section sur DNS). Un proxy est une machine qui se place entre les clients et le serveur.
Au lieu d’envoyer directement leurs requˆetes aux serveurs, les clients l’envoient au proxy,
qui les transmet au serveur si n´ecessaire.
L’avantage d’un tel syst`eme est que, si la page Web `a d´elivrer n’a pas chang´e, le proxy
peut la garder en m´emoire (en cache) pour la d´elivrer lui-mˆeme aux clients, sans devoir
faire appel au serveur. De cette mani`ere, on ´economise de la bande passante (puisqu’il
ne faut plus transmettre la requˆete jusqu’au serveur ; g´en´eralement les proxys sont assez
proches des clients sur le r´eseau) et du travail pour le serveur (si 10 clients demandent
une page que le proxy a en cache, c’est le proxy qui d´elivrera cette page donc le serveur
”´economise” 10 ´echanges)
Cela dit, pour qu’un caching fonctionne efficacement, il faut rajouter quelques ´el´ements
dans le protocole HTTP.
Tout d’abord, notons que dans une requˆete HTTP, le client pr´ecise syst´ematiquement
quel est l’hˆote qu’il veut contacter (via le champ Host). La raison est simple : comme
il transmet sa requˆete HTTP au proxy et plus au serveur directement, il faut que le
proxy sache quelle page le client veut pr´ecis´ement. Imaginons que le client veuille la page
index.php du site de l’ULB, il ne suffit pas d’envoyer une requˆete GET index.php comme
¸ca aurait pu ˆetre le cas si on communiquait directement avec le serveur de l’ULB ; il faut
pr´eciser au proxy de quel site on veut la page index.php, puisqu’il stocke peut-ˆetre aussi
une page index.php pour le site de la Poste ou de MacDonald’s.
Ensuite, il y a la requˆete If-Modified-Since, qui demande si une page a ´et´e modifi´ee
depuis une date donn´ee. Notez qu’il s’agit d’une requˆete que le proxy envoie au serveur,
puisqu’il veut savoir si il doit mettre a` jour sa m´emoire cache ou non. A cette requˆete, le
serveur renvoie soit un code 304 (page non-modifi´ee, sous-entendu celle pr´esente dans la
cache est toujours correcte), soit un code 200 suivi de la nouvelle version de la page, si
celle-ci a chang´e.

2.4
2.4.1

DNS
Principe de base

L’id´ee de base de DNS est d’associer des adresses a` des noms symboliques, c’est-`a-dire
de pouvoir identifier un domaine par un nom, et plus par son adresse IP. Ceci a plusieurs
1. La cr´eation d’un cookie se fait avec Set-Cookie, qui se trouve dans la r´eponse HTTP. Lorsque le
client formule une requˆete HTTP, il ajoute les champs cookie correspondant au site en question.

12

CHAPITRE 2. Couche applicative
avantages :
– Il est plus facile pour un humain de retenir un nom fait de mots qu’une adresse
cod´ee sur 32 bits
– On peut changer d’adresse IP sans devoir changer les liens hypertextes sur toutes les
pages qui pointent vers le serveur en question. En effet, il suffira de changer l’adresse
IP associ´ee au nom de domaine pour que la redirection se fasse correctement.
– En poussant ce m´ecanisme un peu plus loin, cela permet mˆeme de faire du load
balancing, c’est-`a-dire de r´epartir la charge de mani`ere relativement ´equitable entre
les diff´erents serveurs d’un mˆeme domaine. L’id´ee, c’est que le DNS a toute une s´erie
d’adresses IP par domaine (correspondant aux diff´erents serveurs de ce domaine),
mais qu’il ne les d´elivre pas toujours dans le mˆeme ordre. Deux clients qui veulent
donc acc´eder au mˆeme domaine ne seront donc pas forc´ement dirig´es vers la mˆeme
machine physique.
– On peut utiliser un syst`eme d’aliasing, c’est-`a-dire qu’on peut donner plusieurs
noms `a une mˆeme machine physique.
Cependant, DNS pr´esente un gros probl`eme : il y a ´enorm´ement de domaines (c`ad de
sites Web) diff´erents `a l’heure actuelle, et il y a aussi de plus en plus de clients qui veulent
acc´eder `a ces domaines. Il n’est donc pas envisageable d’avoir une seule base de donn´ees
centrale qui fait les correspondances entre noms de domaines et adresses physiques ; au
lieu de cela, on a un syst`eme distribu´e qui sera expliqu´e par la suite.

2.4.2

Structure hi´
erarchique de DNS

Comme dit plus haut, la base de donn´ees ne peut pas se trouver sur un seul serveur
central puisque la charge de travail sera beaucoup trop grande. Elle est donc r´epartie
sur toute une s´erie de serveurs auxquels on acc`ede via des indirections. Admettons par
exemple que le client veuille acc´eder a` www.amazon.com
La premi`ere ´etape consiste a` contacter un des 13 serveurs DNS racines. Ce sont
des serveurs qui contiennent les adresses de tous les Top-Level Domains, c’est-`a-dire les
extensions .com, .org, .be, etc. Dans notre exemple, le client demandera l’adresse du
TLD .com, qu’il obtient d’un de ces serveurs racine.
Ensuite, il faut consulter les serveurs DNS du domaine .com pour obtenir ceux de
amazon.com. On obtient alors les adresses IP des serveurs d’Amazon, qui sont ce qu’on
appelle des authoritative servers.
Enfin, il faut encore r´ep´eter ce processus pour chaque sous-domaine jusqu’`a obtenir
l’adresse du serveur voulu, a` qui on peut alors envoyer des requˆete HTTP.

2.4.3

DNS local et caching

Le lecteur attentif aura remarqu´e que la solution pr´esent´ee ci-dessus ne r´esouds pas
compl`etement le probl`eme de congestion : il faut a` chaque fois contacter un des serveursracine qui, bien qu’au nombre de 13, ont donc ´enorm´ement de charge de travail.
C’est pourquoi on introduit un proxy et un syst`eme de caching tels qui d´ecrits plus
haut. Ce proxy s’appelle le serveur DNS local et garde en cache les entr´ees correspondant
aux diff´erents domaines qu’on lui a demand´e. Lorsqu’un client veut acc´eder a` un domaine,

13

CHAPITRE 2. Couche applicative
il formule alors sa requˆete au serveur DNS local, qui fait toutes les indirections d´ecrites cidessus s’il ne poss`ede pas l’information en cache, et lui retourne les adresses IP associ´ees
au domaine voulu. Pour cela, les serveurs DNS locaux peuvent agir de deux mani`eres :
M´
ethode it´
erative
Il s’agit principalement de la m´ethode d´ecrite ci-dessus. Admettons que le client veuille
l’adresse des serveurs de www.amazon.com. Le DNS local va alors demander a` un serveurracine l’adresse IP du TLD .com (s’il ne la pas en cache), puis va demander a` .com
l’adresse du DNS amazon.com, et ainsi de suite jusqu’`a obtenir l’IP voulue, qu’il retourne
au client. Evidemment, le DNS local ne contacte les DNS distant que s’il n’a pas l’information dans sa m´emoire cache.
M´
ethode r´
ecursive
Reprenons le mˆeme exemple. Cette fois, le DNS local transmet la requˆete a` un serveurracine. Celui-ci trouve l’adresse IP du TLD .com et lui transmet directement la suite de
la requˆete. Le TLD .com contacte alors le DNS amazon.com et lui transmet la requˆete,
et ainsi de suite. Une fois que l’adresse IP est trouv´ee, elle refait alors le trajet en sens
inverse jusqu’`a arriver au client.
Cette technique ne permet pas le caching au niveau du DNS local ; par contre, les DNS
distants (root DNS, TLD, serveurs authoritatifs, ...) peuvent avoir une cache, qui sera
alors effective pour l’enti`eret´e du r´eseau (le caching au niveau du local DNS ne concerne
que les machines connect´ees `a ce local DNS)

2.5

Programmation socket

Les sockets, introduits avec Unix, sont une mani`ere d’utiliser le r´eseau de mani`ere
relativement transparente pour le programmeur. L’id´ee est que le programme envoie des
donn´ees sur le r´eseau en ´ecrivant dans le socket (comme n’importe quel autre fichier), et
c’est ensuite l’OS qui se charge de faire ce qu’il faut pour que ¸ca marche (utiliser les bons
protocoles, etc.). Comme nous allons le voir, le programmeur doit quand mˆeme choisir
le protocole qu’il veut utiliser (TCP ou UDP), donc ce n’est pas totalement transparent
pour lui.
La programmation socket se base sur l’architecture client-serveur. Lors de la cr´eation
du socket chez le client, il faut qu’il y ait un socket ouvert du cˆot´e du serveur. Ces sockets
sont identifi´es grˆace au num´ero de port. La suite du m´ecanisme d´epend du protocole
utilis´e

2.5.1

Sockets et UDP

Pour rappel, UDP est un protocole qui n’utilise pas de connexion entre le client et le
serveur. Par cons´equent, lorsqu’on envoie un segment, le client doit a` chaque fois pr´eciser
explicitement l’IP et le num´ero de port du destinataire. A cela, l’OS ajoute l’IP et le
num´ero de port de l’exp´editeur (c’est-`a-dire lui-mˆeme), car le destinataire doit savoir
d’o`
u vient le message pour savoir comment le traiter (et a` qui r´epondre).
14

CHAPITRE 2. Couche applicative
En effet, du cˆot´e du serveur, plusieurs processus peuvent utiliser le mˆeme num´ero de
port. A la r´eception d’un paquet, celui-ci est d’abord aiguill´e vers le bon port, puis si
n´ecessaire, l’IP et le num´ero de port de la source sont utilis´es pour savoir a` quel socket
pr´ecis´ement il faut transmettre le segment.

2.5.2

Sockets et TCP

Comme on l’a vu, TCP est un protocole orient´e connexions. Cette fois, lorsque le
client se connecte au serveur via un socket existant, il pr´ecise l’IP et le num´ero de port `a
utiliser, ce qui a pour effet que le serveur cr´ee un nouveau socket, sp´ecifique `a la communication. Ensuite, une fois la connexion ´etablie, client et serveur peuvent communiquer
en s’´echangeant des flux de donn´ees directement ; il n’est plus n´ecessaire de pr´eciser manuellement les adresses IP et num´eros de port puisque la connexion reste ouverte.
Notez qu’au moment de la connexion, on pr´ecise aussi l’IP et le num´ero de port de
l’exp´editeur. Cela permet au serveur de distinguer les diff´erents clients connect´es au mˆeme
port.

15

Chapitre 3
Couche de transport
3.1

Introduction : protocoles et services

La couche de transport a pour but de mettre en communication des processus, c’esta`-dire que c’est la couche qui g`ere les sockets, les num´eros de ports, etc. Par cons´equent,
la couche de transport n’est pr´esente que dans les syst`emes d’extr´emit´e (pas dans les
routeurs et autres appareils de coeur du r´eseau).
La structure de donn´ees ´echang´ee est le segment.
Notons enfin que, comme on l’a d´ej`a vu, il y a principalement 2 protocoles dans cette
couche : UDP (qui ne fournit aucune garantie quant `a l’ordre dans lequel arrivent les
segments ni mˆeme quant au fait que les segments arrivent a` destination), et TCP, plus
fiable mais plus lent.

3.1.1

Multiplexage et d´
emultiplexage

Admettons que l’information soit arriv´ee correctement jusqu’`a la couche de transport 1 ; il faut maintenant s’arranger pour la d´elivrer au bon processus. De fait, le d´emultiplexage
est le m´ecanisme qui permet, a` partir d’un paquet donn´e, de savoir a` quel processus il
faut le d´elivrer (c’est donc un proc´ed´e qui a lieu chez le r´ecepteur ). A l’inverse, le multiplexage consiste `a envoyer l’information a` la couche r´eseau pour qu’elle puisse l’envoyer
correctement au r´ecepteur (c’est donc un proc´ed´e qui a lieu chez l’´emetteur )
D´
emultiplexage et UDP
Comme on l’a d´ej`a vu, UDP n’est pas orient´e connexions, ce qui signifie qu’il faut
pr´eciser dans chaque paquet (chaque datagramme, l’unit´e d’information envoy´ee sur le
r´eseau) quelle est le num´ero de port de la source et de la destination. Contrairement `a
l’intuition, lors du multiplexage, la couche de transport pr´ecise juste les num´eros de port
et pas les adresses IP, car cela fait partie de la couche r´eseau. En revanche, lorsqu’on
passe de la couche r´eseau `a la couche de transport chez le r´ecepteur, il faut indiquer
1. Du cˆ
ot´e de l’´emetteur, l’information vient de la couche applicative via les sockets et autres
m´ecanismes expliqu´es plus haut ; du cˆ
ot´e du r´ecepteur, l’information arrive du r´eseau via la couche
de r´eseau et le protocole IP expliqu´e plus bas

16

CHAPITRE 3. Couche de transport
d’une mani`ere ou d’une autre l’IP source (g´en´eralement via un param`etre de l’interface),
tout simplement pour que le r´ecepteur puisse r´epondre `a l’exp´editeur.
D´
emultiplexage et TCP
Ici aussi, le segment est identifi´e par un tuple (IP source, port source,
IP dest, port dest), mais pour une raison diff´erente : comme on l’a vu, plusieurs
processus peuvent utiliser le mˆeme num´ero de port et il faut donc d´elivrer le bon paquet
au bon processus. Un exemple typique est un serveur Web, qui utilise par d´efinition le
port 80. Si deux clients (A et B) sont connect´es au mˆeme serveur, il faut que les requˆetes
venant de A soient bien transmises au processus qui communique avec A, et ne soient
pas m´elang´ees avec celles de B 2 .

3.2

Le protocole UDP

Un segment UDP est relativement simple. En plus des donn´ees et des num´eros de
port dont on a d´ej`a parl´e, on rajoute juste un champ indiquant la longueur du segment,
et une somme de contrˆole (checksum). Si le rˆole de l’information de longueur est assez
´evidente, la checksum m´erite une petite explication.
Comme on l’a d´ej`a dit, UDP est un protocole non-fiable. On ne fait donc pas de
v´erifications pouss´ees ; la checksum est juste un moyen rapide de v´erifier qu’il n’y a pas
(ou peu) d’erreurs dans le segment transmis, sans quoi on ”jette” le paquet (on n’essaye
pas de corriger les erreurs). Pour cela, on d´ecoupe le segment en paquets de 16 bits (entˆete compris ! ) qu’on additionne. On prend ensuite le compl´ement du r´esultat, ce qui nous
donne la checksum. A la r´eception d’un paquet, le r´ecepteur va alors refaire la somme et
l’ajouter a` la checksum fournie dans le paquet ; si on obtient 0 comme r´esultat, c’est que
le transfert s’est bien d´eroul´e ; sinon, cela signifie que certains bits ont chang´e de valeur
en cours de route 3 .
Il n’y a pas d’autre m´ecanisme complexe mis en place par UDP. Pas de garantie que les
paquets arriveront, pas de contrˆole de flux (pour ´eviter que le r´ecepteur ne soit submerg´e
par les paquets qu’il ne sait pas traiter), pas de contrˆole de congestion (pour ´eviter de
cr´eer des ”embouteillages” sur le r´eseau), ... Ces m´ecanismes sont impl´ement´es par TCP,
que nous allons voir juste apr`es. Commen¸cons par parler de la fiabilit´e du transfert.

3.3

Introduction aux protocoles de transfert fiables

Le but du jeu est de ”faire croire” `a la couche applicative que le transfert de donn´ees
est fiable, alors qu’en fait physiquement ce n’est pas le cas (il peut y avoir des erreurs, des
paquets perdus, etc.). Le protocole de transport est donc appel´e par la couche applicative
dans le but que les donn´ees soient correctement transmises (et dans l’ordre), et celui-ci
va donc mettre en place plusieurs m´ecanismes pour s’assurer que c’est le cas. TCP est
un de ces protocoles fiables, mais nous allons d’abord voir une version ”all´eg´ee” : RDT
2. Si ce n’est pas clair, relisez la section 2.5
3. Attention, la checksum ne d´etecte pas toutes les erreurs ! Si deux bits, pr´esents `a la mˆeme position
modulo 16, changent de valeur, la checksum restera la mˆeme, donc l’erreur ne sera pas d´etect´ee !

17

CHAPITRE 3. Couche de transport
(TCP inclut aussi d’autres m´ecanismes comme le contrˆole de flux et de congestion qui
seront expliqu´es plus tard)
RDT 1.0 : canal fiable (pas d’erreur, pas de retard)
Il s’agit du cas de figure le plus simple, qui n’arrive bien entendu jamais dans la r´ealit´e.
Si on suppose que le canal est parfait (il n’y a ni erreur, ni perte, ni retard), il suffit que
l’´emetteur forme des paquets avec les donn´ees a` envoyer, qu’il les envoie au r´ecepteur,
que le r´ecepteur d´epaquette ces donn´ees et les transmettent `a sa couche applicative. Les
m´ecanismes pour cela viennent d’ˆetre d´ecrits (multiplexage et d´emultiplexage).
RDT 2.0 : pas de retard, erreurs possibles
Cette fois, il peut y avoir des erreurs dans le paquet transmis. Par ”erreur”, on entend
qu’un ou plusieurs bits ont ´et´e invers´es dans le paquet, et que la checksum ne correspond
plus 4 . Il faut alors un m´ecanisme pour savoir si le paquet a bien ´et´e transmis ou pas.
L’id´ee est simplement qu’`a la r´eception d’un paquet, le r´ecepteur va envoyer une
r´eponse a` l’´emetteur : si le paquet est correct (checksum = 0), il renvoie un paquet
ACK (acknowledgement) ; si le paquet est corrompu, il renvoie un r´eponse NAK (negative
acknowledgement). Dans ce cas, l’´emetteur renverra le paquet une nouvelle fois. Cette
technique pr´esente par contre toujours un probl`eme.
RDT 2.1 : le cas des ACK corrompus
On vient de dire que le r´ecepteur envoyait un paquet sp´ecial ACK (ou NAK) `a la
r´eception. Or on sait aussi que le canal n’est pas fiable et peut provoquer des erreurs
dans les paquets transmis. Il se peut donc qu’un paquet ACK soit corrompu en arrivant
chez l’´emetteur initial 5 .
Dans ce cas, dans le doute, l’´emetteur va choisir de renvoyer le paquet de donn´ees
une nouvelle fois par prudence. Le probl`eme est que les donn´ees ´etaient peut-ˆetre arriv´ee
correctement la premi`ere fois (si c’est un ACK qui a ´et´e corrompu), donc dans ce cas le
r´ecepteur va recevoir ce paquet deux fois. Pour que le r´ecepteur sache s’il a d´ej`a re¸cu ce
paquet ou non, on va inclure dans l’en-tˆete un num´ero de s´equence : si le r´ecepteur re¸coit
un paquet avec un num´ero de s´equence d´ej`a utilis´e, il sait qu’il y a un doublon et ne va
pas transmettre ce nouveau paquet a` la couche applicative. Ce num´ero de s´equence se
trouve dans l’en-tˆete du paquet et est ajout´e par la couche de transport.
Dans cette version simplifi´ee du protocole, on imagine qu’on n’envoie les paquets qu’un
a` la fois ; plus pr´ecis´ement, on n’envoie le paquet suivant que lorsqu’on a re¸cu un ACK (ou
NAK, ou paquet corrompu) de la part du r´ecepteur ; ce proc´ed´e s’appelle stop and wait.
Du coup, le num´ero de s´equence peut se r´esumer `a un seul bit, qui vaudra donc une fois
sur deux 1, l’autre fois 0 ; si le r´ecepteur re¸coit deux paquets de num´ero 0 (ou 1), il sait
qu’il y a un doublon.
4. Attention, rappellez-vous que la checksum n’est pas infaillible et peut laisser passer des erreurs si
un nombre pair de bits a chang´e de valeur !
5. Le packet ACK est un paquet tout `a fait classique, qui a sa propre checksum. La d´etection des
erreurs est donc facile `
a faire

18

CHAPITRE 3. Couche de transport
RDT 2.2 : plus de paquets NAK
Cette fois, au lieu d’envoyer un paquet ACK ou NAK se rapportant au dernier paquet
re¸cu, le r´ecepteur envoie toujours un paquet ACK, mais en pr´ecisant le num´ero de s´equence
du dernier paquet non-corrompu. Chez l’´emetteur, un NAK sera alors repr´esent´e par une
double r´eception d’un paquet ACK de mˆeme num´ero. (lorsque le r´ecepteur re¸coit le paquet
0, il envoie ACK(0). Ensuite, s’il re¸coit un paquet #1 corrompu, il renvoie ACK(0))
Ce changement n’a pour l’instant pas beaucoup d’int´erˆet, mais il sera utilis´e par la
suite
RDT 3.0 : erreurs possibles, pertes possibles
Cette fois, on ajoute le fait qu’un paquet peut se perdre sur le r´eseau (en cas de
congestion par exemple, si les routeurs n’ont pas une m´emoire suffisante pour stocker
tous les paquets qui arrivent). Dans ce cas, l’id´ee est qu’on introduit un timeout ; c’esta`-dire que l’´emetteur attend un certain temps, et s’il n’a pas re¸cu d’ACK apr`es ce d´elai,
il retransmet le paquet (en effet, sans cela, on pourrait se trouver dans une situation de
deadlock : l’´emetteur attend l’arriv´ee d’un ACK avant d’envoyer le paquet suivant, mais
celui-ci n’arrivera jamais puisque le paquet s’est perdu ...) 6
De plus, certains paquets peuvent se croiser sur le r´eseau, c’est-`a-dire que le timeout
peut se d´eclencher avant que le ACK n’arrive. Pour que ce syst`eme fonctionne correctement, il faut donc encore modifier deux choses au protocole :
– Il faut que les ACK soient num´erot´es ´egalement, pour indiquer a` quel paquet ils se
rapportent. ACK(0) signifie donc qu’on a bien re¸cu le paquet num´ero 0
– Comme indiqu´e, deux paquets peuvent se croiser. On ne peut donc plus se contenter
d’une num´erotation a` 1 bit, il va falloir choisir une base plus grande.
Performances de RDT 3.0
Cette section se base sur les calculs pr´esent´es aux slides 3-43 et 3-44 du cours, et les
explique plus en d´etails.
Soit R le d´ebit du canal et L la longueur de celui-ci. Le temps de propagation d’un
L
paquet est alors R
Consid´erons aussi le RTT , c’est-`a-dire le temps qui s’´ecoule entre le moment o`
u on
envoie un paquet et le moment o`
u on re¸coit son ACK.
T empsdetransmission
Ce que l’on cherche `a optimiser est l’efficacit´e du syst`eme, η = T empstotaldelacommunication
Enfin, notons que RT T × R repr´esente le nombre de bits qui peuvent se trouver
simultan´ement sur le canal (c’est-`a-dire le nombre de bits en transit).
Finalement, si on reprend la formule en fin de slide 3-43, on trouve qu’il faut R
×
L
RT T −→ 0 pour maximiser l’efficacit´e, donc soit le RTT doit tendre vers z´ero, soit
le temps de propagation doit tendre vers l’infini. Des exemples num´eriques (slide 3-44)
montrent que dans une situation r´eelle, le protocole RDT 3.0 stop-and-wait n’est absolument pas efficace ; c’est pourquoi nous allons introduire le pipelining.
6. La perte de paquet peut concerner aussi bien les paquets de donn´ees que les paquets ACK !

19

CHAPITRE 3. Couche de transport

3.4

Le pipelining

3.4.1

Id´
ee g´
en´
erale

On a vu que le protocole stop-and-wait, bien que simple `a mettre en oeuvre, ´etait
terriblement inefficace en situation r´eelle. En effet, pour chaque paquet envoy´e, on attend
d’avoir une r´eponse (... ou de ne pas en avoir apr`es un certain temps), ce qui signifie donc
qu’on n’utilise pas les ressources pendant ce temps-l`a.
L’id´ee du pipelining vient directement de cette observation : au lieu d’envoyer un seul
paquet puis attendre gentiment une r´eponse, on va envoyer plusieurs paquets d’un coup
(... et on recevra donc plusieurs ACK d’un coup plus tard). Au final, on peut envoyer
autant de paquets que la num´erotation le permet, apr`es quoi on est oblig´es d’attendre
une r´eponse (ou un timeout).
Concr`etement, il existe principalement deux mani`ere de mettre en oeuvre le pipelining : Go-Back-N et Selective-Repeat. Nous allons maintenant voir ces techniques en
d´etail

3.4.2

Go-Back-N

En Go-Back-N, le r´ecepteur envoie des ACK au fur et a` mesure qu’il re¸coit les paquets.
Les ACK sont cumulatifs, ce qui signifie que quand on envoie ACK(n), on annonce qu’on a
correctement re¸cu tous les paquets jusqu’`a n. Cela signifie aussi que si on re¸coit un paquet
corrompu ou un paquet dont le num´ero n’est pas n + 1, on renverra ACK(n) (on a bien
re¸cu les paquets jusqu’`a n, mais apr`es il y a visiblement eu un probl`eme). Si l’´emetteur
re¸coit deux fois le mˆeme ACK, il recommencera alors `a transmettre les paquets `a partir
de n + 1.
Notons que le r´ecepteur ne r´ecup`ere qu’un paquet `a la fois, ce qui peut ˆetre un avantage
(pas besoin de buffer, ce qui est pratique dans le cas de syst`emes avec une tr`es petite
m´emoire, style syst`emes embarqu´es) mais est aussi un inconv´enient : si un paquet est
perdu, l’´emetteur doit recommencer `a transmettre ses donn´ees depuis le paquet perdu, ce
qui fait perdre du d´ebit (on renvoie plusieurs fois les paquets n + 2, n + 3, ... qui ´etaient
peut-ˆetre arriv´es correctement la premi`ere fois)
Quand retransmet-on un paquet ?
Plusieurs situations peuvent se produire o`
u l’´emetteur doit re-transmettre ses paquets.
On re¸coit plusieurs fois le mˆ
eme ACK Cela signifie qu’au moins un paquet est arriv´e corrompu, ou n’est pas arriv´e du tout (le r´ecepteur sait qu’un paquet s’est
perdu si les paquets qu’il re¸coit n’ont pas des num´eros de s´equence cons´ecutifs)
On ne re¸coit pas d’ACK pendant un certain temps L’´emetteur dispose d’un (unique)
timer, repr´esentant le d´elai ´ecoul´e depuis l’envoi du paquet le plus ancien. Si on n’a
pas de r´eponse `a la fin du timeout, on suppose qu’il y a eu un probl`eme sur le
r´eseau et, par prudence, on d´ecide de re-transmettre tous les paquets plus r´ecents.
Ce timer n’est pas tr`es pr´ecis car ce n’est tout simplement pas n´ecessaire : il s’agit
d’une solution de dernier secours, le cas o`
u on re¸coit plusieurs fois le mˆeme ACK
´etant beaucoup plus fr´equent.
20

CHAPITRE 3. Couche de transport
Quand renvoie-t-on un ACK ?
De mˆeme, il existe plusieurs situations o`
u le r´ecepteur doit signaler qu’il n’a pas re¸cu
tous les paquets correctement
On re¸coit un paquet corrompu Ceci est v´erifiable facilement en comparant la checksum pr´esente dans le paquet et celle calcul´ee (voir section 3.2 et annexe A.1)
On re¸coit un paquet au num´
ero de s´
equence non-cons´
ecutif Autrement dit, apr`es
le paquet n, on re¸coit le paquet n + 2 (par exemple). Cela signifie que le paquet
n + 1 s’est perdu sur le r´eseau.
Dans les deux cas, la solution adopt´ee est plutˆot radicale : on ne garde aucun des
paquets qui arrivent tant qu’on n’a pas r´ecup´er´e le paquet probl´ematique.
A propos de la taille de la fenˆ
etre d’´
emission
La fenˆetre d’´emission repr´esente conceptuellement l’ensemble des paquets qui ont ´et´e
envoy´es mais pas encore acquit´es. Lorsque l’´emetteur re¸coit un ACK, la fenˆetre d’´emission
”glisse” alors d’un ´el´ement, et le paquet qui vient de rentrer dans la fenˆetre est alors
envoy´e. S’il faut retransmettre des paquets pour les raisons d´ecrites plus haut, c’est alors
tout le contenu de la fenˆetre qui va ˆetre transmis. Int´eressons-nous maintenant a` savoir
quelle est la meilleure taille pour cette fenˆetre. Il va de soi qu’on cherche `a avoir une
fenˆetre la plus grande possible, puisqu’on veut envoyer autant de donn´ees que possible
en continu sur le r´eseau.
Appelons K la base de la num´erotation (donc si K = 8, les num´eros de s´equence
peuvent aller de 0 a` 7), et appelons N la taille de la fenˆetre (que l’on cherche a` optimiser).
Imaginons que N ≥ K, cela signifie qu’il y a dans la fenˆetre plusieurs paquets ayant
le mˆeme num´ero de s´equence (si K = 8 et N = 9, on a dans la fenˆetre les paquets
0, 1, 2, ..., 7, 0). Cette situation est probl´ematique puisque, si on re¸coit un ACK(0), on ne
sait pas s’il se r´ef`ere au premier paquet num´ero 0 ou au second (la slide 3-53 montre
une situation ambig¨
ue). Notons que le probl`eme arrive mˆeme quand N = K (alors qu’`a
premi`ere vue, cela ne devrait pas poser de probl`emes puisqu’on a dans notre fenˆetre les
paquets 0, 1, 2, ..., 7).
En effet, imaginons la situation suivante : l’´emetteur envoie le contenu de la fenˆetre.
Le r´ecepteur re¸coit le paquet 0 et envoie ACK(0) ; `a la r´eception du ACK, la fenˆetre
de l’´emetteur glisse donc de 1 ´el´ement et contient maintenant 1, 2, ..., 7, 0 7 . Ensuite, le
paquet 1 arrive corrompu. Le r´ecepteur renvoie donc ACK(0) puisque le dernier paquet
qu’il a re¸cu correctement portait le num´ero 0. Probl`eme : chez l’´emetteur, le paquet
portant le num´ero 0 n’est plus le mˆeme ! (il s’agit de celui not´e 0 ci-dessus).
Au final, pour que cette situation ne puisse pas se produire, il faut donc prendre
N ≤ K − 1. Comme on veut avoir la fenˆetre la plus grande possible, on prendra donc
N =K −1
7. J’ai not´e le nouveau paquet 0 pour faire la distinction avec l’”ancien” paquet 0 ; il va de soi que
la ”vraie” num´erotation est 0 tout court.

21

CHAPITRE 3. Couche de transport

3.4.3

Selective Repeat

Go-Back-N est une technique fort confortable puisqu’elle nous permet d’utiliser le
r´eseau de mani`ere plus intensive qu’en stop-and-wait. Cependant, vous avez dˆ
u remarquer
un probl`eme assez embˆetant : lorsqu’on perd un paquet, on est oblig´e de retransmettre
tous les paquets transmis apr`es celui-ci. Pour ´eviter cela, nous allons introduire une autre
forme de pipelining : le Selective Repeat.
L’id´ee derri`ere Selective Repeat est la suivante : le r´ecepteur stocke en m´emoire tous
les paquets qu’il re¸coit et qui n’ont pas encore ´et´e d´elivr´es `a la couche du dessus, mˆeme
ceux qui n’arrivent pas dans l’ordre. L’avantage est ´evidemment qu’on ne doit pas retransmettre les paquets qui sont acquitt´es correctement ; en revanche, cela n´ecessite l’utilisation
d’un buffer (donc de m´emoire) du cˆot´e du r´ecepteur. Voyons cela plus en d´etails
Gestion des paquets : cˆ
ot´
e´
emetteur
Du cˆot´e de l’´emetteur, tr`es peu de choses changent. La seule grosse diff´erence concerne
l’interpr´etation des acquits. En effet, comme en Stop-and-Wait, un ACK(n) signifie que
le paquet n a ´et´e re¸cu correctement, et ne dit rien sur les autres paquets (ACK noncumulatifs). Les paquets sont donc acquit´es ind´ependamment les uns des autres, et la
fenˆetre d’´emission ne glisse que lorsque le paquet au bord de la fenˆetre est acquitt´e. De
plus, on introduit d´esormais un timer diff´erent pour chaque paquet dans la fenˆetre : si le
timer est ´ecoul´e, on d´ecide de renvoyer ce paquet-l`a (... et celui-l`a uniquement) puisque
cela signifie sans doute qu’il s’est perdu sur le r´eseau. Ce sont les seuls changements du
cˆot´e de l’´emetteur par rapport a` Go-Back-N
Gestion des paquets : cˆ
ot´
e r´
ecepteur
C’est ici que c¸a se corse. Grˆace au num´ero de paquet (pr´esent dans celui-ci), le
r´ecepteur sait si le paquet qu’il vient de recevoir est la suite directe de ce qu’il a d´ej`a
d´elivr´e a` la couche applicative 8 . Dans ce cas tout se passe comme en Go-Back-N : le
paquet est stock´e dans un buffer et d´elivr´e a` la couche applicative, un ACK est envoy´e, et
la fenˆetre de r´eception glisse jusqu’au prochain ´el´ement qu’on n’a pas encore re¸cu.
En revanche, si le paquet qu’il vient de recevoir n’est pas la suite directe de celui
qu’il a re¸cu juste avant (les paquets arrivent dans le d´esordre), le r´ecepteur va agir de
la mani`ere suivante : il va stocker en m´emoire a` la bonne place le paquet en question
(donc, si on a re¸cu le paquet n + 3, on laissera 3 buffers libres entre le d´ebut de la fenˆetre
de r´eception et le paquet stock´e) et renverra le ACK correspondant, mais ne d´elivrera
pas ce paquet `a la couche applicative (pour le moment). De cette mani`ere, lorsque les
paquets interm´ediaires arriveront, ils pourront ˆetre d´elivr´es dans le bon ordre a` la couche
applicative et la fenˆetre pourra glisser jusqu’au prochain ´el´ement en attente.
Enfin, il y a le cas habituel o`
u on re¸coit un paquet d´ej`a acquitt´e (le timeout de
l’´emetteur s’est d´eclench´e avant qu’il n’ait re¸cu l’ACK). Dans ce cas, en toute logique, on
se contente de renvoyer un ACK a` l’´emetteur pour ˆetre sˆ
ur qu’il ne renvoie pas le paquet
une troisi`eme fois.
8. Autrement dit, il s’agit du paquet qu’il s’attendait `a recevoir

22

CHAPITRE 3. Couche de transport
A propos de la taille et disposition des fenˆ
etres
Comme pour Go-Back-N, r´efl´echissons maintenant a` la position et la taille maximale
des fenˆetres d’´emissions et de r´eception.
Remarquons d´ej`a que la fenˆetre de r´eception peut ˆetre en avance sur la fenˆetre
d’´emission, mais jamais en retard. En effet, la fenˆetre de r´eception avance lorsque le
r´ecepteur re¸coit un paquet de donn´ees, tandis que la fenˆetre d’´emission avance lorsque
l’´emetteur re¸coit un ACK. De plus, les deux fenˆetres doivent forc´ement avoir la mˆeme
taille. Donc, si les fenˆetres ont une taille n, la fenˆetre de r´eception ne peut ˆetre en
avance (dans le cas extrˆeme) que de n ´el´ements maximum sur la fenˆetre d’´emission.
Par cons´equent, si on reprend le mˆeme type de raisonnement que pour Go-Back-N, on en
u K est la base de la num´erotation.
conclut que la taille de la fenˆetre doit ˆetre N ≤ K2 , o`
Imaginons en effet que la taille des fenˆetres soit plus grande que K2 de 1 ´el´ement
(notons cette taille N ). Dans le cas extrˆeme, la fenˆetre de r´eception sera donc en avance
de N = K2 + 1 ´el´ements sur la fenˆetre d’´emission. Autrement dit, les paquets dans la
fenˆetre d’´emission porteront les num´eros 0.. K2 tandis que les buffers dans la fenˆetre de
r´eception porteront les num´eros K2 +1...K +1, or K +1 = 1 puisque la num´erotation se fait
modulo K 9 . Comme pour Go-Back-N, il y a alors une ambigu¨ıt´e si l’´emetteur retransmet
son paquet n˚0. (si vous n’ˆetes pas a` l’aise avec le raisonnement math´ematique, le sch´ema
explique cela a` la slide 3-59)

3.5

Le protocole TCP

Nous avons maintenant tous les ´el´ements n´ecessaires pour voir TCP en profondeur.
Pour rappel, TCP est un protocole fiable orient´e connexion. Cela signifie que TCP garantit
entre autres :
– Que les donn´ees transmises arriveront toutes, et dans le bon ordre. Pour cela, TCP
utilise le pipelining (de mani`ere un peu diff´erente de ce qu’on a vu jusqu’`a pr´esent,
on y reviendra)
– Que l’´emetteur r´egulera son d´ebit en fonction de ce que le r´ecepteur est capable de
recevoir (contrˆole de flux). Autrement dit, l’´emetteur n’enverra pas de paquets s’il
sait que le r´ecepteur est submerg´e et ne sait pas les stocker
– Que l’´emetteur ne surchargera pas le r´eseau (contrˆole de congestions), c’est-`a-dire
qu’il ralentira son d´ebit s’il se rend compte qu’il y a beaucoup de trafic sur le r´eseau
Pour cela, TCP n´ecessite d’´etablir une connexion avant de commencer `a communiquer.
Il ne s’agit pas d’une connexion physique (circuit switching vu au premier chapitre), il
s’agit en fait d’´etablir toutes sortes de variables dont TCP a besoin avant de commencer
vraiment a` communiquer. On y reviendra.

3.5.1

Structure d’un segment

Les segments de TCP contiennent un peu plus d’informations que ceux d’UDP (les
header TCP d’un segment font 20 bytes). On y trouve tout d’abord les num´eros de port
9. Essayez en posant par exemple K = 8 et donc N = 5 si vous n’ˆetes pas `a l’aise avec les constantes.
On obtient que les paquets dans la fenˆetre d’´emission sont num´erot´es {0, 1, 2, 3, 4} tandis que ceux dans
la fenˆetre de r´eception sont num´erot´es {5, 6, 7, 0, 1}

23

CHAPITRE 3. Couche de transport
de la source et de la destination. Leur utilit´e a d´ej`a ´et´e expliqu´ee, mais pour rappel, ils
permettent de savoir `a quel processus transmettre le paquet a` la r´eception (le num´ero de
port de la source est n´ecessaire si une mˆeme machine a ´etabli plusieurs connexions avec
une autre).
Le num´ero de s´equence est un peu particulier : il ne s’agit pas d’un ”simple” num´ero
comme pr´esent´e jusqu’ici ; il s’agit en fait du num´ero du premier byte des donn´ees empaquet´ees. En effet, TCP consid`ere que ce qu’on envoie est un flux continu de donn´ees, qui
n’est donc pas d´ecoup´e en segments `a priori 10 . Chaque byte de donn´ee est donc num´erot´e,
et c’est ce num´ero qui est utilis´e comme num´ero de s´equence.
Dans le mˆeme ordre d’id´ees, le num´ero d’ACK est en fait le num´ero du dernier byte
correctement re¸cu + 1 ; autrement dit, c’est le num´ero du premier byte que le r´ecepteur
s’attend a` recevoir, mais n’a pas encore re¸cu. Notons qu’on trouve dans un mˆeme paquet
les champs ”sequence number” et ”ACK number” car TCP est bidirectionnel, c’est-`a-dire
qu’on peut en mˆeme temps acquitter des donn´ees re¸cues et envoyer nous-mˆeme d’autres
donn´ees.
Nous trouvons ensuite un ensemble de flags, et la taille de la fenˆetre de r´eception.
Cette taille repr´esente le nombre de bytes que le r´ecepteur a encore `a sa disposition pour
recevoir de nouvelles donn´ees ; il est utilis´e pour le contrˆole de flux (en gros, si l’´emetteur
sait que le r´ecepteur ne peut prendre que 30 bytes, il enverra un paquet de 30 bytes
et pas plus – plus de d´etails sur le sujet plus loin). Les flags couramment utilis´es sont
SYN (”synchronize”, pour l’´etablissement d’une nouvelle connexion), FIN (”finish”, pour
clˆoturer une connexion) et RST (”reset”, pour refuser une connexion). Les autres flags
sont l`a pour des raisons historiques mais ne sont pas utilis´es couramment.
Enfin, on trouve l’habituelle checksum qui permet de d´etecter les ´eventuelles erreurs.

3.5.2

A propos du timer

TCP n’utilise qu’un seul timer pour le pipelining, comme Go-Back-N. Si le timer
expire, on n’envoie que le segment qui n’a pas encore ´et´e ACK, et on recommence. Tout
le probl`eme consiste donc a` choisir une bonne valeur de d´epart pour le timer : trop grande,
TCP ne sera pas assez r´eactif (et donc on perdra du temps a` ne rien faire en cas de perte),
trop petite, TCP re-transmettra trop souvent un mˆeme segment et consommera donc des
ressources inutilement.
Pour ´etablir une valeur de timeout acceptable, on va se baser sur le RTT (”round-trip
time”, c’est-`a-dire le temps qui s’´ecoule entre le moment o`
u on a envoy´e un paquet et
celui o`
u on re¸coit son ACK). Le probl`eme, c’est qu’on ne connaˆıt pas le RTT a` priori,
qu’il change sans cesse au cours du temps (en fonction de l’´etat du r´eseau) et qu’il peut
ˆetre tr`es diff´erent selon qu’on est sur un r´eseau local ou sur Internet.
L’id´ee est alors d’envoyer un segment de test de temps en temps et de mesurer le temps
qu’il faut pour que l’ACK revienne. Grˆace `a cette mesure, on calcule alors une sorte de
”moyenne” d´ecrite a` la slide 3-66, que l’on met `a jour a` chaque nouvelle mesure. La valeur
de α indique a` quel point la ”moyenne” calcul´ee est sensible (si α est petit, l’estimation
variera tr`es lentement puisque le poids de la nouvelle mesure est faible ; inversement, si
10. Notons d’ailleurs que les segments TCP n’ont pas forc´ement tous la mˆeme taille, la taille maximum
autoris´ee (MSS) ´etant pr´ecis´ee lors de la connexion

24

CHAPITRE 3. Couche de transport
α est grand, l’influence qu’aura chaque nouvelle mesure sur l’estimation sera beaucoup
plus importante).
Une fois qu’on a estim´e la valeur moyenne du RTT, on peut donc choisir la valeur du
timer en prenant un RTT + une certaine marge de s´ecurit´e. Comme marge de s´ecurit´e,
on prend g´en´eralement 4 fois l’´ecart-type des mesures effectu´ees (la formule se trouve a`
la slide 3-68 ; l’estimation de l’´ecart-type est calcul´e de mani`ere semblable `a l’estimation
de la moyenne).
De cette mani`ere, on obtient un timer r´ealiste, qui s’adapte plus ou moins `a la situation
du r´eseau et qui fonctionne dans tous les cas (que le RTT soit tr`es petit ou tr`es grand).

3.5.3

La garantie de fiabilit´
e

TCP utilise une technique de pipelining `a mi-chemin entre Go-Back-N et Selective
Repeat. Le comportement est en fait plus proche de Selective Repeat, mais on n’utilise
qu’un seul timer.
L’id´ee de base est la suivante : Lorsqu’on envoie un nouveau segment, on d´emarre le
timer (dont la valeur a ´et´e calcul´ee `a la section 3.5.2). Ensuite, si aucun ACK n’a ´et´e
re¸cu au moment du timeout, on d´ecide de renvoyer le plus vieux paquet non-acquitt´e
(et uniquement celui-l`a !) 11 . Lorsqu’on re¸coit un ACK, on agit de la mani`ere habituelle :
si l’ACK confirme la r´eception d’un paquet d´ej`a acquitt´e, on ne fait rien 12 ; sinon, on
d´eplace la fenˆetre d’´emission jusqu’`a premier segment non-acquitt´e (Note : les ACK sont
cumulatifs en TCP. Comme on l’a vu dans l’introduction, un ACK indique le num´ero du
premier byte que le r´ecepteur n’a pas encore re¸cu). Dans tous les cas, on relance le timer.
Du cˆot´e du r´ecepteur par contre, il y a une petite astuce permettant d’´economiser du
trafic. Lorsqu’on re¸coit le segment auquel on s’attendait (c’est-`a-dire un segment qui a
le plus petit num´ero de s´equence), on n’envoie pas un ACK directement. On attend un
certain temps (typiquement 500ms), au cas o`
u le segment suivant arriverait aussi. Dans
ce cas, comme les ACK sont cumulatifs, on peut acquitter les deux segments d’un seul
coup. Petit exemple : a` l’instant t, le segment num´ero 500, de longueur 8, arrive chez
le r´ecepteur. 250ms plus tard, le segment num´ero 508, de longueur 8, arrive aussi. A ce
moment-l`a, le r´ecepteur renverra un unique paquet ACK(516). Cette technique s’appelle
Delayed ACK, et n’est appliqu´e qu’avec maximum 2 paquets cons´ecutifs (lorsqu’on re¸coit
le 2`eme paquet, on envoie le ACK directement)
Enfin, il existe un moyen plus efficace que le timeout pour d´etecter qu’un paquet s’est
perdu : Fast Retransmit. On consid`ere que si on a re¸cu 3 fois le mˆeme ACK, c’est que le
paquet suivant s’est perdu, et donc on d´ecide de le r´e´emettre, et ce mˆeme si le timer n’a
pas encore expir´e 13 . Exemple : le r´ecepteur re¸coit un paquet num´ero 500 de longueur 8.
Rien n’arrive dans les 500ms suivantes, il renvoie ACK(508). Ensuite, le r´ecepteur re¸coit
un paquet num´ero 516 de longueur 8. Il le stocke mais comme les ACK sont cumulatifs,
il renvoie ACK(508). Puis arrive un paquet num´ero 524 de longueur 8. Mˆeme sc´enario,
le r´ecepteur renvoie ACK(508). A ce moment-l`a, on consid`ere que le paquet num´ero 508
11. Il s’agit, en toute logique, de celui qui se trouve au tout d´ebut de la fenˆetre d’´emission
12. Cela peut se produire si le paquet suivant est corrompu
13. Il se pourrait que le segment soit simplement tr`es retard´e et se soit fait doubler par les 3 paquets
suivants, mais la probabilit´e que ce soit le cas est tr`es faible dans la pratique donc on choisit de consid´erer
un tel ´ev´enement comme une perte de paquet.

25

CHAPITRE 3. Couche de transport
s’est perdu, et l’´emetteur re-transmet alors ce paquet. A la r´eception, le r´ecepteur renverra
directement ACK(532)

3.5.4

Le contrˆ
ole de flux

Le contrˆole de flux a pour but de ne pas submerger le r´ecepteur avec des paquets qu’il
ne saura de toutes fa¸cons pas stocker. En effet, lorsque le r´ecepteur re¸coit des paquets
du r´eseau, il les stocke dans un buffer en m´emoire centrale. Ensuite, c’est l’application
qui va lire dans ce buffer a` la vitesse qu’elle veut (via un socket par exemple, cf. couche
applicative). Le probl`eme, c’est que ce buffer a une taille limit´ee et que l’application peut
ˆetre tr`es lente a` lire dedans ; donc `a partir d’un moment, le buffer ”d´eborde” si on ne fait
rien, et les paquets arrivant ne peuvent pas ˆetre stock´es.
Pour r´esoudre ce probl`eme, la technique est tr`es simple (pour une fois) : a` chaque
fois qu’une machine envoie un segment, elle pr´ecise dans l’en-tˆete de celui-ci le nombre
de bytes qu’elle peut encore recevoir 14 . De cette mani`ere, les ´eventuelles machines qui
veulent lui envoyer des informations peuvent faire du speed maching, c’est-`a-dire adapter
leur vitesse d’´emission en fonction du nombre de bytes que le r´ecepteur peut recevoir.
Petite optimisation : l’algorithme de Nagle
Cette technique fonctionne bien, mais il y a quand mˆeme un cas o`
u on peut am´eliorer
son fonctionnement : dans le cas o`
u une application ´ecrit 1 byte a` la fois sur le socket.
Avec l’algorithme ”bˆete et m´echant”, a` chaque fois que l’application ´ecrit un byte sur
le socket, TCP va directement l’envoyer au r´ecepteur. Le probl`eme, c’est qu’un paquet
contient aussi un header, qui a une taille non-n´egligeable puisqu’il faut quand mˆeme 40
bytes (... pour 1 byte de donn´ees, ce qui est un peu du gaspillage). On va donc essayer
de limiter le nombre de segments envoy´es de la mani`ere suivante :
Lorsque le premier byte est ´ecrit dans le buffer, TCP va l’envoyer directement (on a
donc bien un paquet qui a 40 bytes de header pour 1 byte de donn´ees). Mais ensuite,
lorsque les bytes suivants seront ´ecrits, on les bufferise, c’est-`a-dire qu’on les stocke sans
les envoyer. Ce n’est que lorsqu’on re¸coit le ACK du premier segment envoy´e qu’on
va envoyer le segment suivant, qui contient tous les bytes qui ont ´et´e bufferis´es. On
recommence comme ¸ca tant que la communication continue. De cette mani`ere, on ne
perdra de la place qu’avec le premier byte, les paquets suivants pouvant ˆetre plus grands.
Deuxi`
eme optimisation : l’algorithme de Clark
Un probl`eme semblable `a celui d´ecrit ci-dessus peut apparaˆıtre du cˆot´e du r´ecepteur.
Imaginons que la fenˆetre de r´eception soit remplie ; le contrˆole de flux empˆeche donc
l’´emetteur d’ envoyer de nouveaux paquets. Ensuite, la couche applicative lit un seul
byte de donn´ees ; la fenˆetre de r´eception mesure donc maintenant 1 byte. Si on ne fait
rien de sp´ecial, l’´emetteur enverra donc de nouveau un paquet avec 1 byte de donn´ees
pour 40 bytes d’en-tˆete, et on retombe sur le mˆeme cas de figure que pr´ec´edemment.
14. Rappelez-vous : TCP est bidirectionnel, donc chaque ´emetteur est aussi un r´ecepteur et a sa propre
fenˆetre de r´eception

26

CHAPITRE 3. Couche de transport
Il existe une mani`ere simple de r´esoudre le probl`eme : en ”mentant” au r´ecepteur. Au
lieu de pr´evenir le r´ecepteur d`es que de la place est lib´er´ee dans la fenˆetre de r´eception, on
va en fait attendre que cette fenˆetre soit a` moiti´e vide, ou qu’il soit possible de recevoir
un segment complet.

3.5.5

Gestion des connexions

Pour rappel, TCP est orient´e connexions. Cela ne signifie pas qu’il faut ´etablir une
connexion physique entre les deux extr´emit´es (circuit switching) ; on a vu dans le premier
chapitre que les r´eseaux actuels utilisent le packet switching pour des questions d’optimisation. En fait, la connexion dont on parle ici est purement conceptuelle. Le client
signale au serveur qu’il veut se connecter (via un paquet particulier SYN), le serveur indique qu’il accepte la connexion (grˆace `a un paquet particulier appel´e SYNACK), puis le
client confirme la connexion (grˆace a` un ACK) et peut commencer `a envoyer des donn´ees.
La raison pour laquelle TCP proc`ede ainsi est qu’il a besoin d’initialiser toutes sortes de
variables, comme la taille de la fenˆetre de r´eception dont on vient de parler, ou encore
le premier num´ero de s´equence `a utiliser. Notons qu’il n’y a pas de donn´ees dans les
paquets SYN et SYNACK puisque la connexion n’est pas encore fonctionelle. Notons aussi
que le serveur peut r´epondre a` un SYN par un RST, dans le cas o`
u il refuse d’´etablir la
connexion, car le port est incorrect ou bloqu´e par un pare-feu par exemple.
A la fermeture de la connexion, une proc´edure similaire est enclench´ee. Le client qui
n’a plus rien a` envoyer envoie un paquet FIN, auquel le serveur r´epond par un ACK.
Attention : `a ce moment-l`a, la connexion n’est pas encore ferm´ee ! En effet, en envoyant
un paquet FIN, le client dit juste au serveur qu’il n’a plus rien `a envoyer, donc qu’il n’y
aura plus de trafic dans ce sens-l`a. Mais peut-ˆetre que le serveur a encore des segments
a` envoyer, lui ; on peut donc encore recevoir des donn´ees mˆeme apr`es avoir enclench´e le
close. Enfin, quand le serveur n’a plus rien a` envoyer, il envoie lui aussi un segment FIN,
auquel le client r´epond par un ACK. Au moment o`
u le serveur re¸coit le ACK, il ferme alors
la connexion de son cˆot´e.
Il faut cependant faire attention, car en l’´etat, il pourrait y avoir un deadlock. Imaginons en effet que le client ferme sa connexion d`es qu’il a renvoy´e sont paquet FINACK.
Supposons maintenant que ce paquet se perde sur le r´eseau. Le serveur n’aura donc jamais
le FINACK et va continuer a` retransmettre son paquet FIN ind´efiniment. Pour contrer ce
probl`eme (dans la mesure du possible), on introduit donc un timed wait, un d´elai chez
le client avant qu’il ne ferme compl`etement sa connexion. De cette mani`ere, si le paquet
FINACK se perd, le serveur pourra renvoyer un FIN, et le client sera toujours pr´esent pour
lui r´epondre.
Plus de d´etails sur le sujet (`a connaˆıtre !) se trouvent dans l’annexe B

3.5.6

Contrˆ
ole des congestions

Principe de base
Le contrˆole des congestions permet d’´eviter de boucher le r´eseau en envoyant plus
de paquets qu’il ne peut le supporter. Il ne faut pas confondre ce m´ecanisme avec le
contrˆole de flux qui veille `a ne pas surcharger le destinataire (les deux ph´enom`enes sont
ind´ependants).
27

CHAPITRE 3. Couche de transport
Une congestion se manifeste soit par des paquets perdus, soit par des retards importants. En effet, lorsqu’un routeur re¸coit un paquet, il le place dans une file d’attente avant
de le transmettre (le routeur ne peut transmettre qu’un paquet `a la fois par interface).
Par cons´equent, si trop de paquets arrivent d’un coup, ils resteront pendant un certain
temps dans la file d’attente avant d’ˆetre transmis, ce qui provoque des retards. Pire :
comme la m´emoire des routeurs a une taille finie, s’il y a plus de paquets qui arrivent que
ce que le routeur peut stocker, les derniers ne pourront pas ˆetre mis dans la file d’attente
et seront donc tout bonnement perdus. Concr`etement, cela risque d’arriver d’autant plus
que le nombre de machines sur le r´eseau est ´elev´e.
Si un paquet est perdu, la technique adopt´ee est simple : l’´emetteur retransmet ce
paquet. Ca marche (dans la mesure o`
u le r´ecepteur recevra donc toutes les donn´ees), mais
¸ca ”coˆ
ute cher” : une mˆeme donn´ee est transmise plusieurs fois, ce qui ”consomme” des
ressources plusieurs fois ; d’o`
u l’int´erˆet d’´eviter les congestions.
Enfin, signalons qu’il existe des r´eseaux o`
u le contrˆole des congestions se fait par
le r´eseau lui-mˆeme. Ce n’est pas ce qu’on va voir ici ; on consid`ere que les routeurs
transmettent bˆetement l’information si possible, ce sont les syst`emes d’extr´emit´e (et en
l’occurrence TCP) qui font les v´erifications
Impl´
ementation dans TCP
TCP contrˆole les congestions selon un principe assez simple : tant que le r´eseau sait
supporter du trafic suppl´ementaire, on augmente le d´ebit. Une fois que le r´eseau sature
(ce qui se remarque par une perte de paquets), on r´eduit le d´ebit suffisamment. Toute
l’astuce sera de d´eterminer comment doser ces augmentations/diminutions pour ˆetre le
plus souvent possible au d´ebit maximum 15 .
Avant de voir ces m´ecanismes plus en d´etails, introduisons un nouveau concept : la
fenˆetre de congestion (ou cwnd). Il s’agit du nombre de bytes que l’´emetteur sait qu’il
peut envoyer sans congestionner le r´eseau. Elle est a` diff´erencier de la fenˆetre de r´eception
(ou rwnd) qui est le nombre de bytes que le r´ecepteur peut recevoir (contrˆole de flux). Si
on veut avoir en mˆeme temps un contrˆole de flux et un contrˆole de congestion (ce qui est
le cas dans TCP), il faut donc `a chaque fois envoyer le nombre minimum de bits entre ces
deux valeurs (si on d´epasse rwnd, le r´ecepteur sera submerg´e ; si on d´epasse cwnd, c’est
le r´eseau qui sera submerg´e).
Grˆace `a cette nouvelle fenˆetre, on peut reformuler notre probl`eme autrement : on veut
que, tant que le r´eseau n’est pas congestionn´e (donc tant qu’on re¸coit des ACK pour les
paquets qu’on envoie), on augmente la fenˆetre de congestion ; au contraire, lorsqu’on a
des pertes de paquets, on veut r´eduire la taille de la fenˆetre de congestion.
Augmentation exponentielle : slow start
Slow Start est le premier ´etat dans lequel on se trouve, et celui dans lequel on revient
a` chaque fois qu’on sait qu’on peut augmenter la fenˆetre de congestion. En slow start,
a` chaque fois qu’un paquet est acquitt´e, on double la fenˆetre de congestion. Le d´ebit
15. Rappelons d’ailleurs que ce d´ebit maximal est variable, en fonction par exemple du nombre de
stations qui utilisent le r´eseau simultan´ement.

28

CHAPITRE 3. Couche de transport
augmente donc de mani`ere exponentielle (on envoie d’abord 1 MSS 16 , puis 2, puis 4,
etc.). On utilise cet ´etat lorsque la fenˆetre de congestion est tr`es petite puisqu’on sait
qu’elle va augmenter tr`es vite (contrairement a` ce que le nom laisse supposer).
Augmentation lin´
eaire : congestion avoidance
Congestion Avoidance est un autre ´etat permettant d’augmenter le d´ebit ; seulement,
cette fois, on n’augmente plus le d´ebit de mani`ere exponentielle, mais de mani`ere lin´eaire
(on envoie donc d’abord 1 MSS, puis 2, puis 3, ...). On utilise typiquement cet ´etat
lorsqu’on sait qu’on s’approche de la limite de ce que le r´eseau peut supporter.
R´
eductions de la taille de la fenˆ
etre
Il existe deux cas o`
u on r´eduit la taille de la fenˆetre, et ce qu’on fait diff`ere dans
chacun des cas.
– Le premier cas est celui o`
u on re¸coit 3 ACK identiques. Cela signifie, si vous vous
rappelez de la section 3.5.3, qu’un paquet pr´ecis n’est pas arriv´e `a destination, mais
que les suivants sont arriv´es a` bon port. Autrement dit, cela dit que le r´eseau est
l´eg`erement congestionn´e, mais pas trop non plus puisque certains paquets passent.
Dans ce cas, on choisit de r´eduire la fenˆetre de congestion de moiti´e (on r´eduit le
d´ebit suffisamment pour que le r´eseau se d´ecoince, mais pas trop non plus pour ne
pas mettre trop longtemps `a revenir a` un d´ebit acceptable)
– Le deuxi`eme cas est beaucoup plus violent, puisque c’est celui o`
u le timeout se
d´eclenche. Cela signifie que le r´eseau est compl`etement bouch´e, puisqu’aucun paquet
ne passe, mˆeme pas un ACK. Dans ce cas, on ne se pose pas de questions : on
r´einitialise la fenˆetre de congestion `a 1 MSS.
Passage d’un ´
etat `
a l’autre
On a dit plus haut qu’on utilisait plutˆot Slow Start lorsqu’on est loin du d´ebit maximum, et Congestion Avoidance lorsqu’on sait qu’on approche de la limite. Il reste donc
une question importante : comment peut-on savoir si on est proche ou non de la limite ?
Pour le savoir, on va utiliser un seuil (appel´e ssthresh), qui va retenir grosso modo
quand cela devient risqu´e d’utiliser Slow Start. L’id´ee est que, lorsqu’il y a une perte
(que ce soit un timeout ou 3 ACK identiques), on d´efinit le seuil comme valant la moiti´e
de la fenˆetre de congestion. De cette mani`ere, lorsqu’on recommencera a` augmenter le
d´ebit, on utilisera Slow Start jusqu’au seuil, puis on se rendra compte qu’il est risqu´e de
continuer, et on passera plutˆot en Congestion Avoidance (un graphe montrant l’´evolution
du d´ebit en fonction du temps est disponible a` la slide 3-112)

16. Pour rappel, MSS ou Maximum Segment Size est la taille maximum qu’un paquet peut prendre,
en nombre de bytes. Il est d´efini au moment de l’ouverture de la connexion.

29

Chapitre 4
Couche r´
eseau
Nous allons maintenant nous int´eresser a` la couche r´eseau. Contrairement aux autres
couches vues jusqu’`a pr´esent, la couche r´eseau est pr´esente partout sur le r´eseau ; pas
seulement dans les syst`emes d’extr´emit´e, mais aussi dans les routeurs. Il s’agit en gros
de la couche qui permet d’acheminer des donn´ees d’un point `a un autre du r´eseau, de
mani`ere suffisamment efficace (on ne garantit pas le r´esultat le plus efficace, mais juste
un r´esultat assez efficace)
D’un point de vue du vocabulaire, l’unit´e d’information transport´ee s’appelle un datagramme (ou paquet, tout simplement). Il contient, comme d’habitude, le segment de la
couche du dessus plus des en-tˆete suppl´ementaires. De plus, nous allons nous int´eresser
ici principalement a` deux sujets : le forwarding, c’est-`a-dire le probl`eme qui consiste `a
d´eterminer `a quel routeur (voisin du routeur actuel) il faut transmettre le paquet ; et les
probl`emes de routing (ou ”routage”), qui consistent a` d´eterminer le chemin complet de la
source jusqu’`a la destination. Notons que les deux sont ´etroitement li´es (pour d´eterminer
le chemin de la source jusqu’`a la destination, on d´etermine d’une mani`ere ou d’une autre
chaque ´etape du trajet).

4.1

Rappels : packet switching et circuit switching

Tout comme les protocoles TCP et UDP dans la couche de transport, il existe deux
types de protocoles dans la couche r´eseau : ceux orient´es connexion et ceux non-orient´es
connexion. La principale diff´erence est qu’ici, il n’y a pas le choix : un r´eseau est soit
orient´e connexions, soit pas ; les deux ne peuvent pas cohabiter.
Les protocoles a` circuits virtuels sont les procoles orient´es connexion. Ce sont ceux
o`
u une connexion doit ˆetre ´etablie avant de commencer a` s’´echanger les donn´ees ; une
fois que la connexion est ´etablie, les ressources sont bloqu´ees pour cette connexion : les
paquets prendront toujours le mˆeme chemin, et leur bonne livraison est garantie (puisque
les routeurs sur le chemin leur sont r´eserv´es). On a vu `a la section 1.4.1 les garanties d’un
tel protocole.
D’un point de vue de l’impl´ementation, chaque routeur dispose d’une table d’acheminement (forwarding table) d´ecrivant a` chaque fois, pour chaque circuit, quelle est l’interface d’entr´ee et de sortie (c’est-`a-dire d’o`
u vient le paquet, et o`
u le transmettre). Pour
identifier ces circuits, on utilise des num´eros, qui peuvent varier de lien en lien ; un chemin

30

CHAPITRE 4. Couche r´eseau
complet source-destination sera donc d´ecrit par son ensemble de num´eros de circuits (voir
sch´ema slide 4-14).
Les protocoles non-orient´es connexion sont ceux couramment utilis´es dans l’Internet.
Avec ces protocoles, chaque paquet est envoy´e ind´ependamment des autres, et peut donc
prendre un chemin diff´erent entre la mˆeme source et la mˆeme destination. Il n’y a logiquement pas de connexion, ce qui signifie aussi qu’aucune ressource n’est r´eserv´ee pour une
communication particuli`ere. Plus d’infos sur le sujet se trouvent `a la section 1.4.2. Dans
ce cas, les routeurs disposent aussi d’une table d’acheminement, mais son fonctionnement
est l´eg`erement plus compliqu´e ; on en reparlera plus tard.

4.2

Fonctionnement d’un routeur

On consid`ere comme routeur n’importe quel appareil disposant d’une ou plusieurs
interface(s) d’entr´ee, d’une ou plusieurs interface(s) de sortie, et d’une puissance de calcul
lui permettant d’acheminer les paquets de la bonne entr´ee a` la bonne sortie. Il peut
donc s’agir d’un ordinateur de bureau tout a` fait classique (peu conseill´e, quand mˆeme !)
ou de mat´eriel plus sp´ecialis´e. Par la suite, on va appeler switching fabric (matrice de
commutation) le composant permettant d’acheminer un paquet d’une interface d’entr´ee
vers une interface de sortie ; il peut donc s’agir de la m´emoire centrale d’un ordinateur,
d’un bus ou d’une crossbar, c’est-`a-dire un ensemble de bus dispos´es en grille 1 .
Lorsqu’un paquet entre par une des interfaces, le routeur calcule directement par quelle
interface le paquet doit sortir. On sait donc avant d’entrer dans la switching fabric par o`
u
on va en sortir. De plus, `a l’entr´ee comme a` la sortie, on trouve les fameux buffers dont
on avait parl´e dans la couche de transport (ceux qui peuvent ”d´eborder” et provoquer
des pertes de paquets).
– Les buffers a` l’entr´ee sont n´ecessaires car la matrice de commutation n’est pas `a
acc`es infini ; si les paquets arrivent plus vite que ce que le routeur peut acheminer,
il faut bien les stocker en attendant que ¸ca se lib`ere.
– De mˆeme, les buffers `a la sortie sont n´ecessaires car il y a un certain temps de
transmission sur la ligne de sortie. Si les paquets arrivent plus vite que l’interface
de sortie ne sait les ´ecouler, il faut bien les stocker dans une file d’attente. Un
cas typique est celui o`
u plusieurs paquets provenant d’interface d’entr´ee diff´erentes
doivent sortir par la mˆeme interface de sortie.
En ce qui concerne la taille de ces buffers, la r`egle ”math´ematique” estime qu’ils
doivent faire environ RT T ×C o`
u C est la capacit´e du canal. En effet, en cas de congestion,
il faut au moins ”RT T secondes” pour qu’une connexion TCP r´eagisse et diminue le d´ebit ;
or le canal d´elivre jusqu’`a C bits par seconde, d’o`
u on calcule la taille des buffers. Cela
dit, on s’est rendus compte avec le temps que cette taille ´etait en fait surestim´ee, et qu’on
pouvait en fait prendre des buffers plus petits sans trop de probl`emes.
1. Celles-ci sont particuli`erement confortables puisque non seulement elles sont particuli`erement rapide, mais en plus elles permettent le broadcasting : en activant plusieurs noeuds sur la mˆeme ligne (cf
slide 4-23), on peut transmettre un mˆeme paquet `a plusieurs interfaces de sortie.

31

CHAPITRE 4. Couche r´eseau

4.3

Le protocole IP

IP est un des protocoles utilis´es par la couche r´eseau. Contrairement aux autres
couches, la couche r´eseau utilise plusieurs protocoles simultan´ement ; IP est le protocole
qui d´etermine les conventions d’adressage et qui s’occupe de la partie ”bas niveau”. A cela,
il faut au moins ajouter un protocole de routage (qu’on verra plus loin) pour d´eterminer
quel chemin va suivre le paquet. Dans la pratique, d’autres protocoles peuvent aussi ˆetre
utilis´es (pour la d´etection des erreurs par exemple), mais on n’en parlera pas ici. Comme
son nom l’indique, IP est le protocole utilis´e par Internet.

4.3.1

Structure d’un paquet IP

Comme dit dans l’introduction, les paquets IP sont compos´es de donn´ees (typiquement
un paquet TCP ou UDP) et d’un en-tˆete, tout comme les autres couches. La slide 4-34
d´ecrit le contenu de cet en-tˆete. Il y a cependant quelques remarques a` faire a` ce sujet :
– Le champ ”type of service” n’est pas utilis´e. Il est pr´evu dans le standard pour
permettre de pr´eciser un besoin sp´ecifique (d´ebit rapide n´ecessaire, etc.) mais n’est
en fait jamais utilis´e
– Les trois champs de la deuxi`eme ligne sont utilis´es pour la fragmentation, dont on
parlera dans la section 4.3.1
– La checksum ne v´erifie que l’en-tˆete IP. En effet, comme dit dans le chapitre sur la
couche de transport, le r´eseau ne s’occupe pas lui-mˆeme de v´erifier que les donn´ees
sont correctes ; c’est `a la couche de transport de se d´ebrouiller. De plus, hacher
une nouvelle fois les donn´ees serait de la perte de place et de performances puisque
ladite couche de transport l’a d´ej`a fait.
– Indiquer de quel type est la couche sup´erieure (champ ”upper layer”) est n´ecessaire
pour que la couche r´eseau de la machine de destination sache quoi faire avec le
paquet une fois qu’elle le re¸coit (est-ce qu’il faut consid´erer le paquet comme un
segment UDP ou TCP ?)
– Le time to live est le nombre de transmissions que le paquet peut subir avant d’ˆetre
d´etruit. Ceci permet d’´eviter que des vieux paquets se baladent ind´efiniment sur
le r´eseau si la destination n’est pas atteignable par exemple. Certains programmes
comme traceroute (ou d’autres nettement moins sympathiques) utilisent aussi ce
champ pour d´eterminer quel chemin prend un paquet (... ou provoquer un d´eni de
service)
A propos de la fragmentation
Il y a un probl`eme un peu embˆetant avec IP : les paquets peuvent avoir une taille
variable ; or, les liens, eux, sont limit´es par leur MTU (Maximum Transfer Size). Autrement dit, un canal donn´e ne peut transmettre que des paquets d’une certaine taille. D`es
lors, comment faire pour transmettre des paquets plus gros ? Comme pour les couches
au-dessus : on les d´ecoupe en petits bouts qui seront r´eassembl´es `a la fin. Notons d’ailleurs
qu’on ne reconstitue les paquets qu’une fois qu’ils arrivent `a destination : ¸ca ne sert a`
rien de se fatiguer `a les regrouper en cours de route, puisqu’avec un peu de malchance,
ils vont encore devoir traverser un lien qui n´ecessite de les re-d´ecouper ; on les aura donc
regroup´es pour rien !
32

CHAPITRE 4. Couche r´eseau
Pour ne pas m´elanger les fragments d’un mˆeme paquet IP, on va bien entendu devoir
les num´eroter. Par contre, on ne peut pas utiliser une belle num´erotation continue comme
avec TCP, car il se pourrait qu’on doive re-fragmenter un fragment. Exemple : un paquet
de taille 1000 doit passer par un lien de MTU 100. On d´ecoupe donc le paquet en fragments
0, 1, ... Mais ensuite, le fragment 0 (de taille 100 donc) doit passer par un lien de MTU
50. On doit donc le re-d´ecouper, mais comment num´eroter les deux nouveaux fragments
obtenus ?
Au lieu d’utiliser des num´eros de s´equence cons´ecutifs, on va plutˆot choisir de prendre
le num´ero du premier byte du fragment. Si on reprend le mˆeme exemple, notre paquet de
taille 1000 sera donc d´ecoup´e en fragments num´eros 0, 100, 200, ... Une fois que le paquet
num´ero 0 devra passer le deuxi`eme lien, il sera d´ecoup´e en deux fragments num´ero 0 et
50. Probl`eme r´esolu.
De plus, cette num´erotation a un autre avantage : elle facilite la reconstitution du
paquet une fois que les fragments arrivent `a destination. Si le fragment num´ero 50 2
arrive avant le fragment num´ero 0, on sait qu’on doit laisser un ”trou” de 50 bytes pour
le premier fragment.
En fait, il y a un cas o`
u on ne saura pas d´etecter qu’il manque un fragment : si
le dernier fragment est perdu ou retard´e. En effet, il n’y aura pas de ”trou” dans la
num´erotation, et le destinataire ne connaˆıt pas la taille totale que doit faire le paquet IP.
Pour r´esoudre ce probl`eme simplement, on ajoute un fragflag dans chaque fragment,
c’est-`a-dire un flag qui indique si c’est le dernier fragment du paquet ou non. De cette
mani`ere, tant qu’on n’a pas re¸cu de fragment avec le fragflag, on sait que le paquet
n’est pas complet.
Derni`ere subtilit´e : pour des raisons purement historiques, IP ne prend pas comme
num´ero de fragment le num´ero du premier byte directement ; en fait, il divise ce nombre
par 8 (donc un fragment num´ero 10 commence en fait par le 80`eme byte du paquet)
Eviter la fragmentation
Parfois, on veut ´eviter la fragmentation (pour des raisons de s´ecurit´e par exemple :
si la destination est derri`ere un pare-feu, celui-ci doit v´erifier `a quel port le paquet est
destin´e ; or cette information se trouve dans l’en-tˆete IP, donc dans le premier fragment).
Dans ce cas, il y a une solution simple : on ajoute un flag qui pr´ecise qu’on ne veut pas
fragmenter le paquet. Dans ce cas, s’il doit passer par un lien avec un MTU trop petit,
on ne s’acharne pas : on tue le paquet, tout simplement.

4.3.2

Adressage IPv4

Comme on l’a vu dans la section 4.2, pour qu’un paquet puisse aller d’une extr´emit´e `a
l’autre du r´eseau, il faut que tous les routeurs aient une table d’acheminement qui indique
par quelle interface de sortie doit passer le paquet pour atteindre quelle destination. Le
probl`eme, c’est qu’il y a ´enorm´ement de destinations possibles (plusieurs milliards) ; il
n’est pas envisageable d’enregistrer dans chaque routeur une liste exhaustive de destinations. Pour r´esoudre ce probl`eme, on va introduire des adresses hi´erarchiques, et des
sous-r´eseaux.
2. ... du paquet IP num´ero x ; n’oubliez pas qu’il faut aussi num´eroter chaque paquet IP !

33

CHAPITRE 4. Couche r´eseau
Adressage hi´
erarchique
Une adresse IP est un nombre de 32 bits repr´esentant une interface (attention ; pas
une machine ! Les routeurs, qui ont plusieurs interfaces, ont aussi plusieurs adresses IP).
On la note g´en´eralement sous la forme d’une suite de 4 nombres (allant de 0 a` 255, pour
chacun des 4 bytes) s´epar´es par des points. On verra que grˆace a` cette adresse, on pourra
atteindre le noeud du r´eseau le plus proche de la destination.
Sous-r´
eseaux et masques
Introduisons aussi le concept de sous-r´eseau. Un sous-r´eseau est un ensemble d’adresses
IP partageant le mˆeme pr´efixe (et correspondant g´en´eralement a` des machines g´eographiquement
proches). En effet, dans une adresse IP, il faut s´eparer deux ´el´ements :
– Le sous-r´eseau, repr´esent´e par les bits de poids le plus fort (les premiers chiffres de
l’adresse)
– Le num´ero de la machine ou Host ID, repr´esent´e par les derniers bits de l’adresse
et qui repr´esente une machine pr´ecise sur un sous-r´eseau.
Notons enfin que la partie ”sous-r´eseau” et la partie ”host ID” sont de taille variable et
changent en fonction du sous-r´eseau. Pour savoir combien de bits repr´esentent le sousr´eseau et combien repr´esentent l’hˆote, on utilise un masque de sous-r´eseau (ou subnet
mask ). Celui-ci est g´en´eralement repr´esent´e sous la forme /x ajout´e `a la fin de l’adresse
IP, o`
u x est le nombre de bits repr´esentant le sous-r´eseau.
Exemple concret : prenons l’adresse IP 223.13.0.24/16. Le /16 indique que le sousr´eseau est repr´esent´e par les 16 premiers bits. En l’occurrence, on cherche donc `a atteindre le sous-r´eseau 223.13.x.y. Par ´elimination, on en conclut que les 16 derniers bits
repr´esentent le num´ero de la machine (soit 24 dans ce cas-ci)
L’acheminement en IPv4
Bien. Nous avons maintenant tous les ´el´ements n´ecessaires pour le forwarding en IPv4.
Si vous vous souvenez, le probl`eme qu’on avait est qu’on ne pouvait pas lister de mani`ere
exhaustive toutes les destinations possibles et l’interface a` utiliser pour y acc´eder. Et bien
grˆace a` l’adressage hi´erarchique, le probl`eme est pratiquement r´esolu !
En effet, au lieu de lister chaque adresse de destination, nous allons maintenant faire
des agr´egations, c’est-`a-dire stocker directement des sous-r´eseaux dans la table d’acheminement. De cette mani`ere, les routeurs ext´erieurs au sous-r´eseau permettront d’acheminer
le paquet vers le bon sous-r´eseau ; puis c’est aux routeurs de ce sous-r´eseau de tirer leur
plan pour acheminer le paquet vers la bonne machine.
Si le r´eseau est bien organis´e, la r`egle peut ˆetre simple. Imaginons que toutes les
adresses en 223.13.x.y/16 appartiennent au fournisseur d’acc`es A, qui se trouve en
Norv`ege. Supposons que toutes les adresses en 224.10.x.y/16 correspondent au FAI B
qui se trouve au Mexique. Dans les tables de forwarding des routeurs ”internationaux”,
vous aurez donc deux entr´ees ; une disant de transmettre les adresses en 223.13...
vers l’interface de sortie connect´ee au FAI norv´egien et l’autre disant de transmettre
les adresses en 224.10... au FAI mexicain 3 . Les routeurs de chaque FAI ont bien sˆ
ur
3. En r´ealit´e, vous avez encore une troisi`eme entr´ee, ”default”, qui indique ce qu’il faut faire si l’IP
de destination ne correspond `
a rien de tout ¸ca. Son masque est 0.0.0.0/0

34

CHAPITRE 4. Couche r´eseau
des r`egles plus pr´ecises permettant d’acheminer le paquet au bon endroit (du style ”les
adresses en 223.13.1... vont vers Oslo, les adresses en 223.13.2... vont vers Trondheim, etc).
Ce cas-l`a ´etait simple. Dans la r´ealit´e, le r´eseau n’est malheureusement pas aussi bien
structur´e et les groupes d’adresses IP s’entremˆelent. Pour illustrer le probl`eme, reprenons
notre exemple du dessus. Imaginons maintenant que la soci´et´e Machin, qui dispose du
sous-r´eseau 223.13.10.x/24 chez le fournisseur A, d´ecide subitement de passer chez
notre fournisseur B (car il est moins cher par exemple). Elle d´esire par ailleurs garder ses
adresses IP car il serait fastidieux de changer les adresses des 150 postes de l’entreprise.
Comment modifier les tables d’acheminement des routeurs sans trop rajouter d’entr´ees ?
On pourrait bien entendu cr´eer une entr´ee pour chaque sous-r´eseau en /24, mais cela
impliquerait de cr´eer 255 nouvelles entr´ees, donc trop fastidieux.
Il existe en fait une r`egle plus simple, nomm´ee longest prefix match. L’id´ee est que,
dans les routeurs d’Internet, on mettra les lignes suivantes : 223.13.x.y/16 vers A,
223.13.10.x/24 vers B, 224.10.x.y/16 vers B. Lorsqu’on doit acheminer un paquet,
on consid`ere alors l’entr´ee avec le plus long pr´efixe commun. Si on re¸coit un paquet a`
destination de 223.13.10.1, on l’enverra vers B (car il y a 24 bits qui correspondent a` la
deuxi`eme entr´ee, contre seulement 16 pour la premi`ere), tandis que si on re¸coit un paquet
pour 223.13.42.4, on l’enverra a` A 4 .
Retour `
a la couche applicative : DHCP et l’attribution d’adresses
Bien, on a maintenant un m´ecanisme permettant de joindre n’importe quelle machine
a` laquelle est attribu´ee une adresse IP. Reste encore un probl`eme : comment obtenir une
telle adresse IP ?
Une solution simple consiste a` donner une IP fixe a` chaque machine. Autrement dit,
si je d´ecide d’acheter le dernier netbook `a la mode et que je veux le connecter a` Internet,
je dois sp´ecifier manuellement l’IP que je veux lui attribuer. Ca fonctionne, mais cette
technique a plusieurs inconv´enients :
– Ce n’est absolument pas ”plug-n-play” : il faut configurer soi-mˆeme son adresse IP
manuellement (et donc connaˆıtre son sous-r´eseau et le masque associ´e, et savoir
quels host ID sont libres)
– Il faut trouver une adresse IP non-utilis´ee soi-mˆeme, ce qui peut ˆetre assez fastidieux
(pour l’anecdote, dans certains services de l’ULB, il n’y a pas de liste des IP fixes
attribu´ees et il faut donc configurer les machines ”au pif” !)
– Chaque machine ”bloque” une adresse IP en permanence, mˆeme quand elle n’est pas
connect´ee a` Internet. Si cela ne posait pas trop de probl`emes au d´ebut, aujourd’hui
c’est a` ´eviter car on arrive tout doucement a` ´epuisement des adresses IP libres (voir
section suivante)
Bref, si cette technique fonctionne, elle n’est pas vraiment recommendable. Qu’`a cela
ne tienne, on a invent´e les IP dynamiques.
L’id´ee est la suivante : chaque organisation (un FAI, un sous-r´eseau, ...) re¸coit une
”pool” d’adresses IP, c’est-`a-dire un ensemble d’adresses qu’elle peut utiliser. Quelque
part sur le r´eseau se trouve un serveur DHCP (Dynamic Host Configuration Protocol) qui
4. Vous remarquerez donc bien que l’entr´ee ”default” ne sera vraiment utilis´ee qu’en dernier recours,
puisque son masque, donc le nombre de bits en commun avec l’IP de destination, vaut z´ero.

35

CHAPITRE 4. Couche r´eseau
g`ere cette liste d’adresses. Lorsqu’une machine veut se connecter au r´eseau, elle contacte
alors le serveur DHCP, qui lui attribue une adresse dynamiquement. Cela se passe typiquement en 4 ´etapes (voir slide 4-47) :
DHCP discover (optionnel) Ce message a pour but de d´ecouvrir le serveur DHCP
sur le r´eseau. L’id´ee est simplement qu’on broadcaste un message sur le r´eseau (adresse
IP 255.255.255.255), auquel seul le serveur DHCP va r´epondre. Notons que le client a
pour l’instant l’IP 0.0.0.0 qui signifie ”pas encore d’IP”
DHCP offer (optionnel) La r´eponse au DHCP discover. Le serveur DHCP signale
qu’il existe (en donnant son adresse IP), et propose une certaine adresse IP `a la machine (le
champ yiaddr sur la slide). Vu que pour l’instant, le client n’a toujours pas d’adresse IP,
le serveur est oblig´e de broadcaster le message (IP 255.255.255.255). Notez que ces deux
premiers messages sont optionnels : un client peut directement envoyer un DHCP request
pour demander a` avoir une IP en particulier
DHCP request (obligatoire) La requˆete en tant que tel. Le client (qui pour l’instant
n’a toujours pas d’adresse IP, donc 0.0.0.0) demande au serveur DHCP pour obtenir
l’adresse yiaddr (dans l’exemple, 223.1.2.4).
DHCP ACK (obligatoire) Le serveur confirme l’attribution de l’IP au client. Notez
que le client ne re¸coit l’IP qu’apr`es la r´eception du ACK ; donc le serveur DHCP est
encore oblig´e de broadcaster cet ACK puisque l’adresse n’est pas encore attribu´ee.
Grpace `a cette technique, nous pouvons donc maintenant obtenir une adresse IP sans
devoir la configurer soi-mˆeme, et on ne la bloque que lorsqu’on est connect´e. De plus,
cela facilite le passage d’un r´eseau `a un autre pour la mˆeme raison (il ne faut pas changer
manuellement d’IP). En revanche, il y a quelques probl`emes de s´ecurit´e a` r´esoudre pour
´eviter de flooder le r´eseau, mais cela sort du cadre du cours.
Rappelons enfin que DHCP est bien un processus de la couche applicative, dont on
ne parle ici que pour l’`a-propos. Il utilise UDP comme protocole de transport
Le probl`
eme de la p´
enurie d’adresses et NAT
Un des gros probl`emes d’IPv4 est le nombre d’adresses disponibles. En effet, on est
limit´es `a 232 adresses, soit environ 4 milliards, et on arrive tout doucement au bout du
nombre d’adresses disponibles (ce nombre peut paraˆıtre impressionnant mais il ne faut pas
oublier qu’un seul routeur utilise au moins 2 adresses IP, et que les IP sont attribu´ees par
”blocs”). Il va donc falloir trouver des solutions pour r´esoudre ce probl`eme fort ennuyeux.
Une des solutions est d’utiliser IPv6, la nouvelle version du protocole. Celle-ci propose
des adresses encod´ees sur 128 bits, ce qui est d´ej`a nettement plus confortable. On reparlera
d’IPv6 un peu plus tard, mais on peut d´ej`a remarquer un gros d´efaut : on ne sait pas
changer le protocole de tout Internet en un claquement de doigts. On ne peut pas vraiment
se permettre d’”´eteindre Internet” le temps de faire la mise a` jour et de d´ecider un beau
matin de tout passer en IPv6. Si le protocole se g´en´eralise, il faut donc noter qu’il faudra
un certain temps avant que tout Internet ne l’utilise ; il faut donc trouver une solution
temporaire.
36

CHAPITRE 4. Couche r´eseau
Une de ces ”rustines” est NAT (Network Address Translation). C’est une technique
qui ne fonctionne pas trop mal, mais qui a ses limites.
L’id´ee est la suivante : au lieu d’attribuer une IP publique `a toutes les machines
du monde, on n’attribue d’IP publique qu’au routeur a` l’entr´ee d’un r´eseau local. Les
machines pr´esentes sur ce r´eseau disposeront d’une adresse IP ”priv´ee”. Ces adresses
sp´eciales sont utilis´ees partout, dans tous les sous-r´eseaux. Du coup, le routeur de sortie
fonctionne plus ou moins comme un proxy :
– Lorsqu’un paquet vient d’une machine locale et va vers le r´eseau, le routeur change
l’IP source (priv´ee) pour mettre la sienne (publique). Pour pouvoir diff´erencier les
diff´erents clients sur son r´eseau local, il change aussi le num´ero de port source (il
attribue un ”faux” num´ero de port par client). Ces informations sont bien entendu
sauvegard´ees chez le routeur dans une table de correspondances
– Lorsque le routeur re¸coit un paquet qui ”lui” est destin´e, il fait le travail inverse :
il regarde le num´ero de port du paquet en question, puis regarde a` quelle IP locale
ce num´ero de port correspond dans sa table de correspondances. Ensuite, il change
l’IP de destination pour mettre la ”vraie” IP de la destination (celle de la machine,
donc l’IP priv´ee) et transmet le paquet selon le protocole habituel.
Cela dit, si NAT semble bien beau sur papier, il pose quand mˆeme un certain nombre
de probl`emes techniques et ”´ethiques” qui ne le rendent pas optimal :
– Les routeurs doivent commencer a` chipoter dans le contenu d’un paquet IP ; autrement dit, ils doivent chipoter dans une couche qui normalement ne les concerne pas
(on perd l’int´erˆet de l’architecture en couches)
– Cela peut poser des probl`emes si le paquet est fragment´e ou crypt´e : le num´ero de
port se trouve dans le premier fragment, or si ce n’est pas celui-l`a qui arrive en
premier au routeur il ne sait pas a` quelle IP locale il doit le transmettre. Mˆeme
remarque si le paquet est crypt´e.
– Certaines applications, en particulier les applications UDP, doivent prendre en
compte le fait que la ”v´eritable” IP de destination n’est pas publique ; autrement
dit, NAT n’est pas du tout transparent.
– Un client n’est connu du routeur (et donc de NAT) que lorsqu’il a envoy´e au moins
un paquet vers l’ext´erieur 5 . Du coup, il y a un probl`eme si un paquet est adress´e
a` une machine qui n’a encore rien envoy´e. Il y a mˆeme deadlock si les deux machines (´emetteur et r´ecepteur) sont derri`eres un NAT : aucun des deux ne peut
communiquer avec l’autre !
Bien sˆ
ur, dans ce dernier cas, des solutions ont ´et´e trouv´ees. On peut par exemple
imaginer d’installer un serveur-relais entre les deux NAT qui se chargera de transmettre
les paquets comme il faut : au lieu d’envoyer directement son paquet `a la destination,
l’´emetteur l’envoie au serveur-relais (qui n’est pas derri`ere un NAT, donc on connaˆıt son
adresse IP et elle est unique), et c’est lui qui se charge de faire la deuxi`eme partie du
travail. Mais bon, il reste encore tous les autres probl`emes cit´es. Donc en r´esum´e, NAT
c’est bien comme solution temporaire a` la p´enurie d’adresses, mais ¸ca reste un syst`eme
assez bancal a` n’utiliser qu’en attendant l’av`enement d’IPv6.
5. Revoyez le fonctionnement de NAT si vous n’ˆetes pas convaincu : c’est lorsqu’on envoie un paquet
vers l’ext´erieur que le routeur fait ses changements et remplit sa table de correspondances.

37

CHAPITRE 4. Couche r´eseau

4.3.3

ICMP et la d´
etection des erreurs

Avant de nous pencher sur IPv6, citons juste ICMP pour ˆetre complet. Il s’agit d’un
protocole utilis´e par dessus IP pour contrˆoler la transmission de paquets. Il s’agit principalement de messages qui peuvent ˆetre envoy´es par les routeurs vers l’´emetteur pour
signaler qu’une erreur s’est produite. Il est en particulier utilis´e par traceroute, ce programme qui permet de voir par o`
u passent vos paquets IP. L’id´ee est simplement qu’on
envoie des paquets avec un TTL (Time to Live, voir section 4.3.1) croissant en partant de
1. Lorsque le paquet atteint son TTL, le routeur envoie un message ICMP pour avertir
l’´emetteur que son paquet n’est pas arriv´e a` destination ; traceroute utilise ces paquets
pour d´eterminer entre autres le RTT et le chemin emprunt´e par les paquets.
ICMP n’est vraiment cit´e que pour ˆetre complet ; on ne rentrera pas plus dans les
d´etails du protocole ici.

4.3.4

Le procotole IPv6

IPv6 est la nouvelle version du protocole, en cours de d´eploiement. Elle vise principalement a` corriger les erreurs de jeunesse d’IPv4, et en particulier le peu d’adresses
disponibles. On note les changements suivants :
– Les adresses sont maintenant encod´ees sur 128 bits, ce qui devrait ˆetre largement
suffisant pour le moment ; c’est donc un bon moyen d’´eviter la p´enurie d’adresses
(voir section 4.3.2 pour plus d’infos). Notons d’ailleurs qu’on en a profit´e pour
utiliser une notation plus intuitive pour les informaticiens : les diff´erents bytes de
l’adresse sont maintenant ´ecrits en hexad´ecimal, et sont s´epar´es par deux points ( :)
– Le header IPv6 fait maintenant 40 bytes a` lui tout seul, ce qui est deux fois plus
qu’IPv4. C’est le prix a` payer pour l’utilisation d’adresses plus longues, mais avec
l’´evolution des technologies une telle perte de place n’est pas vraiment embˆetante.
– La fragmentation de paquets n’est plus autoris´ee, pour les raisons cit´ees plus haut.
– Il y a quelques changements dans le contenu du header. On trouve par exemple un
champ next header, qui est un pointeur vers le header de la couche du dessus. Il
permet de ne pas devoir pr´eciser quel protocole de transport est utilis´e, ni devoir
pr´eciser les options dans la couche r´eseau. On d´ecouvre aussi un nouveau concept
de flot de donn´ees, qui n’est pas d´efini formellement mais qui permet en gros de
regrouper les donn´ees. Enfin, on notera la disparition totale du contrˆole des erreurs,
qui est maintenant compl`etement g´er´e par les autres couches.
La transition vers IPv6
IPv6 r´esouds donc le probl`eme du manque d’adresses, et change deux-trois autres
choses au passage, mais il reste un probl`eme qu’on a d´ej`a cit´e : comment faire pour
mettre le r´eseau a` jour ? On ne peut pas subitement couper tout Internet, mettre `a jour
tous les routeurs et relancer le tout ; il faut donc faire la mise `a jour graduellement.
En fait, on va proc´eder comme suit : lorsqu’une machine (routeur ou end system)
impl´emente IPv6, elle ne supprime pas IPv4 pour autant ; il est donc possible d’y acc´eder
en IPv4 ou IPv6, au choix 6 . De cette mani`ere, les machines qui impl´ementent IPv6
utilisent ce protocole, tandis que les autres utilisent IPv4.
6. Pour ceux qui jouent en r´eseau, cela explique pourquoi un mˆeme serveur peut apparaˆıtre deux fois

38

CHAPITRE 4. Couche r´eseau
R´
esoudre un probl`
eme de cohabitation : le tunneling
Le probl`eme, c’est que les routeurs qui utilisent IPv6 ne forment pas forc´ement un
chemin complet de la source `a la destination. Or on veut utiliser IPv6 le plus possible.
Comment faire ?
R´eponse : on va consid´erer que si deux machines, qui ont toutes les deux impl´ement´e
IPv6, veulent communiquer entre elles, elles utiliseront IPv6 (mˆeme si les routeurs au
milieu n’ont pas impl´ement´e le protocole). D’un point de vue logique donc, toutes les
machines en IPv6 sont interconnect´ees. Maintenant, lorsqu’une machine A veut envoyer
un flux IPv6 `a une machine F, elle va ´etablir un chemin selon un protocole de routage
classique qu’on verra juste apr`es. Le probl`eme, c’est que d’un point de vue physique,
certains routeurs sur le chemin n’ont pas encore impl´ement´e IPv6. La solution est simple :
pour ”passer” ces routeurs, on encapsule les paquets IPv6 dans des paquets IPv4, qu’on
d´ecapsule une fois qu’on arrive de nouveau a` un routeur IPv6. Le sch´ema correspondant
a` cette situation se trouve a` la slide 4-73.

4.4

Les protocoles de routage

Nous allons maintenant nous int´eresser au probl`eme de routage. Pour rappel, le routage est le probl`eme qui consiste `a trouver un chemin complet allant d’une source `a une
destination ; il ne faut pas le confondre avec le probl`eme d’acheminement (ou forwarding),
qui consiste a` trouver le prochain noeud a` qui un noeud doit transmettre son paquet. Le
probl`eme de routage peut se comparer a` un probl`eme de recherche du plus court chemin
dans un graphe. En l’occurrence, les routeurs et les end systems forment les noeuds, les
liens physiques entre eux forment les arcs, et leur poids pourrait ˆetre par exemple le
temps qu’il faut pour franchir le lien (temps de propagation + temps moyen pass´e dans
les buffers du noeud).On pourrait plutˆot consid´erer un poids unitaire sur tous les arcs ;
autrement dit, le plus court chemin sera celui qui utilise le moins de liens possibles, et
donc qui minimise la charge totale du r´eseau. En revanche, le temps total de la transmission ne sera donc pas forc´ement minimal (il vaut parfois mieux passer par 10 liens `a
10ms que un seul lien a` 1s), et on ne fait pas de contrˆole de congestion. Plus d’infos sur
le sujet dans la section suivante.
Pour trouver le plus court chemin, on pourrait imaginer d’utiliser un algorithme classique style Dijkstra. Mais ce n’est pas conseill´e pour deux raisons :
– Le r´eseau dans son ensemble est beaucoup trop grand. Pour rappel, Dijkstra est en
O(|V |2 ) (avec optimisation possible en O(|V | log |V |), or Internet dans son ensemble
contient plusieurs millions de liens. Autant dire qu’il n’est pas vraiment envisageable
d’ex´ecuter Dijkstra sur tout le graphe `a chaque envoi de paquet.
– Le r´eseau forme un syst`eme distribu´e, c’est-`a-dire qu’il n’y a pas une machine centrale qui calcule le chemin complet, mais chaque routeur doit calculer un ”morceau
de chemin”. Dijkstra n’est pas pr´evu pour ce genre de cas (pas plus que les autres
algorithmes de plus courts chemins d’ailleurs)
Nous allons donc voir deux types d’algorithmes qui permettent de r´esoudre le probl`eme :
les algorithmes link state (ou `a ´etat de lien) et les distance vector (ou `a vecteur de disdans la liste : une fois pour la connexion IPv4, une fois pour IPv6 si votre OS le prend en charge.

39

CHAPITRE 4. Couche r´eseau
tance). Mais avant cela, il faut encore introduire quelques ´el´ements importants.

4.4.1

Avant de s’y mettre ...

Le poids des arcs
On en a d´ej`a un peu parl´e dans l’introduction : pour trouver le plus court chemin,
il faut attribuer un poids `a chaque arc, c’est-`a-dire a` chaque lien physique entre deux
routeurs. Plusieurs techniques existent, et le choix de ce poids change fortement le type
de chemin qui sera choisi (c’est assez ´evident : si on change le poids des arcs, le plus court
chemin change aussi)
Poids unitaire Dans ce cas de figure, on donne un poids de 1 a` chaque lien. L’avantage,
c’est que c’est tr`es facile a` calculer. Ce type de m´etrique permet de trouver le chemin qui
utilise le moins de liens possible, donc qui minimise la charge du r´eseau (on essaie qu’un
paquet utilise le moins de routeurs possibles). En revanche, il n’y a aucune garantie sur
le d´elai de la transmission. Cette m´etrique est constante (le poids des liens ne change pas
dans le temps).
Charge moyenne Cette m´etrique impose de connaˆıtre la matrice de trafic du r´eseau.
Une matrice de trafic est une matrice o`
u chaque case (i, j) donne la quantit´e d’information
qui va du noeud i au noeud j. Dans ce type de m´etrique, on essaye de minimiser la charge
moyenne de chacun des liens, c’est-`a-dire essayer que le paquet utilise le moins de liens
possibles. Vous remarquerez que le r´esultat est exactement le mˆeme qu’avec un poids
unitaire, mais avec des calculs plus compliqu´es.
Utilisation moyenne Cette m´etrique a pour but de minimiser l’utilisation moyenne
d’un lien. Par utilisation, on entend le temps que va passer le paquet sur chacun des liens.
Notons que cela ´equivaut plus ou moins `a minimiser le temps de transmission du paquet
(mais on ne tient pas en compte le temps de propagation !). Pour faire cela, on essaye
de minimiser l’utilisation du lien, c’est-`a-dire l’inverse de sa capacit´e (cf. slide 4-80 pour
la d´emonstration math´ematique). Notons que 1/Ci est l’inverse de la capacit´e d’un lien,
et correspond `a des secondes par byte ; c’est donc d’une certaine mani`ere une estimation
du temps que va passer un paquet sur chaque lien. Notons que la formule exacte devrait
ˆetre L/Ci , mais comme L est constant et qu’on veut juste trouver le minimum (pas une
valeur exacte), on peut l’enlever de l’´equation.
M´
etriques temps-r´
eel Vous remarquerez que toutes ces m´etriques sont statiques,
c’est-`a-dire qu’elles ne changent pas dans le temps. En utilisant la matrice de trafic, on
pourrait imaginer de mettre en place des m´etriques temps r´eel : `a chaque transmission de
paquet, la matrice de trafic est mise `a jour et les poids sont re-calcul´es. Dans la pratique,
on ne fait jamais cela car cela augmente inutilement la quantit´e de donn´ees a` s’´echanger
(il faut propager la mise a` jour dans tout le r´eseau) et le temps d’ex´ecution (il faut sans
cesse recalculer les poids des arcs et les plus courts chemins).

40

CHAPITRE 4. Couche r´eseau
Les types d’algorithmes
Comme on l’a d´ej`a dit dans l’introduction, nous allons voir deux types d’algorithmes :
link state et distance vector.
Link State est une famille de protocoles o`
u chaque noeud connaˆıt l’´etat de tout le
graphe, et peut donc calculer le plus court chemin jusqu’`a la destination. Le graphe
est construit et mis a` jour de mani`ere incr´ementale, de mˆeme que les plus courts
chemins.
Distance Vector est une famille de protocoles o`
u le noeud ne connaˆıt bien que ses
voisins ; il n’a qu’une connaissance vague du reste du graphe : il connaˆıt la distance
qui le s´epare de chaque noeud, mais pas le chemin a` emprunter. On verra que les
algorithmes a` vecteur de distance utilisent l’´equation de Bellman-Ford pour mettre
a` jour le graphe et trouver le plus court chemin.
Rappelons enfin que les algorithmes de plus court chemin (au moins ceux utilis´es ici)
construisent un arbre des plus courts chemins. Il s’agit de l’arbre sous-tendant au graphe,
mais ne gardant que les arcs appartenant au plus court chemin vers chaque destination,
depuis un noeud donn´e (la racine de l’arbre). Pour plus d’informations, retournez lire
votre syllabus d’Algorithmique 2 :)

4.4.2

Algorithmes `
a´
etat de lien

Les algorithmes a` ´etat de lien fonctionnent en deux temps :
1. Chaque noeud transmet a` tout le r´eseau l’´etat des liens entre lui et ses voisins
2. Sur base des informations re¸cues des autres noeuds, chaque noeud calcule les plus
courts chemins vers tous les autres noeuds et g´en`ere une table d’acheminement
Premi`
ere ´
etape
Chaque noeud envoie `a tous ses voisins un paquet d´ecrivant l’´etat de ses liens (d’o`
u le
nom de l’algorithme). Ce paquet contient la source (donc le nom du routeur qui a ´emis le
paquet), un num´ero de s´equence qui augmente `a chaque mise `a jour de l’´etat de lien (ces
paquets sont envoy´es r´eguli`erement, par exemple toutes les 10 secondes), une information
d’ˆage qui permet d’´eviter les probl`emes en cas de crash (on y reviendra), et finalement
l’information v´eritablement int´eressante : une table avec tous les voisins de la source et
la distance entre le noeud et chacun de ses voisins.
Il y a comme d’habitude un probl`eme a` r´esoudre : le cas o`
u il y a un cycle dans le
r´eseau. Admettons par exemple que trois routeurs A, B et C sont reli´es entre eux. Si on
ne fait rien, A va envoyer ses informations a` B et C, B va les transmettre a` C, et C va les
re-transmettre `a A, et ainsi de suite. Il faut donc s’arranger pour ´eviter qu’un paquet ne
revienne `a sa source via un cycle. Pour cela, on va utiliser un packet buffer repr´esent´e a`
la slide 4-87. Ce buffer contient, pour chaque noeud source, le num´ero de s´equence le plus
´elev´e qu’il a re¸cu, l’ˆage de l’information, une liste de flags indiquant `a qui il faut encore
transmettre l’information, et une liste de flags indiquant a` qui il faut envoyer un ACK.
– Le num´ero de s´equence permet pr´ecis´ement d’´eviter les probl`emes de cycles. En
effet, ce num´ero de s´equence est incr´ement´e `a chaque nouvel envoi. Par cons´equent,
41

CHAPITRE 4. Couche r´eseau
si on re¸coit un paquet avec un num´ero de s´equence plus petit que celui qu’on a
sauvegard´e, c’est qu’on vient de recevoir un vieux paquet qui a fait un (ou plusieurs)
tours de boucle. Dans ce cas, on ”jette” simplement le paquet ; on ne s’en occupe
plus puisqu’on a re¸cu une information plus r´ecente.
– L’information d’ˆage permet d’´eviter les probl`emes en cas de crash d’un des routeurs.
En effet, si un routeur plante et doit red´emarrer, il ne se souvient plus de son
num´ero de s´equence, donc il recommence a` 0. Si son ancien num´ero de s´equence
´etait 40 000 et qu’on n’a pas d’information d’ˆage, les autres routeurs vont alors
ignorer le routeur pendant 40 000 envois (puisque le nouveau num´ero de s´equence
est plus petit que l’ancien). Pour ´eviter cela, chaque routeur donne un certain aˆge
a` chacune des entr´ees de sa table (par exemple 60 secondes). A chaque seconde,
l’ˆage de chacune de ces entr´ees est d´ecr´ement´e ; lorsqu’un aˆge atteint 0, l’entr´ee est
supprim´ee. De cette mani`ere, si un routeur plante alors qu’il avait le num´ero de
s´equence 40 000, il ne sera ignor´e que pendant au plus 60 secondes, le temps que
son ancienne information atteigne son ˆage maximum et soit supprim´ee.
– La liste de flags sert a` indiquer quelles sont les op´erations restant `a effectuer a` un
moment donn´e. Au moment o`
u on re¸coit un paquet d’´etat de lien, on met le send
flag de tous les routeurs sauf la source a` 1, de mˆeme que le ACK flag de la source.
A chaque fois qu’une op´eration est termin´ee, on passe le flag correspondant a` z´ero.
Cela explique pourquoi il faut mettre les deux champs, alors qu’`a premi`ere vue ils
sont compl´ementaires l’un de l’autre.
De cette mani`ere, grˆace aux paquets d’´etat de lien, on a pu cr´eer un graphe repr´esentant
le r´eseau ; le m´ecanisme compliqu´e des packet buffer permettant juste de s’assurer qu’on
ne va pas ”tourner en rond” dans un cycle. Maintenant qu’on a un graphe, il ne nous
reste plus qu’`a trouver les plus courts chemins ! 7
Deuxi`
eme ´
etape
Pour la deuxi`eme ´etape, on utilise l’algorithme de Dijkstra permettant de g´en´erer
une table d’acheminement pour chaque destination. Autant le dire tout de suite : cette
technique fonctionne bien sur des petits r´eseaux, mais vu la complexit´e de Dijkstra (au
mieux du O(n log n)), il n’est pas vraiment applicable a` de gros r´eseaux ... D’o`
u l’invention
des algorithmes a` vecteur de distance !
Mais bon, comme link state est utilis´e dans les petits r´eseaux, nous allons quand mˆeme
voir vite fait son fonctionnement. Il s’agit d’un algorithme it´eratif qui, `a la k-`eme ´etape,
permet de connaˆıtre la distance vers k destinations.
La premi`ere ´etape consiste a` attribuer une distance de la source vers chacun des autres
noeuds. Si la destination n’est pas adjacente `a la source, on lui attribue une distance
infinie. On ajoute ensuite u (le noeud-source, cf algorithme de la slide 4-90) `a la liste des
noeuds dont la distance minimale est connue (ensemble N dans les slides).
Ensuite, a` chaque it´eration, on prend un noeud dans la frange (noeud qui n’a pas
encore ´et´e trait´e et qui se trouve `a la distance minimum de la source). On l’ajoute a` N
7. Attention, ici on pr´esente ces deux ´etapes de mani`ere cons´ecutive, mais dans la r´ealit´e elles ont
lieu en continu ; les paquets d’´etat de lien sont envoy´es r´eguli`erement, par ex. toutes les 10 secondes, et
l’arbre des plus courts chemins est re-calcul´e r´eguli`erement aussi. De cette mani`ere, les routeurs vont
naturellement s’adapter si un lien tombe en panne ou un nouveau lien est mis en service. On en reparlera
dans les probl`emes de convergence.

42

CHAPITRE 4. Couche r´eseau
et on met a` jour la distance vers ses voisins. On proc`ede de la sorte jusqu’`a ce que tous
les noeuds aient ´et´e trait´es. De cette mani`ere, au final, on a pour chaque destination la
distance minimum, et le voisin a` qui il faut transmettre le paquet pour qu’il atteigne sa
destination 8 .

4.4.3

Algorithmes `
a vecteur de distance

Ces algorithmes utilisent cette fois l’´equation de Bellman-Ford. Pour rappel, cette
´equation dit que la distance minimale entre un noeud x et un noeud y vaut la distance
minimale entre x et un noeud z sur ce chemin, plus la distance minimale entre z et y.
Du coup, cela nous permet de simplifier pas mal la recherche du plus court chemin :
au final, tout ce qu’un routeur veut savoir, c’est a` quel voisin il doit transmettre son
paquet. Il suffit d`es lors de prendre l’´equation de Bellman-Ford avec z parmi les voisins
de la source x. Probl`eme r´esolu, `a condition qu’on puisse calculer de mani`ere efficace la
distance qui s´epare chaque noeud z de la destination y 9 .
Pour connaˆıtre de mani`ere efficace la distance entre chaque voisin v 10 et la destination
y, on ne va pas calculer la valeur exacte mais plutˆot une estimation (n´ecessaire car au
d´ebut, on ne connaˆıt aucune distance de mani`ere exacte). On va stocker ces distances
dans un vecteur (... d’o`
u le nom de l’algorithme), et on va aussi stocker le vecteur de
distance de chaque voisin direct. Du coup, on peut appliquer Bellman-Ford ”tel quel”
pour trouver les distances vers chaque destination.
Pour que l’initialisation et la mise a` jour de l’´etat du r´eseau puissent se faire correctement, chaque noeud va par ailleurs renvoyer r´eguli`erement son propre vecteur de distance
a` ses voisins. Le plus court chemin est alors re-calcul´e avec la formule habituelle. Si aucune des distances n’a chang´e avec la mise `a jour, rien ne se passe. Sinon, le routeur doit
encore envoyer son nouveau vecteur de distances a` ses voisins pour que la modification
puisse se propager.
Probl`
emes de convergence
Cet algorithme fonctionne bien, y compris dans des r´eseaux de grande taille, mais pose
quand mˆeme un l´eger probl`eme : si la distance entre les noeuds diminue 11 , l’information
sera tr`es vite propag´ee dans les diff´erents noeuds ; par contre, si un lien est rompu (un
routeur tombe en panne par ex.), la mise a` jour va mettre tr`es longtemps a` se propager.
Pour r´esoudre cela, on a invent´e une technique qui fonctionne plus ou moins : poisoned
reverse (aussi appel´ee split horizon). L’id´ee est la suivante :

8. C’est un Dijkstra classique en fait, mais vu du point de vue d’un noeud et pas du graphe dans
son ensemble. A nouveau, je vous renvoie `a votre syllabus d’Algorithmique 2 pour les probl`emes de
compr´ehension de l’algorithme.
9. Vous remarquerez qu’on n’a pas besoin de connaˆıtre le reste du chemin mais uniquement la distance
a parcourir ; cela correspond bien `
`
a la d´efinition de DV que l’on avait donn´e dans l’introduction.
10. Appelons-le v pour rester coh´erent avec les slides. Il s’agit bien sˆ
ur du mˆeme noeud qu’on avait
appel´e z dans l’introduction.
11. Grˆ
ace `
a la mise en service d’un nouveau noeud par exemple

43

CHAPITRE 4. Couche r´eseau
Prenons trois noeuds A, B et C, tels que repr´esent´es `a la slide 4-104, et reproduits a`
la figure 4.1.

Figure 4.1 – Exemple de topologie pour poisoned reverse
Si le noeud C sait qu’il utilise B pour envoyer ses paquets vers A, lorsqu’il envoie ses
vecteurs de distance, il va ”mentir” a` B. Au lieu de lui donner la v´eritable valeur qui le
s´epare de A, il lui donne une valeur infinie 12 . De cette fa¸con, on est sˆ
ur que B ne va pas
essayer de router des paquets pour A via C. En effet :
– Si il existe un lien entre A et B, B va garder son vecteur de distances comme avant,
puisque le ”faux” vecteur de distance de C ne change rien (B est forc´ement `a une
distance non-infinie de A, donc pas de mise a` jour)
– Au contraire, si le lien entre A et B est rompu, B va chercher un autre chemin. Le
seul qui se pr´esente a` lui est celui qui passe par C mais qui a une valeur infinie.
Comme il n’y a pas d’autre alternative, B va alors mettre sa distance `a ∞, et
propager la mise a` jour `a ses voisins. Mission accomplie, la mise `a jour s’est faite
en une seule it´eration pour le noeud B ! 13
Cette technique un peu exotique marche bien dans le cas o`
u il n’y a pas de cycles. S’il
existe plus d’un chemin pour relier deux noeuds, par contre, le comportement devient un
peu chaotique. Prenons le sch´ema de la slide 4-105, reproduit avec amour a` la figure 4.2
(le lien de C a` D est rompu). Au d´epart, A et B savent qu’ils sont `a une distance 2 de
D en passant par C. Lorsque le lien C − D disparaˆıt, C pr´evient alors les deux noeuds
qu’il est a` une distance ∞ de D.
Regardons maintenant ce qui se passe dans le noeud A. A veut mettre `a jour son
vecteur de distances. Pour cela, il a le vecteur de C qui lui dit qu’il est `a une distance ∞,
et le vecteur de B qui lui dit qu’il est `a une distance 2 (ancienne valeur qui n’a pas encore
´et´e mise a` jour). A va alors choisir la plus petite de ces deux valeurs, c’est-`a-dire 2. Il va
donc ”croire” qu’il se trouve a` une distance 3 de D en passant par B. De son cˆot´e, B suit
le mˆeme raisonnement. Seulement, comme A a chang´e son vecteur de distances, il avertit
ses voisins. Il donne un ”faux” vecteur a` B selon le principe du poisoned reverse, donc B
n’est pas mis `a jour et tout va bien. Par contre, comme il ne sait pas que C fait partie
du chemin qu’il envisage, il va lui transmettre le ”vrai” vecteur. C va donc comparer la
distance qui le s´epare de D (∞) avec celle qui lui propose A (3), et va prendre la plus
petite. Maintenant, C croit qu’il se trouve a` une distance 4 de D en passant par A, va
mettre `a jour son vecteur de distances, et ainsi de suite.
12. Par contre, C donne la vraie valeur a` ses autres voisins !
13. L’information se propage ensuite de la mani`ere habituelle : C se rend compte qu’il est maintenant
a une distance ∞ de A, va mettre `
`
a jour son propre vecteur de distance et l’envoyer `a ses voisins, et ainsi
de suite.

44

CHAPITRE 4. Couche r´eseau

Figure 4.2 – Situation probl´ematique pour poisoned reverse
R´esultat des courses : comme il n’y a jamais un moment o`
u les trois noeuds se rendent
compte en mˆeme temps qu’ils sont isol´es de D, l’algorithme va continuer `a cycler. Dans
ce cas de figure, poisoned reverse ne fonctionne pas correctement et c’est bien dommage.
Il n’y a malheureusement pas grand chose a` faire, aucun algorithme plus efficace n’a ´et´e
trouv´e jusqu’`a pr´esent.

4.5
4.5.1

Passage `
a l’´
echelle d’Internet
Le routage hi´
erarchique

Il y a un gros probl`eme avec Internet : c’est un r´eseau ´enorme, avec plein de destinations possibles. Par cons´equent, on ne peut pas imaginer d’utiliser platement les
algorithmes vus ci-dessus pour une raison toute simple : un routeur ne sait pas stocker
toutes les destinations possibles, et quand bien mˆeme il saurait, le temps de calculer les
plus courts chemins serait beaucoup trop long. Il faut donc trouver une solution.
Comme vous devriez le savoir, Internet est en fait compos´e de r´eseaux connect´es entre
eux (je vous invite a` relire la section 1.5 si vous n’ˆetes pas d’accord). Par la suite, nous
allons appeler AS (pour Autonomous System) un de ces r´eseaux. Un ISP, par exemple,
forme un AS. On dira que ces diff´erents AS sont connect´es entre eux via des gateway
(”passerelles”) ; ce sont les routeurs qui disposent d’au moins une connexion avec un
autre AS.
En quoi cela r´esoud-il le probl`eme ? On va en fait appliquer un principe bien connu
des informaticiens : l’encapsulation. De fait, pour calculer le chemin global que va devoir
parcourir un paquet, on va uniquement d´ecider par quels AS ce paquet va devoir passer.
La mani`ere dont chaque AS fait traverser un paquet `a travers son r´eseau est laiss´ee
libre ; autrement dit, chacun fait ce qu’il veut chez lui tant que le paquet est envoy´e
vers l’AS suivant. On va appeler protocole inter-AS le protocole qui d´etermine via quels
AS un paquet doit passer 14 , tandis qu’un protocole intra-AS est le protocole qu’un AS
14. Donc le protocole inter-AS d´efinit aussi pour chaque AS quel est le gateway de sortie vers les

45

CHAPITRE 4. Couche r´eseau
particulier d´ecide d’impl´ementer pour acheminer les paquets vers le bon gateway 15 . Cette
abstraction est repr´esent´ee `a la figure 4.3.

Figure 4.3 – Abstraction inter-AS (`a gauche) et intra-AS (`a droite)
Notons par ailleurs que les routeurs doivent donc impl´ementer deux protocoles : un
protocole intra-AS (pour savoir `a quel routeur de leur r´eseau ils doivent transmettre les
paquets entrants), et un protocole inter-AS (pour savoir vers quel autre r´eseau les paquets
sortants vont devoir passer, donc vers quel gateway il faut les router).
Du coup, le probl`eme de l’acheminement est beaucoup plus simple. En effet, `a l’´echelle
d’Internet, on dispose d’un graphe dont chaque noeud est un AS, et chaque arc est un lien
existant entre deux AS 16 . Le poids attribu´e `a ces arcs peuvent refl´eter soit de contraintes
techniques (un lien transatlantique aura un coˆ
ut plus important qu’un lien courte distance), ou ´economiques (un ISP peut par exemple favoriser le trafic ”qui rapporte”,
c’est-`a-dire celui venant d’un r´eseau qui le paye pour le trafic engendr´e)

4.5.2

Le routage intra-AS : RIP

On va commencer par pr´esenter un protocole de routage intra-AS car ce sont les
plus simples. RIP (Routing Information Protocol ) est un de ces protocoles ; c’est celui
impl´ement´e par d´efaut dans Unix, et qui est donc massivement utilis´e aujourd’hui. Il s’agit
d’un protocole bas´e sur le principe ”hot potato” : quand on re¸coit un paquet, on essaye
de s’en d´ebarrasser le plus vite possible, c’est-`a-dire en passant par un minimum de liens
(on adopte une m´etrique unitaire, cf section 4.4.1). Pour cela, on utilise un algorithme a`
destinations en-dehors de ce r´eseau
15. Donc, le protocole inter-AS est impos´e, tandis que le choix du protocole intra-AS est laiss´e libre `
a
chaque AS
16. Ils peuvent passer par deux gateways diff´erents au sein du mˆeme AS, ce n’est pas le probl`eme du
protocole inter-AS de le savoir.

46

CHAPITRE 4. Couche r´eseau
vecteur de distances tout a` fait classique, a` la diff´erence qu’on calcule une distance vers
un sous-r´eseau et pas un routeur directement 17 .
Petite remarque : comme tout protocole, RIP est g´er´e par la couche applicative (pour
la mise `a jour des tables), les vecteurs de distance ´etant envoy´es dans des paquets UDP.
Notons aussi qu’il existe un protocole a` ´etat de liens, nomm´e OSPF, qui a le mˆeme but.
Cependant, on n’en parlera pas ici tout simplement parce qu’il ne fait pas partie de la
mati`ere.

4.5.3

Le routage inter-AS : BGP

Pour terminer, il ne nous reste plus qu’`a voir comment les AS communiquent entre eux
et choisissent un itin´eraire global. Ceci est fait grˆace `a BGP (Border Gateway Protocol ).
Selon ce protocole, chaque AS fournit des ”promesses” aux autres AS, c’est-`a-dire que
chaque AS indique quel r´eseau il sait joindre, et a` quel coˆ
ut avec un ”genre de” vecteur de
distances. Grˆace a` ces informations, les AS peuvent alors d´eterminer le chemin optimal
(c’est-`a-dire quels AS le paquet devra traverser pour atteindre sa destination). Comme
pour les protocoles intra-domaines, BGP appartient en tant que tel a` la couche applicative,
et utilise TCP pour ses communications (ou, pour ˆetre plus pr´ecis, les gateways des
diff´erents r´eseaux utilisent TCP pour s’´echanger les informations de connectivit´e). Pour
terminer avec l’introduction, notons encore qu’en fait BGP est divis´e en deux protocoles :
eBGP (external BGP ) qui correspond `a la communication d’un r´eseau vers un autre, et
iBGP (ou internal BGP ) qui sert pour la propagation de l’information au sein du r´eseau.
Lorsqu’un AS envoie ses informations de connectivit´e, il l’enverra donc aux gateways de
ses voisins via eBGP. Ensuite, chaque voisin va propager l’information sur son r´eseau
via iBGP, pour que les routeurs internes au r´eseau puissent mettre a` jour leur table
d’acheminement (ils doivent savoir a` quel gateway amener les paquets dont la destination
est en-dehors de leur r´eseau). De cette mani`ere, chaque routeur peut avoir une table
d’acheminement mise a` jour avec les informations correctes 18
Contenu d’un message eBGP
Les messages eBGP contiennent principalement deux informations importantes, en
plus du sous-r´eseau `a atteindre (appel´e le pr´efixe) :
L’AS-PATH , qui est la liste des AS a` traverser pour atteindre la destination. BGP est
donc un ”genre de” protocole a` vecteur de distances, sauf qu’au lieu d’enregistrer la
distance vers chaque destination, on enregistre plutˆot les AS a` traverser. On appelle
cela un protocole a` vecteur de chemins.
Le NEXT-HOP , c’est-`a-dire le routeur de sortie de l’AS qui m`ene `a l’AS suivant.
Lorsqu’un paquet entre dans l’AS, cela permet donc de d´eterminer vers quel gateway
de sortie il faudra l’acheminer. Cette information est utile car en pratique, il y a
17. Notez que ¸ca ne change pas grand-chose : au niveau intra-AS, un sous-r´eseau peut ˆetre repr´esent´e
par son gateway puisqu’on se fiche de savoir ce qui se passe derri`ere ; on a transmis le paquet au bon
r´eseau donc on est content.
18. Les routeurs internes au r´eseau ont une table d’acheminement indiquant vers quel gateway amener
les paquets pour chaque destination ; les gateway ont une table d’acheminement leur permettant de
d´ecider quel est l’AS suivant `
a atteindre.

47

CHAPITRE 4. Couche r´eseau
souvent plusieurs liens entre les diff´erents r´eseaux (question de redondance : ce
serait dommage que tout un AS ne soit plus joignable si un seul gateway tombait
en panne) ; le Next-Hop permet donc de d´eterminer quel est le lien qui sera utilis´e
Le filtrage `
a l’importation
Comme on vient de le dire, les messages de BGP contiennent la liste des AS a` traverser
pour chaque destination (AS-PATH). Cela offre un ´enorme avantage face aux autres
algorithmes vus jusqu’`a pr´esent : on sait ´eviter les cycles et les probl`emes de convergence
tr`es facilement (ce qui signifie qu’on n’a pas besoin de poisoned reverse `a ce niveau, et
c’est une bonne nouvelle vu les probl`emes qu’il pose ...). En effet, imaginons que nous
soyons un gateway de l’AS F . On re¸coit donc de la part de nos voisins leurs vecteurs
de chemins pour les diff´erents AS. Maintenant, deux situations sont possibles pour une
destination donn´ee, appelons-la D 19 :
1. F n’apparaˆıt pas dans l’AS-PATH. Tout va bien, les gateways de F vont enregistrer
l’AS-PATH vers D. Lorsqu’un paquet arrivera a` destination de D, ils vont donc ˆetre
rout´es selon ce chemin.
2. F apparaˆıt dans l’AS-PATH. Dans ce cas, on jette l’information. En effet, si on
avait gard´e cet AS-PATH, cela aurait cr´e´e un cycle. Imaginons qu’on ait gard´e
l’AS-PATH, et qu’un paquet arrive `a destination de D. Le gateway l’aurait rout´e
selon son AS-PATH, donc le paquet repasserait par F avant d’atteindre D. Non
seulement cela ajoute de la charge inutile sur le r´eseau, mais en plus ne pas le faire
permet d’´eviter les probl`emes de convergence. Voir contre-exemple de la Figure 4.4

Figure 4.4 – Le routeur E envoie l’AS-PATH EF D (en rouge). Si F ne fait rien, son
AS-PATH sera F EF D (en vert), donc les paquets passant par F cycleront entre E et F .

Le filtrage `
a l’exportation
C’est l’inverse du filtrage `a l’importation : un AS choisit (d´elib´er´ement) de ne pas
avertir certains de ses voisins qu’une connexion sp´ecifique existe. Contrairement au filtrage
a` l’importation, cette technique n’existe pas pour des raisons techniques, mais purement
´economiques : si un AS paye pour le trafic qu’il engendre chez un autre, il va en toute
19. Ces noms, choisis tout `
a fait au hasard, correspondent `a ceux de la slide 4-134, o`
u vous avez un
sch´ema de la topologie du r´eseau.

48

CHAPITRE 4. Couche r´eseau
logique essayer de g´en´erer le moins de trafic possible, et va donc s’arranger pour que
les paquets des autres r´eseaux ne passent pas chez lui. Cette technique ne demande pas
d’algorithmes compliqu´es : il suffit juste de choisir pr´ecautionneusement `a qui on envoie
quelles informations, et les autres r´eseaux n’y verront que du feu !
Le choix du plus court chemin
Grˆace au filtrage a` l’importation et a` l’exportation, on peut d´ej`a ´eliminer toute une
s´erie de chemins : ceux qui comportent un cycle, et ceux qui ont ´et´e volontairement
cach´es. Il n’empˆeche qu’il peut encore y avoir plusieurs possibilit´es (si un AS peut ˆetre
atteint par plusieurs chemins diff´erents, sans cycles). Comme on n’a pas d’information de
distance, il va falloir choisir un chemin selon un autre crit`ere :
1. Le premier crit`ere est un choix arbitraire : on peut choisir de favoriser un AS plutˆot
qu’un autre, pour des raisons techniques ou ´economiques ; on en a d´ej`a parl´e.
2. Si tous les chemins sont indiff´erents, on choisi le chemin le plus court, c’est-`a-dire
celui qui passe par le moins d’AS possible
3. Enfin, si tous les chemins ont la mˆeme longueur, on choisit celui dont le Next-Hop
est le plus proche, c’est-`a-dire qu’on essaye de se d´ebarrasser du paquet le plus
vite possible (c’est le principe de ”hot potato” dont on a d´ej`a parl´e : on redirige le
paquet vers le gateway le plus proche possible histoire que ce paquet ne vienne pas
encombrer notre r´eseau)
Voil`a qui conclut ce chapitre sur la couche r´eseau (le reste du protocole BGP est tout
a` fait semblable `a un bˆete algorithme `a vecteur de chemins). Il ne nous reste donc plus
qu’`a nous plonger dans l’avant-derni`ere couche du mod`ele OSI : la couche de liaison.

49

Chapitre 5
Couche de liaison
5.1

Introduction

Nous voici maintenant `a la derni`ere couche couverte par le cours, qui est aussi l’avantderni`ere du mod`ele OSI 1 .
Comme on l’a dit dans le chapitre pr´ec´edent, la couche r´eseau s’occupe d’acheminer
les paquets a` bon port, ce qui implique donc de r´esoudre les probl`emes de routage. Il reste
encore un probl`eme `a r´esoudre : le forwarding (ou ”acheminement” en fran¸cais correct).
Le probl`eme de forwarding consiste `a trouver l’´el´ement suivant d’un chemin sur un sousr´eseau. Pour bien faire la distinction entre ces deux couches, prenons un r´eseau Wi-Fi
style Urbizone. Lorsque vous ˆetes connect´e a` ce r´eseau Wi-Fi, les paquets sont achemin´es
vers le routeur Urbizone par la couche r´eseau. Il reste encore un probl`eme a` r´esoudre :
comment le routeur fait-il pour vous transmettre le paquet a` vous pr´ecis´ement, et pas
a` votre voisin qui est connect´e au mˆeme r´eseau ? C’est la raison d’ˆetre principale de la
couche de liaison 2 !
Par la suite, on va parler indiff´eremment de ”machine” ou de ”noeud” pour d´esigner
n’importe quel ´el´ement qui impl´emente au moins la couche r´eseau.
Comme d’habitude, avant d’entrer dans le vif du sujet, nous allons introduire quelques
notions de base.
– Un lien est ... Un lien entre plusieurs machines. Notons qu’un lien peut avoir plus de
deux extr´emit´es : c’est le cas des r´eseaux sans-fil par exemple (qui ont une extr´emit´e
par machine connect´ee), mais aussi d’autres types de liens dont on parlera en temps
voulu (on peut d´ej`a citer les bus, les switches et les hubs).
– La structure de donn´ees ´echang´ee par la couche de liaison s’appelle la trame (frame
en anglais). Elle se compose du paquet de la couche r´eseau auquel on a ajout´e des
adresses MAC, qui identifient de mani`ere univoque la source et la destination.
– Une adresse MAC est un nombre encod´e sur 48 bits qui permet d’identifier une interface. Il ne faut pas la confondre avec les adresses IP, car ces deux types d’adresses
ont plusieurs diff´erences :
– La premi`ere diff´erence, ´evidente, est qu’elles ne font pas partie de la mˆeme couche
1. Vous ˆetes sˆ
urement tr`es d´e¸cu, mais la couche physique n’est pas abord´ee ici. Cela dit, vous pouvez
toujours en avoir un avant-goˆ
ut avec le chapitre 1 si vous le voulez
2. Notez qu’on a pris ici un exemple o`
u la destination ´etait un end system. En r´ealit´e, la couche de
liaison apparaˆıt sur chaque lien, y compris donc entre deux routeurs par exemple.

50

CHAPITRE 5. Couche de liaison
et n’ont donc pas le mˆeme rˆole. L’adresse IP appartient a` la couche r´eseau, tandis
que l’adresse MAC appartient a` la couche de liaison.
– Une adresse MAC identifie une, et une seule, interface. Contrairement a` IP, deux
machines ne partageront donc jamais la mˆeme adresse MAC.
– L’adresse MAC est encod´ee dans le hardware, et il est th´eoriquement impossible de la changer (en pratique, des moyens existent, mais ce n’est vraiment pas
conseill´e de le faire). Chaque carte r´eseau re¸coit donc une adresse MAC a` sa
construction, et la garde pour l’´eternit´e.

5.2

D´
efinition du protocole de liaison

Le protocole de liaison (typiquement Ethernet, mais d’autres existent) est impl´ement´e
en partie en hardware, sur la carte r´eseau directement, et en partie en software, via des
drivers par exemple. Il doit fournir les services suivants :
D´
etection et correction des erreurs Sur les liens peu fiables (typiquement le Wi-Fi,
mais pas seulement), les erreurs de transmission sont tr`es fr´equentes. On pourrait
ne pas s’en soucier et laisser aux couches du dessus le plaisir de faire les v´erifications
n´ecessaires a` la r´eception d’un paquet, mais pour des questions d’optimisation, on
pr´ef`ere v´erifier le plus vite possible que la transmission s’est bien d´eroul´ee. En effet,
des collisions entre paquets et autres erreurs arrivent extrˆemement souvent, donc
s’amuser a` transmettre sur le r´eseau (´eventuellement plusieurs fois en cas d’´echecs
r´ep´et´es) des paquets qui ´etaient corrompus d`es le d´epart serait une pure perte de
performances.
Partage ´
equitable des ressources Pour ´eviter qu’une des machines ne monopolise le
lien et que les autres ne puissent jamais rien transmettre.
Gestion des acc`
es multiples Dans le cas d’un lien a` plusieurs extr´emit´es, il se pourrait
que deux machines se mettent a` envoyer une trame en mˆeme temps. Comme le lien
n’est pas pr´evu pour cela, il y a alors collision (les deux paquets se ”m´elangent”) et
les informations sont perdues. Il faut essayer de limiter cette situation au maximum,
et trouver un moyen d’y rem´edier quand elle arrive.

5.3

D´
etection et correction des erreurs

On en a d´ej`a parl´e dans la section 3.2 et dans l’annexe A.1 ; le principe de d´etection
d’erreurs est le mˆeme ici que dans la couche de transport. Dans la trame, on peut ajouter
un code d´etecteur d’erreur calcul´e avant l’envoi. A la r´eception, on va alors re-calculer ce
code et le comparer avec celui fourni ; si les deux correspondent, on est content, sinon,
c’est qu’il y a eu une erreur lors de la transmission, et on d´ecide de jeter la trame.
Cela dit, on pourrait utiliser une ”bˆete” checksum comme en couche de transport.
Seulement, comme les erreurs sur le lien sont assez fr´equentes, on peut plutˆot essayer
d’optimiser un peu et introduire plutˆot des codes correcteurs d’erreurs. Ce sont des codes
qui, non seulement, d´etectent qu’une erreur a eu lieu dans le paquet, mais peuvent aussi
d´eterminer a` quel endroit pr´ecis´ement. Il ne reste alors plus qu’`a corriger l’erreur (inverser

51

CHAPITRE 5. Couche de liaison
les bits erron´es), et le probl`eme est r´esolu. Plus d’informations sur les codes correcteurs
d’erreur sont donn´es en annexe 3 .
Cela dit, impl´ementer un code correcteur d’erreur est assez lourd, tant en puissance
de calcul qu’en quantit´e d’informations `a ´echanger. D`es lors, on ne va pas les impl´ementer
syst´ematiquement : g´en´eralement, on choisit d’impl´ementer un code correcteur d’erreur
sur un canal dont on sait que des erreurs ont lieu fr´equemment (Wi-Fi par exemple n’est
vraiment pas fiable ; impl´ementer un tel code pour un lien en fibre optique a beaucoup
moins de sens vu qu’il n’y a que tr`es peu d’erreurs).

5.4

Gestion des acc`
es multiples

Dans la couche de liaison, il y a lieu de faire la distinction entre deux types de liens :
les liens dits point-`a-point (point-to-point ou PPP ) qui sont des liens a` deux extr´emit´es,
et les liens a` acc`es multiple o`
u l’information est diffus´ee vers tous les syst`emes connect´es
au lien (avec filtrage ´eventuellement). Ici, nous allons uniquement nous concentrer sur les
protocoles `a acc`es multiples vu que ce sont ceux qui posent le plus de probl`emes.
Le probl`eme des liens a` acc`es multiples est ... que tout le monde y a acc`es en mˆeme
temps. Il va donc falloir trouver un moyen de se synchroniser pour que deux machines
ne commencent pas a` utiliser le lien au mˆeme moment, sans quoi il pourrait y avoir une
collision entre les deux paquets envoy´es. Pour cela, on aimerait bien ne devoir utiliser
que le lien lui-mˆeme (il n’est pas imaginable d’utiliser un autre moyen de communication pour se synchroniser), et partager le lien de mani`ere ´equitable (de telle mani`ere que
les diff´erents syst`emes puissent utiliser le lien avec la mˆeme efficacit´e). De plus, le protocole doit ´evidemment ˆetre d´ecentralis´e (on n’a pas une machine ”maˆıtre” qui donne
explicitement la parole aux autres).

5.4.1

Protocoles `
a partitionnement

Partitionnement en fonction du temps (TDMA)
Il s’agit d’une famille de protocoles basique, dont le fonctionnement a d´ej`a ´et´e abord´e
dans l’introduction (section 1.4.2). Ces protocoles fonctionnent de la mani`ere suivante :
le temps est s´epar´e en ”slots” d’une dur´ee T , et chaque machine a acc`es `a la ressource a`
tour de rˆole pendant le temps T . Par cons´equent, si on a 6 machines sur le lien, chaque
machine n’a acc`es au lien que pendant T millisecondes toutes les 6T millisecondes. Le
principal probl`eme de ce type de protocole est ´evident : si une machine n’envoie rien
pendant son slot, celui-ci est ”perdu”, donc le lien n’est pas exploit´e au maximum de sa
capacit´e.
Partitionnement en fonction de la fr´
equence (FDMA)
L’id´ee est la mˆeme, sauf que cette fois nous allons diviser la ressource en bandes de
fr´equences et plus en temps. Autrement dit, chaque machine peut envoyer des donn´ees
tout le temps, mais elle ne peut utiliser qu’une certaine bande de fr´equences (la machine
3. Seuls les codes `
a bit de parit´e son expliqu´es. Les slides du cours mentionnent ´egalement CRC, mais
ce code n’entre pas dans la mati`ere ; il n’est donc pas expliqu´e ici.

52

CHAPITRE 5. Couche de liaison
1 utilise la bande allant de 0 kHz `a x kHz, la machine 2 celle allant de x + 1 kHz a` 2x
kHz, et ainsi de suite). Notons que la bande de fr´equences utilis´ee est directement li´ee au
d´ebit disponible ; par cons´equent, le probl`eme est exactement le mˆeme que pour TDMA :
si une machine n’utilise pas sa bande de fr´equences, celle-ci est perdue, et le d´ebit pour
les autres machines est limit´e inutilement.
Conclusion
En conclusion, ces protocoles ont du bon et du moins bon. Le bon cˆot´e, c’est qu’on
est sˆ
ur que chaque machine aura acc`es au lien de la mˆeme mani`ere que ses voisines. De
plus, on est sˆ
ur qu’il n’y aura jamais de collision entre deux paquets puisqu’ils ne sont pas
envoy´es en mˆeme temps, ou pas `a la mˆeme fr´equence. En revanche, le gros d´esavantage
est que si toutes les machines n’utilisent pas le lien `a fond tout le temps, celui-ci sera
sous-exploit´e.

5.4.2

Protocoles `
a acc`
es al´
eatoire

Principe de base
Les protocoles `a acc`es al´eatoire font en fait exactement l’inverse des protocoles `a
partitionnement. Autrement dit, tout le canal est mis `a la disposition de tout le monde,
et lorsque quelqu’un veut envoyer quelque chose, il utilise le canal au maximum, sans se
soucier de savoir si d’autres sont d´ej`a occup´es 4 . Du coup, il y a de tr`es fortes chances
que des collisions aient lieu ; dans ce cas, les protocoles `a acc`es al´eatoire sp´ecifient ce qu’il
faut faire pour les d´etecter et les ´eviter par la suite.
Petite note Cette section est typiquement une question d’examen. A bon entendeur
...
ALOHA discr´
etis´
e
ALOHA discr´etis´e est le premier protocole que l’on va aborder ici, mais qui n’est pas
vraiment le plus efficace. En ALOHA discr´etis´e, on va s´eparer le temps en ”slots” de
taille T , T ´etant le temps n´ecessaire a` la transmission d’une trame. Lorsqu’une machine
veut envoyer une trame, elle ne peut le faire qu’au d´ebut d’un slot 5 . Si deux machines
envoient une trame dans le mˆeme slot, il y a collision ; dans ce cas, le slot est perdu et
les deux machines doivent retransmettre leur trame.
Lorsque ces deux machines doivent re-transmettre leur trame, elle ne vont en fait pas
d’office le faire au slot suivant. En effet, si elles attendaient toutes les deux le mˆeme temps
avant de retransmettre, il y aurait de nouveau une collision, donc au final les paquets
ne seraient jamais transmis. Pour ´eviter cela, on va plutˆot d´ecider qu’une machine ne
re-transmettra sa trame au slot suivant qu’avec une certaine probabilit´e p.
Enfin, notons qu’il faut un moyen pour que les machines sachent si la transmission
s’est bien pass´ee ou pas. Pour cela, on peut soit utiliser une sorte de trame ACK, comme
4. Sauf optimisation possible ; par exemple, il y a moyen de d´etecter qu’une machine est d´ej`a en train
d’utiliser le canal, mais ce n’est pas fiable `a 100%. On en reparlera.
5. d’o`
u le terme ”discr´etis´e” : on n’a acc`es au lien qu’`a certains moments pr´ecis, pas en continu

53

CHAPITRE 5. Couche de liaison
dans la couche de transport, soit utiliser une technique qui consiste a` ´ecouter sur le canal
en mˆeme temps qu’on envoie ; si on ´ecoute exactement ce qu’on envoie, tout va bien,
sinon cela signifie qu’il y a une collision (un autre paquet vient faire interf´erence).
Efficacit´
e d’ALOHA discr´
etis´
e
Instinctivement, on remarque que si ALOHA fonctionne bien lorsqu’il y a peu de
machines utilisant le lien, au plus ce nombre augmente, au plus le risque de collisions
augmente aussi, donc au plus l’efficacit´e diminue. De plus, l’efficacit´e d´epend de la probabilit´e p que l’on prend pour la re-transmission de paquets.
Essayons justement de d´eterminer un p optimal. Pour cela, imaginons que l’on transmette chaque paquet avec une probabilit´e p, mˆeme quand il n’y a pas de collision ; ce n’est
pas vraiment le comportement d’ALOHA mais c¸a nous permet de simplifier les calculs.
1. La probabilit´
e qu’une machine sp´
ecifique (parmi les N machine connect´ees)
envoie un paquet correctement est alors
p(1 − p)N −1
– p est la probabilit´e que le paquet soit transmis correctement
– 1 − p est la probabilit´e qu’une collision avec une machine sp´ecifique intervienne
– il y a N −1 machines connect´ee au lien, en plus de l’exp´editeur, donc la probabilit´e
qu’il y ait une collision apparaˆıt N − 1 fois
2. Pour trouver la probabilit´
e que n’importe quelle machine puisse envoyer ses
donn´ees dans un slot particulier, il suffit de multiplier cette quantit´e par le nombre
de machines, soit N .
N p(1 − p)N −1
En effet, on essaye de maximiser le nombre de slots dans lesquels il n’y a pas de
collision, quelle que soit la machine qui l’utilise.
3. On cherche ici a` maximiser cette quantit´
e. Pour cela, on proc`ede comme pour
n’importe quelle fonction : en cherchant les racines de sa d´eriv´ee.
∂(N p(1 − p)N −1 )
=0
∂p
En faisant les calculs, on en arrive a` la conclusion
p=

1
N

Ce qui est un r´esultat logique.
On sait maintenant que pour maximiser l’efficacit´e d’un lien en ALOHA discr´etis´e, il
faut prendre p inversement proportionnel au nombre de machines connect´ees. Maintenant,
voyons ce que vaut ladite efficacit´e. Pour cela, nous allons repartir de la formule trouv´ee
ci-dessus :
η = N p(1 − p)N −1

54

CHAPITRE 5. Couche de liaison
o`
u on peut remplacer p par N1 :
η = 1−

1
N

N −1

Regardons ce que vaut cette valeur lorsque le nombre de machines connect´ees augmente
(il s’agit d’un r´esultat d’alg`ebre qui n’est pas d´etaill´e ici)
lim

N →∞

1−

1
N

N −1

=

1
e

Conclusion : mˆeme avec la meilleure efficacit´e possible, on n’atteint que p´eniblement les
1
, c’est-`a-dire environ 37% d’efficacit´e ! Il va donc falloir trouver un autre protocole plus
e
efficace.
ALOHA pur
Cette fois, reprenons le principe d’ALOHA, mais enlevons le cˆot´e synchrone ; les
trames peuvent donc maintenant ˆetre envoy´ees n’importe quand. De cette mani`ere, en
cas de collision, on n’est plus oblig´e d’attendre un nombre entier de slots avant de retransmettre.
Cette id´ee peut sembler bonne a` premi`ere vue, mais en fait elle donne des r´esultats
encore pire qu’ALOHA discr´etis´e. En effet, en proc´edant ainsi, on augmente le risque de
collisions :
1. Si on commence a` transmettre trop tˆot, on risque d’interf´erer avec la trame pr´ec´edente
qui n’a pas encore termin´e d’ˆetre envoy´ee.
2. Si on envoie alors que le canal est silencieux, il y a toujours le risque que l’´emetteur
suivant commence d´ej`a a` ´emettre sa trame alors que la notre n’est pas encore
termin´ee (c’est le mˆeme cas que pr´ec´edemment, mais du point de vue de l’autre
´emetteur)
Au final, pour ˆetre sˆ
ur de pouvoir transmettre son paquet correctement, il faut que le
canal soit silencieux pendant au moins 2T (le temps de transmission a` proprement parler,
et un ”d´elai de s´ecurit´e” pour ˆetre sˆ
ur qu’il n’y ait pas de collision)
Efficacit´
e d’ALOHA pur
Le calcul de la probabilit´e p est exactement le mˆeme qu’en ALOHA discr´etis´e ; je vous
passe donc les calculs. L’efficacit´e se calcule aussi de la mˆeme mani`ere, sauf que (1 − N1 )
doit ˆetre mis au carr´e vu qu’il faut attendre deux fois plus longtemps. Au final, on trouve
1
η = 2e
, soit environ 18%.
On aurait aussi pu trouver ce r´esultat par la r´eflexion : on sait qu’un ALOHA
discr´etis´e, o`
u les paquets peuvent s’enchaˆıner sans interruption, a une efficacit´e de 37%.
Comme cette fois, il faut attendre `a chaque transmission de trame, on ”perd” en moyenne
la moiti´e du temps a` attendre ; donc l’efficacit´e est divis´ee par deux, soit environ 18%.

55

CHAPITRE 5. Couche de liaison
D´
etection rapide des collisions : CSMA
Le principal probl`eme d’ALOHA pur est qu’on ne savait pas d´etecter qu’une trame
´etait en cours d’envoi, d’o`
u l’introduction d’un temps d’attente. En effet, en ALOHA, les
noeuds sont ”bˆetes” et essayent d’envoyer leur trame quand ils veulent, sans se soucier de
savoir si le canal est d´ej`a occup´e. Du coup, un moyen simple d’am´eliorer l’efficacit´e est
d’empˆecher les noeuds de ”parler `a travers tout” : avant de transmettre, chaque noeud
va ´ecouter sur le canal, et ne va transmettre que si le canal est silencieux, c’est-`a-dire que
personne n’est occup´e a` transmettre. De mˆeme, lorsqu’on transmet une trame, on va en
mˆeme temps ´ecouter sur le canal pour v´erifier qu’il n’y a pas de collision : si on ´ecoute
ce qu’on est en train de transmettre tout va bien, sinon c’est que quelqu’un d’autre est
aussi en train de transmettre et qu’il y a une collision 6 .
Cette technique permet d’´eviter beaucoup de collisions, mais pas toutes. En effet,
comme vous le savez d´ej`a, il y a un certain temps de propagation sur le canal. Du coup,
lorsqu’un noeud commence a` transmettre une trame, les autres ne s’en rendent compte
qu’avec un peu de retard (le temps que la trame arrive jusqu’`a eux). Il se pourrait, si deux
noeuds commencent `a transmettre exactement au mˆeme moment ou presque, qu’il y ait
alors quand mˆeme une collision. Voyons maintenant les diff´erentes familles de protocoles
CSMA, qui diff`erent dans la mani`ere dont ils r´eagissent en cas de collision.
CSMA p-persistant
Un protocole CSMA persistant va ´ecouter le canal en continu, et va envisager de
transmettre sa trame d`es que le canal se lib`ere. Au contraire, un protocole CSMA nonpersistant va regarder ”de temps en temps” si le canal est libre ; si oui, il va envoyer sa
trame, sinon, il recommence un peu plus tard.
Le p dans le nom de CSMA d´esigne la probabilit´e avec laquelle un noeud va transmettre sa trame lorsqu’il se rend compte que le canal est libre (un peu comme en ALOHA).
Par cons´equent, lorsque le canal est libre, un noeud ne va pas forc´ement transmettre sa
trame. Par d´efinition, on estime qu’un noeud qui n’envoie pas sa trame attend un temps
τ qui correspond a` une fois le d´elai de propagation d’une trame.
Efficacit´
e de CSMA p-persistant et taille des trames
Comme on l’a dit, on ne va transmettre une trame qu’avec une probabilit´e p lorsque
le canal est libre. Cette valeur p influence sur l’efficacit´e du canal, mais malheureusement
elle n’est pas aussi simple a` calculer qu’en ALOHA. En effet, un p trop grand va faire
perdre beaucoup d’efficacit´e (on risque d’attendre plus souvent), mais un p trop petit
risque d’augmenter le nombre de collisions lorsqu’il y a beaucoup de noeuds.
On essaye alors de minimiser le temps de risque, c’est-`a-dire Tτ . τ est le temps de
propagation (c’est-`a-dire le temps qu’il faut pour qu’un bit de donn´ee traverse le lien), et
T le temps de transmission d’une trame (c’est-`a-dire le temps qu’il faut pour que toute la
trame arrive a` bon port). Tτ repr´esente donc la proportion de temps durant laquelle une
collision risque de se produire (on cherche donc a` minimiser cette valeur). Comme on ne
sait pas facilement changer τ (`a moins de prendre des cˆables plus courts ou a` vitesse de
6. Vous remarquerez que ceci n´ecessite un hardware adapt´e ; d’o`
u la raison pour laquelle on disait
dans l’introduction que la couche de liaison ´etait partiellement software, partiellement hardware.

56

CHAPITRE 5. Couche de liaison
propagation plus ´elev´ee), on va donc vouloir prendre T le plus grand possible. Autrement
dit, on va vouloir prendre une taille de trame suffisamment grande pour limiter le risque.
A titre d’exemple, Ethernet utilise des trames d’environ 12.5 kbits.
D´
etection des collisions : CSMA/CD
On a d´ej`a abord´e le sujet dans l’introduction (section 5.4.2). Pour d´etecter les collisions, une technique qui marche bien (au moins sur les r´eseaux filaires) consiste `a ´ecouter
pendant qu’on transmet, et comparer ce qu’on ´ecoute a` ce qu’on envoie. L’avantage de
proc´eder ainsi, c’est qu’on va pouvoir d´etecter les collisions quasi-instantan´ement (en fait,
il faut au pire 2τ millisecondes pour qu’un noeud se rende compte qu’il a eu une collision : le temps que ”sa” trame arrive jusqu’`a l’autre ´emetteur, et le temps que la trame
de l’autre ´emetteur, m´elang´ee `a la trame de d´epart, revienne jusqu’au noeud initial).

5.4.3

Protocoles `
a acc`
es alternatifs

La famille des protocoles alternatifs essaye de prendre le meilleur des deux familles
pr´ec´edentes. En effet, l’avantage des protocoles `a partitionnement est qu’on est sˆ
ur
d’´eviter les collisions, au prix d’une efficacit´e limit´ee. Au contraire, l’avantage des protocoles `a acc`es al´eatoire est qu’on peut utiliser le canal au maximum, au risque de provoquer
des collisions si beaucoup de machines utilisent le mˆeme canal. On va donc essayer ici
d’avoir un protocole qui permet d’utiliser le canal au maximum de sa capacit´e, sans pour
autant provoquer de collisions.
Pour cela, une premi`ere id´ee pourrait ˆetre d’ajouter sur le canal une machine maˆıtre,
qui donne explicitement la parole `a quelqu’un. Notez que du coup on perd le cˆot´e
d´ecentralis´e du protocole, puisqu’il faut un maˆıtre par lien. Cette technique s’appelle
le polling. De plus, il y a le probl`eme de la fiabilit´e : si le maˆıtre tombe en panne, les
noeuds ne savent plus quoi faire, et le lien est inutilisable (sauf si on d´ecide d’impl´ementer
un autre protocole ”de secours” ´evidemment).
Une autre id´ee s’inspire du principe de la course-relai. On a en fait une trame sp´eciale,
le token (jeton), qui ”donne la parole” `a celui qui l’a. Autrement dit, une machine ne
peut transmettre des donn´ees que si elle a le token. Une fois qu’elle a fini sa transmission
(ou ´eventuellement apr`es un certain timeout, il peut y avoir des variantes ´evidemment),
elle donne le token au noeud suivant, et ainsi de suite (le token suit un ordre pr´ed´efini).
Cette technique ressemble un peu `a une partition dans le temps (les machines ne peuvent
transmettre qu’`a certains moments), a` la diff´erence que les machines qui n’ont rien a`
transmettre passeront le token directement, et n’immobiliseront donc pas le canal inutilement. Cette technique peut sembler sympa a` premi`ere vue, mais elle pose quand mˆeme
des probl`emes : elle g´en`ere du trafic inutile (le token a` passer `a chaque fois), et, surtout,
elle n´ecessite quand mˆeme une machine maˆıtre. En effet, si le token est corrompu (`a
cause d’une collision par exemple), il faut une machine sp´eciale qui puisse le ”recycler”.
De plus, il faut que les machines se mettent d’accord pour savoir dans quel ordre le token
va voyager.
En conclusion, les protocoles a` acc`es alternatifs essayent de prendre le meilleur des
deux autres familles, mais en proc´edant ainsi, ils introduisent leurs propres contraintes.
Il n’y a donc malheureusement pas de ”solution miracle”.

57

CHAPITRE 5. Couche de liaison

5.5
5.5.1

Adressage MAC
Principe de l’adressage MAC

On a d´ej`a pas mal parl´e de l’adressage MAC dans l’introduction (section 5.1). Pour
rappel, une adresse MAC est un nombre de 48 bits qui est attribu´e de mani`ere univoque
a` chaque interface (carte r´eseau). Par cons´equent, chaque carte r´eseau du monde a une
adresse MAC qui lui est propre, et qu’on ne sait (en principe) pas changer. Contrairement
aux adresses IP, les adresses MAC ne sont pas structur´ees, c’est-`a-dire qu’il n’y a pas `a
proprement parler de sous-r´eseau : vous avez votre adresse MAC, et vous la gardez quel
que soit le r´eseau o`
u vous ˆetes connect´e. Lorsqu’on doit envoyer une trame a` une certaine
interface, on pr´ecise alors l’adresse MAC de destination 7 . La trame est ensuite diffus´ee
sur le lien, et l’interface ayant l’adresse MAC correspondante la r´ecup`ere 8 . Il reste alors
un gros probl`eme `a r´esoudre : comment connaˆıtre l’adresse MAC d’une machine ?

5.5.2

Comment connaˆıtre l’adresse MAC d’une machine ?

A ce stade, la seule information qu’on connaˆıt concernant la destination d’un paquet
est l’adresse IP finale. Il va donc falloir faire une correspondance entre adresses IP et
adresses MAC. Pour cela, on va utiliser la mˆeme technique que d’habitude : chaque
interface retient une table qui fait la correspondance entre les diff´erentes adresses IP
et les interfaces MAC associ´ees. On appelle une telle table table ARP, pour Address
Resolution Protocol.
ARP fonctionne selon un principe semblable a` DHCP (section 4.3.2) :
1. Lorsqu’une machine voit passer une trame, elle enregistre dans sa table ARP le
couple (adresse IP source, adresse MAC) histoire de l’avoir sous la main si
n´ecessaire. Notons que les entr´ees ont un TTL (time-to-live), c’est-`a-dire qu’apr`es
un certain temps d’inactivit´e, elles seront enlev´ees de la table. En effet, il se pourrait
que la machine en question se soit d´econnect´ee ou ait chang´e d’adresse IP pour une
raison ou l’autre.
2. Si vous devez envoyer une trame a` un noeud qui ne se trouve pas dans votre table
ARP, vous allez diffuser une requˆete ARP sur le lien, en pr´ecisant l’adresse IP que
vous voulez joindre. La machine qui dispose de cette adresse IP (si elle est pr´esente
bien entendu) r´eagit en diffusant une trame contenant son adresse IP et son adresse
MAC. De cette mani`ere, tous les liens connaissent la machine et la rajoutent a` leur
propre table ARP.

7. Attention, on est toujours dans les probl`emes de forwarding : la ”destination” est donc ici le noeud
suivant sur un chemin. A ne pas confondre avec la ”destination” de la couche r´eseau qui est l’end system
qu’on cherche `
a atteindre.
8. Rappelez-vous : tout ce qui est envoy´e sur le lien est accessible `a tout ceux qui y sont connect´es ;
un certain noeud verra donc passer toutes les trames, mˆeme celles qui ne lui sont pas destin´ees.

58

CHAPITRE 5. Couche de liaison
3. Si la destination se trouve dans votre table ARP, on proc`ede comme expliqu´e dans
l’introduction : on pr´ecise dans l’en-tˆete de la trame a` quelle adresse MAC celle-ci
est destin´ee, et on la diffuse sur le lien. La machine qui a l’adresse MAC en question
va conserver la trame, tandis que les autres ne vont pas s’en pr´eoccuper 9 .
A ce stade, il est important d’insister sur un point qui a ´et´e abord´e dans l’introduction :
la couche de liaison existe pour chaque sous-r´eseau, y compris donc ceux qui relient deux
routeurs sur le trajet. Dans ce cas, il faut a` chaque ´etape pr´eciser l’adresse MAC du
routeur suivant sur le chemin (... mais on ne touche pas a` l’adresse IP, qui doit rester
celle de la destination finale !). On va donc utiliser ARP pour chaque lien `a traverser.
Cette situation est repr´esent´ee de mani`ere sch´ematique a` la figure 5.1.

Figure 5.1 – Modification de l’en-tˆete de la trame (en bleu) pour chaque lien a` traverser,
celle-ci allant de A vers D.
Notons qu’en vrai, chaque routeur a 2 adresses IP et 2 adresses MAC ; de mˆeme,
chaque machine a bien entendu une adresse IP et une adresse MAC ; elles n’ont pas ´et´e
repr´esent´ees sur le graphique par manque de place.

5.6

Exemple de protocole de liaison : Ethernet

Ethernet est le protocole de liaison le plus utilis´e `a l’heure actuelle. Il ´etait initialement
con¸cu pour fonctionner sur un bus, mais continue a` fonctionner avec d’autres types de
dispositifs (voir section suivante). Ethernet est un protocole non-orient´e connexion, et
non-fiable. En particulier, cela signifie qu’il n’y a pas de trame de type ACK qui est envoy´ee ;
donc si une trame est corrompue, Ethernet ne le saura pas, et c’est une des couches
sup´erieures qui se rendra compte, a` la r´eception, qu’il manque des morceaux. En revanche,
Ethernet est un protocole de type CSMA/CD, donc il est capable d’´eviter/d´etecter la
plupart des collisions.
9. Du moins on l’esp`ere ... Comme vous l’avez peut-ˆetre remarqu´e cela pose comme un probl`eme de
s´ecurit´e, dans la mesure o`
u vous pourriez tr`es bien intercepter des paquets qui ne vous sont pas destin´es,
et ”espionner” le r´eseau de cette mani`ere.

59

CHAPITRE 5. Couche de liaison
En cas de collision, la carte r´eseau qui utilise Ethernet va arrˆeter l’envoi, et va plutˆot
envoyer du bruit pour ˆetre sˆ
ur que tout le monde s’arrˆete de transmettre (en envoyant 48
bytes de bruit, on est sˆ
ur que tout le monde s’est rendu compte qu’il y a eu une collision).
Ensuite, la carte r´eseau va attendre un certain temps, choisi au hasard dans l’intervalle
[0, 2n − 1] o`
u n est proportionnel au nombre de collisions qui se sont produites. De cette
mani`ere, les machines peuvent s’adapter a` la charge du lien : si subitement, beaucoup de
machines utilisent le lien simultan´ement, beaucoup de collisions vont se produire, donc le
temps d’attente moyen va augmenter pour compenser, et la charge sera mieux r´epartie.
Notons enfin qu’Ethernet est un protocole de liaison qui peut fonctionner avec ´enorm´ement
de protocoles diff´erents de la couche physique. Nous n’entrerons pas dans les d´etails ici

5.7
5.7.1

Les diff´
erents dispositifs de liaison
Le bus

Un bus est simplement un cˆable (fil de cuivre, fibre optique, ...) auquel plusieurs
machines ont acc`es. Les informations sont donc forc´ement diffus´ees a` toutes les machines
qui sont connect´ees `a ce bus, puisqu’elles sont toutes connect´ees au mˆeme cˆable. Pour
envoyer une information, une machine va donc ´ecrire dans le bus directement, et la trame
sera alors propag´ee dans tout le bus.
Ce dispositif ´etait tr`es `a la mode dans les ann´ees 90, mais n’est plus beaucoup utilis´e de
nos jours parce qu’il pose un tas de probl`emes de fiabilit´e, de d´egradation de l’information,
d’optimisation et de s´ecurit´e.

5.7.2

Le hub

D’un point de vue fonctionnel, un hub fonctionne exactement comme un bus ; l’utilisation d’un hub au lieu d’un bus (ou inversement) est donc totalement transparente pour
les machines connect´ees. La principale diff´erence entre les deux, est que le hub utilise une
topologie en ´etoile : toutes les machines se connectent au hub, et lorsque l’une d’elles
envoie une trame, celle-ci est dupliqu´ee sur tous les liens (sauf celui de l’exp´editeur). Un
hub n’est donc pas vraiment plus malin qu’un bus ; son principal avantage est que les
machines ne sont pas directement reli´ees entre elles, donc si un des liens est d´efaillant,
les autres machines peuvent continuer a` communiquer.

5.7.3

Le switch

Le switch (ou commutateur en fran¸cais correct) est en quelque sorte l’´evolution du
hub. Comme un hub, un switch est reli´e `a toutes les machines faisant partie d’un mˆeme
lien. Comme un hub, son utilisation est totalement transparente et plug-and-play (il ne
n´ecessite aucune intervention ext´erieure pour fonctionner). La principale diff´erence entre
le switch et le hub, c’est qu’un switch ne diffuse pas bˆetement tout ce qu’il re¸coit, mais
fait un tri et essaye, dans la mesure du possible, de ne transmettre les trames qu’aux
interfaces a` qui elles sont destin´ees. De plus, quitte `a faire un dispositif ”intelligent”, on a
choisi de lui rajouter des buffers et une matrice de commutation pour ´eviter les collisions

60

CHAPITRE 5. Couche de liaison
dans la plupart des cas 10 . Au final, un switch ressemble donc fort `a un routeur d’un point
de vue fonctionnement (mais attention, ils n’appartiennent pas a` la mˆeme couche donc
n’ont pas le mˆeme rˆole !)
Pour qu’un switch puisse faire un tri dans les trames qu’il transmet, il va utiliser la
technique habituelle de la table d’acheminement :
1. Lorsqu’une trame arrive au switch, celui-ci va stocker dans sa table d’acheminement
l’adresse MAC (pas IP ! Les switch n’ont pas de couche r´eseau !) de l’exp´editeur,
ainsi que l’interface qui permet de joindre ledit exp´editeur. Comme d’habitude, on
rajoute aussi un TTL histoire que les informations soient remises `a jour r´eguli`erement
(pour rappel, une entr´ee est supprim´ee lorsque son TTL est atteint)
2. Pour acheminer la trame en question, le switch va alors consulter sa table d’acheminement. Deux possibilit´es :
– Soit l’adresse MAC de la destination se trouve dans la table d’acheminement. Le
switch va alors rediriger la trame vers l’interface de sortie correspondante.
– Soit l’adresse MAC de la destination ne se trouve pas dans la table d’acheminement (par exemple, le noeud en question n’a jamais rien envoy´e et n’est donc pas
connu du switch). Dans ce cas, le switch n’a pas vraiment d’autre choix que de
diffuser la trame a` toutes ses interfaces. Dans ce cas pr´ecis, on n’a pas su faire de
filtrage ; c’est fort dommage, mais c¸a arrive.
Attention il est bien n´ecessaire de faire la diff´erence entre les tables ARP et les tables
d’acheminement des switches ! La premi`ere permet de faire la correspondance entre les
couches de r´eseau et de liaison, et ne se trouvent donc jamais dans un switch (puisqu’un
switch ne connaˆıt pas la couche r´eseau). La seconde sert uniquement pour les switches
et permet d’optimiser le nombre de transmissions au sein d’un lien ; on ne la retrouvera
donc jamais dans un autre composant.
Connecter plusieurs switches : le probl`
eme des cycles
Le syst`eme d´ecrit fonctionne bien lorsqu’on connecte plusieurs machines a` un switch,
et mˆeme lorsque l’on connecte plusieurs switches ensemble (ou ´eventuellement des switches
et d’autres dispositifs). Cependant, il y a un cas probl´ematique : lorsqu’un cycle est cr´e´e,
c’est-`a-dire que deux switches sont reli´es entre eux par des chemins diff´erents. En effet,
dans ce cas pr´ecis, si on ne fait rien de sp´ecial, le premier switch va diffuser ses paquets
sur toutes les interfaces, paquets qui vont donc arriver au deuxi`eme switch. Ensuite, le
deuxi`eme switch va lui aussi diffuser le paquet sur toutes les interfaces ... Donc ledit
paquet va revenir au premier switch, et ainsi de suite !
Pour ´eviter cela, on va alors choisir de bloquer certaines interfaces de telle mani`ere
qu’il n’y ait plus qu’un seul chemin entre chaque paire de switches. Autrement dit (et
pour parler en termes algorithmiques), on va chercher l’arbre sous-tendant minimal a`
notre r´eseau de switches. Pour rappel, on veut que nos switches soient plug-and-play,
donc qu’ils ne n´ecessitent aucune intervention ext´erieure pour fonctionner. Il va donc
10. Il peut quand mˆeme y avoir des collisions, si du trafic va en mˆeme temps de et vers une certaine
machine par exemple.

61

CHAPITRE 5. Couche de liaison
falloir trouver non seulement une technique qui fonctionne, mais surtout une technique
qui fonctionne toute seule 11 .
On va en fait proc´eder de la mani`ere suivante : chaque switch va, de temps en temps,
envoyer un BPDU, c’est-`a-dire un message avertissant les switches de la mani`ere dont il
voit le r´eseau. Ensuite, grˆace aux informations pr´esentes dans les diff´erents BPDU, les
switches vont construire un arbre sous-tendant minimal qui leur permettra de d´eterminer
quelles interfaces bloquer.
Ce BPDU contient les informations suivantes :
– Le num´ero du switch qui a envoy´e le paquet. Pour le bon fonctionnement de l’algorithme, il est important que chaque switch dispose d’un num´ero (ID) qui lui est
propre.
– La racine de l’arbre sous-tendant minimal que l’on est en train de construire. Plus
d’infos l`a-dessus tout de suite.
– La distance minimale qui s´epare le switch en question de la racine.
Ensuite, chaque switch va proc´eder de la mani`ere suivante :
1. Choix de la racine Par d´efinition, on va dire que la racine de l’arbre est le switch
ayant l’ID le plus petit.
(a) Au tout d´ebut de l’algorithme, chaque switch va consid´erer qu’il est lui-mˆeme
la racine (puisqu’il ne connaˆıt pas ses voisins, donc il ne sait pas s’il existe
un switch d’ID plus petit). Il va alors envoyer un BPDU a` ses voisins avec
les valeurs (ID, ID, 0) (on est soi-mˆeme la racine, donc on se trouve `a une
distance 0 de celle-ci).
(b) Ensuite, lorsqu’on re¸coit un BPDU, si l’ID de l’´emetteur est plus petit que son
propre ID, on change la racine et on incr´emente la distance. Sinon, on ne fait
rien.
(c) A la fin, on aura donc bien comme racine le switch d’ID le plus petit.
2. La deuxi`eme ´etape consiste `a construire l’arbre. Au vu des informations qu’on
a, ce n’est pas vraiment compliqu´e :
(a) On consid`ere comme root port un port qui est sur le chemin vers la racine. On
n’a mˆeme pas besoin de calculer cette information : il s’agit en fait du port qui
re¸coit le BPDU d’ID le plus petit. Ce port sera toujours passant : c’est celui
qu’on utilisera pour transmettre une trame.
(b) On consid`ere comme forwarding port le port qui ne m`ene pas vers la racine,
mais qui a re¸cu le BPDU le plus petit (c’est-`a-dire celui avec la distance ou
l’ID le plus petit). Ces ports sont passants, dans le sens o`
u on va ´ecouter ce
qui arrive par ces ports, et les transmettre au root port.
(c) Enfin, on consid`ere que tous les autres ports sont bloquants (blocking port).
On n’´ecoute pas ce qui passe sur ces ports (sauf les BPDU), et on ne transmet
jamais rien sur ces ports.
Au final, si on oublie tous les ports bloquants, on a donc bien un arbre sous-tendant
au graphe initial. De plus, comme les BPDU sont renvoy´es a` intervalles r´eguliers (et que
11. on pourrait bien sˆ
ur bloquer manuellement les interfaces qu’on veut, mais ce n’est pas tr`es pratique
...

62

CHAPITRE 5. Couche de liaison
les ports passants et bloquants sont recalcul´es en cons´equence), on est sˆ
ur qu’il sera remis
a` jour r´eguli`erement.
Efficacit´
e d’un tel syst`
eme
Un tel syst`eme permet d’´eviter les cycles, mais a un gros d´esavantage, qui est qu’on
n’utilisera pas tous les liens disponibles. En particulier, pour aller d’un switch a` un autre,
il pourrait y avoir un chemin beaucoup plus court que celui utilisant uniquement les
ports passants. C’est fort dommage, mais c’est un prix a` payer pour pouvoir avoir de la
redondance sur le r´eseau. En effet, le gros avantage de ce syst`eme est qu’il permet d’avoir
plusieurs chemins entre chaque paire de switches (qui ne seront donc jamais utilis´es
simultan´ement) ; par cons´equent, si un lien tombe en panne, l’arbre sous-tendant sera
recalcul´e autrement, et les trames utiliseront un autre chemin pour arriver `a destination.

5.8
5.8.1

Pour terminer ...
Quelques mots sur la virtualisation

Comme on l’a vu tout au long du cours, le mod`ele OSI fonctionne selon diff´erents niveaux d’abstraction : a chaque fois, on a suppos´e que les couches du dessous fonctionnaient
correctement pour fournir le r´esultat voulu, sans se soucier de la structure des couches en
question. On peut en particulier utiliser un raisonnement semblable en ce qui concerne
la couche r´eseau. On a parl´e dans le chapitre pr´ec´edent de ”r´eseaux interconnect´es” du
point de vue de la couche r´eseau, sans pr´eciser comment ils ´etaient r´eellement connect´es.
Grˆace `a ce chapitre sur la couche de liaison, on remarque qu’en fait deux gateways (ou un
gateway et un syst`eme d’extr´emit´e, ou ...) peuvent ˆetre reli´es diff´eremment de mani`ere
tout `a fait transparente : avec un simple cˆable (liaison point-`a-point), avec des switches
ou hubs, voire mˆeme avec des ”r´eseaux de switches”. En conclusion, on remarque que IP
consid`ere en fait des liaisons ”logiques” (tel noeud est accessible a` partir de tel autre),
peu importe la mani`ere dont ces liaison se font.

5.8.2

Ce qui n’a pas ´
et´
e vu dans ce chapitre

Par manque de temps, les protocoles point-`a-point n’ont pas ´et´e abord´es au cours, et
ne se retrouvent donc pas dans ce r´esum´e. De mˆeme, on n’a pas abord´e les m´ecanismes
de d´etection d’erreur utilis´es dans ces protocoles. Si cette mati`ere venait `a r´eapparaˆıtre
dans les ann´ees `a venir, n’h´esitez pas `a compl´eter ce r´esum´e !

63

Annexe A
La d´
etection et la correction
d’erreurs
A.1
A.1.1

D´
etection d’erreurs simple : checksum
Cr´
eation de la checksum

D´etaillons le fonctionnement de la v´erification d’erreurs par checksum, utilis´ee principalement dans la couche de transport. La premi`ere ´etape consiste a` diviser un paquet
en morceaux de 16 bits. Prenons par exemple le paquet suivant :
0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1
0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1
La deuxi`eme ´etape consiste `a additionner les morceaux ainsi form´es entre eux. Pour
rappel, l’addition binaire fonctionne comme l’addition d´ecimale (0+0 = 0, 1+0 = 0+1 =
1, 1 + 1 = 10 c’est-`a-dire qu’il faut reporter 1 `a gauche). L’addition se fait en compl´ement
a` 1, c’est-`a-dire que s’il y a un bit de report a` la fin de l’addition, il est ajout´e au r´esultat.
Ici, cela donne la somme suivante :
1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0
Enfin, la derni`ere ´etape de la cr´eation de la somme consiste simplement a` inverser
chacun des bits, pour avoir l’´equivalent n´egatif de la checksum
0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1
C’est cette valeur-l`a qui est mise dans le paquet lors de l’envoi par la couche de
transport.

A.1.2

V´
erification du paquet

Pour v´erifier qu’un paquet est correct, il faut re-d´ecouper ce paquet en blocs de 16
bits et les additionner. (avec l’exemple du dessus, le r´esultat est donn´e a` la deuxi`eme
´etape). Il faut alors ensuite additionner ce r´esultat `a la checksum fournie ; si au final on
obtient z´ero, c’est que le paquet est correct (comme la checksum fournie est normalement
l’inverse de la checksum calcul´ee, on a bien x + (−x) = 0) 1
1. Petit rappel : en compl´ement `
a 1, le nombre z´ero peut se coder soit 000...0, soit 111...1

64

CHAPITRE A. La d´etection et la correction d’erreurs
1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0
0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1

A.1.3

Erreurs non-d´
etectables

Si deux bits, pr´esents a` la mˆeme position dans deux ”blocs” diff´erents, changent de
valeur en mˆeme temps, alors le r´esultat de la somme sera le mˆeme. Il s’en suit que l’erreur
ne sera donc pas d´etect´ee. Si on reprend l’exemple de la section A.1.1 en inversant les
bits en 2`eme position, on obtient
1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0
0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1
= 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
qui est bien le r´esultat ”normal”.
Il en va de mˆeme si un des bits change dans le paquet et un dans la checksum.
Enfin, remarquons que dans le cas des r´eseaux informatiques, il peut aussi y avoir des
”faux positifs” : si aucun bit de donn´ee ne change mais qu’un des bits de la checksum
fournie change de valeur, le r´ecepteur va consid´erer le paquet comme invalide (puisque la
somme ne vaudra pas 0), alors qu’en fait les donn´ees sont tout a` fait correctes.

A.2
A.2.1

Les codes d´
etecteurs et correcteurs d’erreur :
bits de parit´
e
La d´
etection d’erreurs par bit de parit´
e

Fonctionnement
Les bits de parit´e sont une autre m´ethode permettant de d´etecter des erreurs lors
d’une transmission. Ils fonctionnent selon un principe tr`es simple : pour chaque mot de
N bits transmis (par la suite on prendra des octets, donc N = 8), on ”r´eserve” un bit
en fin de mot dont la valeur ne pourra pas ˆetre choisie librement. En fait, on va fixer la
valeur de ce bit de telle mani`ere que le nombre total de bits valant 1 dans le mot soit
toujours pair. Autrement dit, s’il y a un nombre impair de bits a` 1 dans le mot, on mettra
le bit de parit´e a` 1 aussi ; s’il y a un nombre pair de bits `a 1, on mettra le bit de parit´e a`
0.
Exemple 1 Codons la valeur 42 sur un octet avec bit de parit´e :
0 1 0 1 0 1 0?
Il y a un nombre impair de bits a` 1. Il faudra donc que le bit de parit´e (not´e ?) valle
1 aussi. Au final, on aura donc la valeur 01010101.

65

CHAPITRE A. La d´etection et la correction d’erreurs
Exemple 2 Codons cette fois la valeur 51 :
0 1 1 0 0 1 1?
On a cette fois un nombre pair de bits a` 1. Le bit de parit´e vaudra donc z´ero. Au
final, on aura donc 01100110
D´
etection des erreurs
La d´etection des erreurs est d`es lors tr`es simple : lorsqu’on re¸coit une information, il
suffit de v´erifier que le nombre total de bits a` 1 est pair. Si il est impair, on est sˆ
ur qu’il
y a eu une erreur 2
Efficacit´
e et limitations
D’un point de vue de la puissance de calcul, la d´etection d’erreurs par bits de parit´e
est excellente puisqu’il suffit de savoir faire des comparaisons et des additions. De ce point
de vue, cet algorithme est donc plus efficace qu’une checksum.
En revanche, on y perd facilement en place. L`a o`
u une checksum prend 16 bits quelle
que soit la taille des donn´ees a` v´erifier, notre v´erification va ”consommer” un bit pour la
parit´e tous les 7 bits de donn´ees. Autrement dit, `a partir de 16 octets de donn´ees, une
checksum prendra moins de place.
Enfin, le probl`eme le plus embˆetant est la vuln´erabilit´e de cette solution : en effet, si
il y a deux erreurs dans un mˆeme octet, on ne remarquera rien vu que le nombre de bits
a` 1 sera toujours pair.
En conclusion, on se retrouve donc avec un algorithme qui prend plus de place et
laisse passer plus d’erreurs qu’une checksum ... Alors, quel int´erˆet ?

A.2.2

La correction d’erreur et les blocs de parit´
e

Fonctionnement
Nous allons cette fois nous servir des bits de parit´e non seulement pour d´etecter une
erreur, mais aussi pour la corriger. Pour cela, en plus des bits de parit´e, nous allons
ajouter un bloc de parit´e tous les N mots. Un bloc de parit´e est un mot (disons un octet,
comme d’habitude) dont la valeur de chaque bit est fix´ee de la mani`ere suivante : un bit
a` la position i est mis a` 1 ssi il y a un nombre impair de bits a` 1 a` la position i dans
les blocs pr´ec´edents. Chaque bit de ce bloc est donc un bit de parit´e qui se r´ef`ere a` une
position pr´ecise.
Comme un petit dessin vaut mieux qu’un long discours, voyons tout de suite cela
a` la figure A.1. Les octets ont ´et´e mis l’un en-dessous de l’autre pour la facilit´e. Vous
remarquerez que grˆace au bit de parit´e en fin de chaque octet, et grˆace au bloc de parit´e,
il y a un nombre pair de bits dans chaque ligne et chaque colonne.
2. Notez que la r´eciproque n’est pas vraie ! Ce n’est pas parce que le test de parit´e `a r´eussi qu’il n’y
a pas d’erreurs ! Voir section suivante

66

CHAPITRE A. La d´etection et la correction d’erreurs

Figure A.1 – Construction du bloc de parit´e ; les bits de parit´e sont en rouge
Correction des erreurs
Que se passe-t-il maintenant s’il y a une erreur dans le mot, c’est-`a-dire si un bit
change de valeur ? La r´eponse est simple : il y aura forc´ement une ligne et une colonne o`
u
le nombre de bits a` 1 sera impair. Par cons´equent, d´etecter et corriger une erreur devient
assez simple : si un octet a un nombre impair de bits `a 1, c’est qu’il y a une erreur dans cet
octet. Dans ce cas, le bloc de parit´e permet de savoir a` quelle position se trouve l’erreur,
c’est-`a-dire quel est le bit fautif. Il ne reste alors plus qu’`a corriger l’erreur en inversant
la valeur de ce bit, et le tour est jou´e !
Graphiquement, si on reprend la notation en tableau, on peut trouver l’erreur en
mettant en ´evidence la ligne et la colonne o`
u la parit´e n’est pas respect´ee ; l’erreur se
trouve a` l’intersection des deux (cf figure A.2).

Figure A.2 – D´etection d’une erreur ; le bit fautif est en orange
Efficacit´
e et limitations
D’un point de vue puissance de calcul, cet algorithme n’est pas vraiment plus compliqu´e que le pr´ec´edent ; il suffit toujours de savoir faire des additions et des comparaisons.
67

CHAPITRE A. La d´etection et la correction d’erreurs
D’un point de vue de la place perdue, par contre, cet algorithme prend ´evidemment
encore beaucoup plus de place que le pr´ec´edent, puisqu’on ”perd” un octet tous les x
octets. En contrepartie, on est capable de corriger des erreurs simples, donc on devra
retransmettre des paquets moins souvent, ce qui est un avantage d’un point de vue efficacit´e.
Par contre, cet algorithme a une grosse limitation : il n’est capable de d´etecter qu’une
seule erreur binaire, pas plus. En effet, si on a deux erreurs en mˆeme temps, cela signifie
qu’il y a deux lignes et deux colonnes o`
u la parit´e n’est plus respect´ee. Autrement dit,
il y a 4 positions possibles pour les erreurs (chacune des 4 intersections), et l’algorithme
n’est pas capable de d´eterminer o`
u se trouvent r´eellement les erreurs. Ce probl`eme n’est
pas r´esoluble d´efinitivement : on pourra d´etecter au mieux un nombre fini d’erreurs (en
rajoutant des informations de parit´e r´eguli`erement), mais il y aura toujours une limitation
avec cette technique.

68

Annexe B
TCP et la gestion des connexions
B.1

Les types de message et leur contenu

SYN Le premier message qui est envoy´e (par le client) lors de l’ouverture d’une connexion
TCP. Ce paquet ne peut pas contenir de donn´ees puisque la connexion n’est pas encore
´etablie et, entre autres, il n’y a pas encore de buffers disponibles chez le serveur. En
revanche, ce message contient le num´ero de s´equence que le client va utiliser comme
num´ero initial. A la r´eception d’un paquet SYN, le serveur alloue des buffers a` la nouvelle
connexion et renvoie un paquet SYNACK
SYNACK Ce paquet est envoy´e par le serveur en r´eponse a` un paquet SYN. Il confirme
que la connexion est accept´ee. Le paquet SYNACK ne contient pas de donn´ees non plus.
En revanche, il contient l’ACK du paquet SYN (d’o`
u son nom), ainsi que le num´ero de
s´equence que le serveur va utiliser comme num´ero de s´equence de base (rappelez-vous :
TCP est bidirectionnel, chaque r´ecepteur est aussi un ´emetteur potentiel et doit pr´eciser
quel num´ero de s´equence il va utiliser). A la r´eception d’un paquet SYNACK, le client
alloue lui aussi des buffers pour la connexion serveur → client. Notons d’ailleurs qu’`a
ce moment-l`a, le client connaˆıt d´ej`a la taille de la fenˆetre de r´eception du serveur (cette
information se trouve dans l’en-tˆete de chaque segment, y compris le segment SYNACK) ;
l’inverse n’est pas vrai puisque le serveur n’a pas encore re¸cu de ”vrai” paquet de la part
du client.
RST Un paquet un peu sp´ecial, envoy´e par le serveur, qui indique qu’il refuse la
connexion. Un tel paquet peut par exemple ˆetre envoy´e si le port n’est pas ouvert (il
n’y a pas de processus qui ´ecoute sur ce port) ou si la connexion est bloqu´ee par un
pare-feu. Dans ce cas, aucun buffer n’est allou´e et la connexion est interrompue.
Premier ACK du client Acquitte le paquet SYNACK du serveur. Maintenant, la connexion
est pleinement fonctionelle, et le client peut directement commencer a` envoyer des donn´ees.
C’est grˆace a` ce paquet que le serveur connaˆıt la taille de la fenˆetre de r´eception du client
et peut faire du contrˆole de flux.
FIN Paquet envoy´e pour signifier que la connexion ne sera plus utilis´ee dans ce sens-l`a.
Attention, le nom est trompeur : le paquet FIN en soi n’interrompt pas la connexion ; il
69

CHAPITRE B. TCP et la gestion des connexions
indique juste que l’´emetteur du FIN n’a plus rien a` transmettre et que, si la connexion
n’est plus utilis´ee dans l’autre sens non plus, alors elle peut ˆetre ferm´ee.

B.2

Le choix des premiers num´
eros de s´
equence et
les probl`
emes de s´
ecurit´
e

Lorsqu’une connexion est ´etablie, le client (puis le serveur) doivent choisir un num´ero
de s´equence de base a` partir duquel ils vont num´eroter tous les paquets qui transitent
sur le r´eseau. Le choix de ce num´ero n’est pas anodin. Il faut en effet que le num´ero de
s´equence n’ait pas ´et´e utilis´e r´ecemment, et qu’il ne soit pas facilement ”devinable”, pour
des raisons de s´ecurit´e.
Rappelons que c’est le le tuple (IP source, port source, IP destination, port destination) qui sert a` l’identification d’une connexion. D`es lors, si le num´ero de s´equence
initial du serveur est facilement devinable (par exemple, toujours 0), rien n’empˆecherait
un client mal intentionn´e d’envoyer plein de paquets SYN puis ACK avec une fausse
adresse IP source, pour que le serveur r´eserve de la m´emoire pour toutes ces ”fausses”
connexions, et provoque un d´eni de service. Si le num´ero de s´equence est difficilement
devinable, le client ne peut d`es lors plus deviner le num´ero du ACK a` envoyer, et il ne
peut plus ´etablir de ”fausse” connexion avec le serveur.
Mais mˆeme sans prendre en compte ces soucis de s´ecurit´e, changer de num´ero de
s´equence a` chaque connexion est indispensable. Pourquoi ? Imaginons qu’un serveur envoie un paquet num´ero x a` un client. A ce moment-l`a, la connexion est interrompue, et le
client r´e-´etablit une connexion avec le serveur 1 . Si le nouveau num´ero de s´equence choisi
est trop proche de x (par exemple x−1), au moment o`
u le client recevra x, il ne saura pas
d´etecter qu’il s’agit d’un vieux paquet qui n’a plus aucune importance, et va le stocker
comme une information venant du serveur, ce qui n’est ´evidemment plus pertinent.
On pourrait bien sˆ
ur ´eviter ces probl`emes de mani`ere assez primitive, par exemple en
empˆechant de r´eutiliser le mˆeme num´ero de port, ou en empˆechant le client de r´e-´etablir
une connexion avec le serveur tant que la dur´ee de vie maximale du dernier paquet
´echang´e n’est pas atteinte. Mais il y a un moyen plus ´el´egant de faire : changer de num´ero
de s´equence a` chaque nouvelle connexion.
Pour cela, on se base sur une clock, c’est-`a-dire un compteur qu’on suppose fiable (il
continue a` tourner mˆeme quand la machine plante ou est `a l’arrˆet). Au moment o`
u on
veut ´etablir une connexion (donc envoyer un paquet SYN ou SYNACK), on prend alors la
valeur de la clock comme valeur de d´epart 2 . De cette mani`ere, a` priori, aucune confusion
n’est possible entre deux connexions trop proches l’une de l’autre.
Mais il y a un mais : comme toute autre valeur informatique, la valeur de la clock est
finie, et encod´ee sur k bits. Donc, tous les 2k ticks, la clock revient a` z´ero. Imaginons du
coup la situation suivante : le client ´etablit une connexion avec le serveur avec num´ero
de s´equence de base 42. Le temps s’´ecoule gentiment, le dernier paquet ´echang´e atteint
par exemple le num´ero 50, puis la connexion est interrompue inopin´ement. Le client
r´e´etablit une connexion avec le serveur ; entre-temps, la clock a fait un ”tour complet”
et est revenue a` la valeur 47. Le vieux paquet num´ero 50, lui, n’est pas encore arriv´e au
1. On suppose qu’il utilise la mˆeme adresse IP et le mˆeme num´ero de port qu’avant
2. Les paquets suivants sont bien sˆ
ur num´erot´es de mani`ere lin´eaire.

70

CHAPITRE B. TCP et la gestion des connexions
serveur. Au moment o`
u il arrivera, le serveur le consid´erera alors comme un paquet n˚50
appartenant `a la nouvelle connexion, et pas comme un reste de l’ancienne connexion. Tel
quel, le probl`eme pr´esent´e plus haut n’est donc pas r´esolu.
Cette fois, il n’existe pas d’autre solution que d’´eviter de s’approcher trop de la valeur
de la clock. Autrement dit, si le num´ero de s´equence d’un paquet est trop proche de la
valeur de la clock, on ´evite d’envoyer ce paquet puisqu’il pourrait ˆetre mal interpr´et´e si
la connexion devait ˆetre r´einitialis´ee. Concr`etement, la valeur critique qu’on ne veut pas
d´epasser est C − T , o`
u C est la valeur actuelle de la clock et T est la dur´ee de vie d’un
paquet sur le r´eseau.

71

Index
A
Address Resolution Protocol, 71
adresse IP, 12, 41
adresses MAC, 62
algorithme `a ´etat de lien, voir link state
algorithme a` vecteur de distance, voir distance vector
aliasing, 14
ALOHA discr´etis´e, 65
ALOHA pur, 67
arbre des plus courts chemins, 50
arbre sous-tendant minimal, 75
AS, voir Autonomous System
authoritative servers, 15
Autonomous System, 56
B
baud, 6
BGP, 58
bloc de parit´e, 82
blocking port, 77
Border Gateway Protocol, voir BGP
BPDU, 76
bus, 73
C
cache, 13
Carrier Sens Multiple Access, voir CSMA
checksum, 19
Circuit switching, 8
client-serveur, 11
clock, 87
congestion avoidance, 34
connections HTTP non-persistantes, 13
connections HTTP persistantes, 13
Contrˆole de flux, 30
cookies, 13
Couche applicative, 11
Couche de liaison, 61

couches, 10
crossbar, 37
CSMA, 68
CSMA non-persistant, 68
CSMA p-persistant, 68
CSMA persistant, 68
CSMA/CD, 69
D
d´emultiplexage, 18
datagramme, 19, 36
Delayed ACK, 30
DHCP, 43
Distance Vector, 53
distance vector, 48
DNS, 14
Domain Name System, voir DNS
Dynamic Host Configuration Protocol, voir
DHCP
E
eBGP, 58
end systems, 5
external BGP, voir eBGP
F
Fast Retransmit, 30
FDM, voir Frequency Division Method
FDMA, 64
fenˆetre de congestion, 33
fenˆetre de r´eception, 33
Fibre monomode, 8
Fibre multimode, 7
Fibre multimode `a coefficient variable, 8
fibre optique, 7
FIN, 28
Flow controle, voir Contrˆole de flux
forwarding, 36
forwarding port, 76
forwarding table, 37

72

INDEX
frame, voir trame
P
Frequency Division Method, 8
p´eriph´erie du r´eseau, 5
Frequency Division Multiple Access, voir FDMA
packet buffer, 51
Packet switching, 8
G
paquet, 36
gateway, 56
peer-to-peer, 11
peering, 9
H
pipelining, 13
Host ID, 41
point-`a-point, 63
HTTP, 12
point-to-point, voir point-`a-point
hub, 74
poisoned reverse, 54
polling, 70
I
PPP, voir point-`a-point
iBGP, 58
protocole, 10, 12
ICMP, 46
protocole inter-AS, 57
internal BGP, voir iBGP
Internet Control Message Protocol, voir ICMPprotocole intra-AS, 57
protocoles, 5
Internet Protocol, 38
proxy, 13
IP, voir Internet Protocol
pure ALOHA, voir ALOHA pur
IP dynamiques, 43
IP fixe, 43
R
requˆete ARP, 71
L
RIP, voir Routing Information Protocol
lien, 61
root port, 76
liens `a acc`es multiples, 63
Round-Trip Time, voir RTT
link state, 48
routage, 48
load balancing, 14
routing, 36
longest prefix match, 42
Routing Information Protocol, 57
M
RST, 28
masque de sous-r´eseau, 41
RTT, 22
matrice de trafic, 49
S
Maximum Segment Size, 34
segment, 18
modem, 6
serveur DNS local, 15
Modulation d’amplitude, 6
serveurs DNS racines, 15
Modulation de phase, 6
slotted ALOHA, voir ALOHA discr´etis´e
MTU, 39
Slow Start, 34
multiplexage, 18
socket, 12
N
sockets, 11
NAT, 44
somme de contrˆole, voir checksum
Network Address Translation, 44
sous-r´eseau, 41
network edge, 5
speed maching, 31
num´ero de port, 12
split horizon, voir poisoned reverse
stateless, 13
O
stop and wait, 21
OSPF, 57
subnet mask, 41
switch, 74
73

INDEX
switching fabric, 37
SYN, 28
syst`emes d’extr´emit´e, 5
T
table ARP, 71
table d’acheminement, 37, 40
TCP, 12
TDM, voir Time Division Method
TDMA, 64
temps de propagation, 22
Time Division Method, 9
Time Division Multiple Access, voir TDMA
time to live, 39
timed wait, 32
TLD, voir Top-Level Domains
token, 70
Top-Level Domains, 15
trame, 62
TTL, voir time to live, 71
tunneling, 47
U
UDP, 12
V
vitesse de propagation, 9
vitesse de transmission, 9

74

